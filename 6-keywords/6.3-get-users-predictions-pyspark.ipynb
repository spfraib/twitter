{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.appName(\"\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: CO\n",
      "Labels: loss, unemployed, search, hire, offer\n"
     ]
    }
   ],
   "source": [
    "country_code=\"US\"\n",
    "labels=['loss', 'unemployed', 'search', 'hire', 'offer']\n",
    "out_of_work = ['loss', 'unemployed', 'search']\n",
    "path_to_data='/user/spf248/twitter/data'\n",
    "print('Country:',country_code)\n",
    "print('Labels:',', '.join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load...')\n",
    "start = timer()\n",
    "users_attributes=spark.read.orc(os.path.join(path_to_data,'users','by-country','country_code='+country_code))\n",
    "predictions=spark.read.orc(os.path.join(path_to_data,'classification',country_code,'keywords'))\n",
    "end = timer()\n",
    "print('Computing time (in sec.):', round(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    array = [float(x)-float(value) if float(x)-float(value)>0 else float(value)-float(x) for x in array]\n",
    "    return int(np.argmin(array))\n",
    "find_nearest_udf = F.udf(find_nearest, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=predictions.groupby(['user_id','year']).agg(F.count('tweet_id').alias('n_tweets'))\n",
    "tmp=tmp.join(predictions.groupby('user_id').agg(F.min('created_at').alias('oldest_tweet')),on='user_id')\n",
    "tmp=tmp.join(users_attributes.select('user_id','created_at').withColumnRenamed('created_at','account_creation'),on='user_id')\n",
    "tmp=tmp.withColumn(\"start\",F.concat_ws(\"-\",F.col(\"year\"),F.lit(1),F.lit(1)).cast(\"timestamp\"))\n",
    "tmp=tmp.withColumn(\"end\",F.concat_ws(\"-\",F.col(\"year\"),F.lit(12),F.lit(31)).cast(\"timestamp\"))\n",
    "tmp=tmp.withColumn('account_age_by_year', F.datediff(tmp['end'],F.least(tmp['account_creation'],tmp['oldest_tweet']))) # Some account creation date are posterior to the first tweet\n",
    "tmp=tmp.withColumn('activity_period_by_year', F.datediff(tmp['end'],F.greatest(tmp['start'],tmp['oldest_tweet'])))\n",
    "tmp=tmp.withColumn('activity_by_year',tmp['n_tweets']/tmp['activity_period_by_year'])\n",
    "w_user = Window().partitionBy('user_id').orderBy(F.col(\"year\"))\n",
    "tmp=tmp.withColumn('lag_account_age_by_year',F.lag('account_age_by_year').over(w_user))\n",
    "tmp=tmp.withColumn('lag_activity_by_year',F.lag('activity_by_year').over(w_user))\n",
    "tmp=tmp.na.drop()\n",
    "quantiles_account_age=tmp.groupBy('year').agg(F.expr('percentile_approx(lag_account_age_by_year, array(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9))').alias('lag_account_age_by_year_quantiles'))\n",
    "quantiles_activity=tmp.groupBy('year').agg(F.expr('percentile_approx(lag_activity_by_year, array(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9))').alias('lag_activity_by_year_quantiles'))\n",
    "tmp=tmp.join(quantiles_account_age,on='year')\n",
    "tmp=tmp.join(quantiles_activity,on='year')\n",
    "tmp=tmp.withColumn('lag_account_age_by_year_quantile',find_nearest_udf('lag_account_age_by_year_quantiles','lag_account_age_by_year'))\n",
    "tmp=tmp.withColumn('lag_activity_by_year_quantile',find_nearest_udf('lag_activity_by_year_quantiles','lag_activity_by_year'))\n",
    "tmp=tmp.select('year','user_id','lag_account_age_by_year_quantile','lag_activity_by_year_quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count users in each bucket...')\n",
    "start = timer()\n",
    "df=predictions.withColumn('out_of_work',(sum([predictions[col] for col in out_of_work])>0).cast(\"int\"))\n",
    "df=df.withColumn('year',F.year('created_at').cast(\"string\"))\n",
    "df=df.withColumn('month',F.month('created_at').cast(\"string\"))\n",
    "df=df.groupBy('year','month','user_location','user_id').max()\n",
    "df=df.toDF(*(col.replace('max(','').replace(')','') for col in df.columns))\n",
    "df=df.join(tmp,on=['year','user_id'])\n",
    "col2func=dict((label,'sum') for label in labels+['out_of_work'])\n",
    "col2func.update({'user_id':'count'})\n",
    "df=df.groupby(['year','month','user_location','lag_account_age_by_year_quantile','lag_activity_by_year_quantile']).agg(col2func)\n",
    "df=df.toDF(*(col.replace('count(','').replace('sum(','').replace(')','') for col in df.columns))\n",
    "df.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,'classification',country_code,'users_predictions'))\n",
    "end = timer()\n",
    "print('Computing time (in sec.):', round(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
