{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "    \n",
    "https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/user-object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import socket\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import lzma\n",
    "import ujson as json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Data Collected: tweets\n",
      "Function Used To Collect Data: get_tweets_with_identified_location\n",
      "Output Files Extension: .csv.bz2\n",
      "# Input File Parsed per Output File: 1\n"
     ]
    }
   ],
   "source": [
    "# data_type = 'users'\n",
    "data_type = 'tweets'\n",
    "print('Type of Data Collected:', data_type)\n",
    "\n",
    "# get_data = 'get_user_id_and_location'\n",
    "# get_data = 'get_tweets_with_geocoordinates_or_place'\n",
    "get_data = 'get_tweets_with_identified_location'\n",
    "print('Function Used To Collect Data:', get_data)\n",
    "\n",
    "ext = '.csv.bz2'\n",
    "# ext = '.json.bz2'\n",
    "print('Output Files Extension:', ext)\n",
    "\n",
    "block_size = 3\n",
    "print('# Input File Parsed per Output File:', block_size)\n",
    "\n",
    "path_to_locations='../../data/locations/profiles'\n",
    "\n",
    "# Check The Decahose Files Integrity\n",
    "date_min = datetime(year=2011, month=1, day=1).date()\n",
    "date_max = datetime(year=2018, month=12, day=31).date()\n",
    "# date_max = datetime(year=2019, month=6, day=30).date()\n",
    "n_files  = 2815 # 2974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Identified Locations: 39779\n"
     ]
    }
   ],
   "source": [
    "identified_locations = frozenset(pd.read_csv(os.path.join(path_to_locations,'account-locations-identified.csv')['LOCATION'])\n",
    "print('# Identified Locations:', len(identified_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Partitions: 16\n",
      "Hostname: FAC38c9860d5a89\n",
      "Index Partition: 0\n"
     ]
    }
   ],
   "source": [
    "hostname2partition={\n",
    "# 'achtung02':,\n",
    "'achtung03':0,\n",
    "'achtung04':1,\n",
    "'achtung05':2,\n",
    "'achtung06':3,\n",
    "'achtung07':4,\n",
    "'achtung08':5,\n",
    "'achtung09':6,\n",
    "# 'achtung10':,\n",
    "# 'achtung11':,\n",
    "'achtung12':7,\n",
    "'achtung13':8,\n",
    "'achtung14':9,\n",
    "'achtung15':10,\n",
    "'achtung16':11,\n",
    "'achtung17':12,\n",
    "}\n",
    "print('# Partitions:', len(hostname2partition))\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "print('Hostname:', hostname)\n",
    "\n",
    "partition = hostname2partition.get(hostname,0)\n",
    "print('Index Partition:', partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_filename(filename):\n",
    "    \n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', filename)\n",
    "    \n",
    "    return datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "\n",
    "# get_date_from_filename(input_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Input Files: 1\n",
      "First File: ../data/decahose/json/tweets.json.2016-12-23.xz\n",
      "Last File: ../data/decahose/json/tweets.json.2016-12-23.xz\n",
      "\n",
      "Partition List of Input Files:\n",
      "# Files: 1\n",
      "First File: ../data/decahose/json/tweets.json.2016-12-23.xz\n",
      "Last File: ../data/decahose/json/tweets.json.2016-12-23.xz\n"
     ]
    }
   ],
   "source": [
    "# Create Array of Input Files\n",
    "def get_input_files(partition,date_min=date_min,date_max=date_max,n_files=n_files):\n",
    "    \n",
    "    # Tweets Stored on Cluster \n",
    "    if os.path.exists('/net/twitter/gardenhose-data/json/'):\n",
    "        path_to_input_files  = '/net/twitter/gardenhose-data/json/'\n",
    "    # For testing\n",
    "    elif os.path.exists('../../data/decahose/'):\n",
    "        path_to_input_files  = '../../data/decahose/'\n",
    "    else:\n",
    "        sys.exit('Incorrect working directory...exiting.')\n",
    "        \n",
    "    input_files = sorted(glob(path_to_input_files+'tweets.json.*.xz'))\n",
    "    \n",
    "    input_files = [input_file for input_file in input_files if \n",
    "                   get_date_from_filename(input_file)>=date_min and\n",
    "                   get_date_from_filename(input_file)<=date_max]\n",
    "\n",
    "    if len(input_files) != n_files:\n",
    "        sys.exit('Check input files...exiting.')\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    input_files = np.random.permutation(input_files)\n",
    "    \n",
    "    print('# Input Files:', len(input_files))\n",
    "    print('First File:', input_files[0])\n",
    "    print('Last File:', input_files[-1])\n",
    "    print()\n",
    "    \n",
    "    partitioned_files = np.array_split(input_files, len(hostname2partition))\n",
    "    input_files = partitioned_files[partition]\n",
    "    \n",
    "    print('Partition List of Input Files:')\n",
    "    print('# Files:', len(input_files))\n",
    "    print('First File:', input_files[0])\n",
    "    print('Last File:', input_files[-1])\n",
    "\n",
    "    return input_files\n",
    "\n",
    "input_files = get_input_files(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Root:\n",
      "../data/decahose/parsed/tweets/tweets-with-identified-location-from-decahose-partition-0\n"
     ]
    }
   ],
   "source": [
    "# Create Path to Output File\n",
    "def get_output_root(partition,data_type,get_data):\n",
    "    \n",
    "    path_to_output_files = '../data/decahose/'+data_type+'/'\n",
    "    \n",
    "    os.makedirs(path_to_output_files, exist_ok=True)\n",
    "    \n",
    "    output_file = get_data.replace('get_','').replace('_','-')+'-from-decahose-partition-'+str(partition)\n",
    "        \n",
    "    return path_to_output_files+output_file\n",
    "\n",
    "print('Output Root:')\n",
    "print(get_output_root(partition,data_type,get_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_with_identified_location(input_file):\n",
    "    \n",
    "    fields = [\n",
    "    'created_at',\n",
    "    'id_str',\n",
    "    'lang',\n",
    "    ]\n",
    "\n",
    "    fields_user = [\n",
    "    'id_str',\n",
    "#     'name',\n",
    "#     'screen_name',\n",
    "    'location',\n",
    "#     'description',\n",
    "#     'created_at',\n",
    "#     'profile_image_url_https',\n",
    "#     'default_profile_image',\n",
    "#     'time_zone',\n",
    "    ]\n",
    "\n",
    "    cols = fields+['text','place_id','tweet_longitude','tweet_latitude']+['user_'+x for x in fields_user]\n",
    "\n",
    "    tweets = []\n",
    "\n",
    "    with lzma.open(input_file,'rb') as f:\n",
    "\n",
    "        for i,line in enumerate(f):\n",
    "\n",
    "            # Only Select Tweets With Account Location (Could Be in the Replies)\n",
    "            if b'\"location\":\"' in line:\n",
    "                \n",
    "                # Json Parsing Can Fail\n",
    "                try:\n",
    "                    \n",
    "                    tweet = json.loads(line)\n",
    "                    \n",
    "                    # Only Select Tweets With Identified Location\n",
    "                    if tweet.get('user', {}).get('location',None) in identified_locations:\n",
    "            \n",
    "                        tweets.append(\n",
    "                        [tweet.get(field, None) for field in fields]+\\\n",
    "                        [tweet['extended_tweet']['full_text'] if tweet['truncated'] else tweet['text']]+\\\n",
    "                        [tweet['place']['id'] if tweet['place'] else None]+\\\n",
    "                        [tweet['coordinates']['coordinates'][0] if tweet['coordinates'] else None]+\\\n",
    "                        [tweet['coordinates']['coordinates'][1] if tweet['coordinates'] else None]+\\\n",
    "                        [tweet['user'].get(field, None) for field in fields_user]\n",
    "                        )\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "    return pd.DataFrame(tweets, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There Could Be Some Duplicated Rows\n",
    "def get_user_id_and_location(input_file):\n",
    "    \n",
    "    tweets = []\n",
    "\n",
    "    with lzma.open(input_file,'rb') as f:\n",
    "\n",
    "        for i,line in enumerate(f):\n",
    "\n",
    "            # Only Select Tweets With Account Location (Could Be in the Replies)\n",
    "            if b'\"location\":\"' in line:\n",
    "\n",
    "                # Json Parsing Can Fail\n",
    "                try:\n",
    "                    \n",
    "                    tweet = json.loads(line.decode(\"utf-8\"))\n",
    "                    \n",
    "                    tweets.append([\n",
    "                    tweet.get('user', {}).get('id_str',None), \n",
    "                    tweet.get('user', {}).get('location',None),\n",
    "                    ])\n",
    "\n",
    "                except:\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "#             if i == 100000:\n",
    "#                 break\n",
    "                \n",
    "    return pd.DataFrame(tweets,columns=['USER ID','USER LOCATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_with_geocoordinates_or_place(input_file):\n",
    "    \n",
    "    tweets = []\n",
    "\n",
    "    with lzma.open(input_file,'rb') as f:\n",
    "\n",
    "        for i,line in enumerate(f):\n",
    "            \n",
    "            # Only Select Tweets With Geocoordinates (Could Be in the Replies)\n",
    "            if b'\"coordinates\":{' in line or b'\"place\":{' in line:\n",
    "\n",
    "                # Json Parsing Can Fail\n",
    "                try:\n",
    "                    \n",
    "                    tweet = json.loads(line.decode(\"utf-8\"))\n",
    "                    \n",
    "                    # Only Collect If Selected Data in the Original Tweet (Not RT etc.)\n",
    "                    if tweet.get('coordinates',None) or tweet.get('place',None):\n",
    "                    \n",
    "                        tweets.append(tweet)\n",
    "\n",
    "                except:\n",
    "                    \n",
    "                    continue\n",
    "                \n",
    "#             if i == 100000:\n",
    "#                 break\n",
    "                \n",
    "    return pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    tweets = pd.DataFrame()\n",
    "\n",
    "    for i, input_file in enumerate(input_files):\n",
    "\n",
    "        start = timer()\n",
    "        print()\n",
    "        print('File', i)\n",
    "        print(input_file)\n",
    "        \n",
    "        # Create an output file every <size> file\n",
    "        output_file = get_output_root(partition,data_type,get_data)+'-block-'+str(i//block_size)+ext\n",
    "        \n",
    "        if os.path.exists(output_file):\n",
    "            print('Output file', output_file, 'already exists.')\n",
    "            continue\n",
    "\n",
    "        tweets = pd.concat([tweets, eval(get_data)(input_file)])\n",
    "        print('# Tweets:', tweets.shape[0])\n",
    "        \n",
    "        # Save if Next Index is a Multiple of <size> or Reading Last File\n",
    "        if not (i+1)%block_size or i==len(input_files)-1:\n",
    "            \n",
    "            print('Save Output File:', output_file)\n",
    "            \n",
    "            if 'csv' in ext:\n",
    "                \n",
    "                tweets.to_csv(\n",
    "                output_file, \n",
    "                sep=',', \n",
    "                line_terminator='\\n')\n",
    "                \n",
    "                # pd.read_csv(\n",
    "                # output_file, \n",
    "                # index_col=0, \n",
    "                # sep=',', \n",
    "                # dtype=object, \n",
    "                # na_filter=False,\n",
    "                # lineterminator='\\n')\n",
    "                \n",
    "            elif 'json' in ext:\n",
    "                \n",
    "                tweets.to_json(\n",
    "                output_file,\n",
    "                orient='records',\n",
    "                force_ascii=False,\n",
    "                date_format=None,\n",
    "                double_precision=15)\n",
    "                \n",
    "                # pd.read_json(output_file,\n",
    "                # orient='records',\n",
    "                # dtype=False,\n",
    "                # convert_dates=False)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                sys.exit('Extension error... exiting.')\n",
    "            \n",
    "            del tweets\n",
    "            tweets = pd.DataFrame()  \n",
    "\n",
    "        end = timer()\n",
    "        print('Computing Time:', round(end - start), 'Sec')\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File 0\n",
      "../data/decahose/json/tweets.json.2016-12-23.xz\n",
      "# Tweets: 102024\n",
      "Save Output File: ../data/decahose/parsed/tweets/tweets-with-identified-location-from-decahose-partition-0-block-0.csv.bz2\n",
      "Computing Time: 43 Sec\n",
      "\n",
      "Total Computing Time: 43 Sec\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "end = timer()\n",
    "print()\n",
    "print('Total Computing Time:', round(end - start), 'Sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_json(\n",
    "# glob(get_output_root(partition,data_type,get_data)+'*'+ext)[0],\n",
    "# orient='records',\n",
    "# dtype=False,\n",
    "# convert_dates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\n",
    "# glob(get_output_root(partition,data_type,get_data)+'*'+ext)[0], \n",
    "# index_col=0, \n",
    "# sep=',', \n",
    "# dtype=object, \n",
    "# na_filter=False,\n",
    "# lineterminator='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
