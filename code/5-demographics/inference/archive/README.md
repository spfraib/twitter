# Demographic Inference of Twitter users

## Requirements:
Install required libraries using requirements.txt file from M3 repository: https://github.com/euagendas/m3inference

## Scripts

There are two main scripts in this folder:
1. tar_user_pictures.py: This file has two modes controlled by the option ``resize_before_tar``. If ``resize_before_tar=True`` (default), it will resize the images using the utility ``resize_imgs`` from m3, otherwise it will only tar the images from a directory.
2. demographic_inference.py: This script does the demographic inference from a folder of resized images generated by ``tar_user_pictures.py`` with ``resize_before_tar=True``.

### Running Scripts

- ``tar_resize.sbatch`` and ``onlytar.sbatch`` are helpers for the ``tar_user_pictures.py`` script.
- ``m3.sbatch`` is the helper for the script ``demograic_inference.py``.

These scripts allow the use of job arrays. 

For example, running ``sbatch --array=0-49 onlytar.sbatch``, will run ``tar_user_pictures.py`` with its default parameters with ``input_dir="/scratch/spf248/twitter/data/profile_pictures/US/profile_pictures_sam/"`` and ``output_dir=/scratch/spf248/joao/data/profile_pictures/resized_tars/US/`` (change this parameters accordingly). At the ``output_dir`` temporary files will be generated in parallel following a pattern like ``XXX_YY.tar``, with ``XXX`` being a number from 100 to 999 (see hash function) and YY from 0 to 49 (job array).

These temporary files can be merged together with a simple bash command like:
```bash
for i in `seq 100 999`; do mkdir d${i}; cat ${i}_*.tar > ${i}.tar; tar -C d${i} -ixf ${i}.tar; cd d${i}; tar -zcf ../${i}.tar *; cd -; rm -rf d${i}; echo "Done ${i}"; done
```

### Hash Function
As hash function to map a user picture filename to a tar file, we used the three first numbers in the filename (e.g., 1234567.jpg will be in 123.tar).
After running all the scripts, I figured out that this was not a very good decision, as there is a way large number of filenames started with `100` compared to, e.g., `541`, the tar file `100.tar` can be up to 10x bigger than `541.tar`. A better solution could be to use as hash a function like (sum(digits filename) % 1000).

### Parallel re-tar
The loop above going from 100 to 999 can be paralleized. A simple way to do that is starting a slurm job with 20 cpus and running the retarall.sh script, that run ``retar.sh`` in parallel for every 20 items and wait the result before moving to the next loop of 20 items. Concreatelly the steps are:
1. Go to the output folder: e.g., /scratch/spf248/joao/data/profile_pictures/resized_tars/BR/
2. Copy both retar.sh and retarall.sh to the output folder.
3. Start an iteractive slurm process with ``srun --cpus-per-task 16 --mem-per-cpu 1000 --time 8:00:00 --pty bash``
4. Run retarall.sh with ``bash retarall.sh``.


