{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:63: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID 123123123\n",
      "SLURM_ARRAY_TASK_ID 10\n",
      "SLURM_ARRAY_TASK_COUNT 500\n",
      "Load Random Tweets:\n",
      "#files: 1\n",
      "/scratch/spf248/twitter/data/classification/US/random_1perct_sample/part-00974-1c1e6466-49fa-411b-beb0-276d14cdffab-c000.snappy.parquet\n",
      "(42521, 2)\n",
      "time taken to load random sample: 0.05587458610534668 seconds\n",
      "(100, 2)\n",
      "GENSIM_DATA_DIR /scratch/da2734/twitter/code/11-baseline/logit_glove/downloaded\n",
      "loading glove\n",
      "time taken: 222.1632137298584 seconds\n",
      "calculating embeddings\n",
      "time taken: 0.00710606575012207 seconds\n",
      "per tweet: 7.481813430786133e-05 seconds\n",
      "\n",
      "\n",
      "!!!!! is_unemployed\n",
      "************ is_unemployed ************\n",
      "     id  \\\n",
      "0  2772   \n",
      "1  1205   \n",
      "2  4512   \n",
      "3  7249   \n",
      "4  9453   \n",
      "\n",
      "                                                                                                                                        text  \\\n",
      "0  I was late again got work today. They gonna fire the shit outta me one day                                                                  \n",
      "1  Got laid off today :/#work                                                                                                                  \n",
      "2  Microsoft posted a job you might be interested in. Sr. SW Engineer for Windows System - Beijing City China CN via LinkedIn.                 \n",
      "3  Now the owners wife is having a breakdown and saying she wanna quit. This is hilarious                                                      \n",
      "4  I am happy today. Good. Time to get some food and do some work on Unemployed. Season 2 premiere is scary really liking what I have though   \n",
      "\n",
      "   class  \n",
      "0  0      \n",
      "1  1      \n",
      "2  0      \n",
      "3  0      \n",
      "4  1      \n",
      "Precision:  0.7065217391304348\n",
      "Recall:  0.7008086253369272\n",
      "Predictions of Random Tweets:\n",
      "<class 'numpy.ndarray'>\n",
      "time taken: 0.0004112720489501953 seconds\n",
      "per tweet: 6.792545318603515e-06 seconds\n",
      "Save Predictions of Random Tweets:\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_1pct_sample_GLOVE/is_unemployed/random-123123123-10.csv saved\n",
      "time taken: 0.006476640701293945 seconds\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# run using: sbatch --array=0-9 7.9-get-predictions-from-BERT.sh\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# column = sys.argv[1]\n",
    "# column = 'is_unemployed'\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# loading the model\n",
    "####################################################################################################################################\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# import sys\n",
    "import random\n",
    "import numpy as np\n",
    "# import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "# sys.path.append('../')\n",
    "\n",
    "# from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "# from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, \\\n",
    "#     convert_examples_to_features\n",
    "# from fast_bert.learner_cls import BertLearner\n",
    "# # from fast_bert.metrics import accuracy_multilabel, accuracy_thresh, fbeta, roc_auc, accuracy\n",
    "# from fast_bert.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "root_path='/scratch/da2734/twitter/jobs/running_on_200Msamples/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def create_model(columnm, epoch):\n",
    "#     if not os.path.exists('/scratch/da2734/twitter/jobs/running_on_200Msamples/logs/log_binary_pos_neg_{}/'.format(column)):\n",
    "#         os.makedirs('/scratch/da2734/twitter/jobs/running_on_200Msamples/logs/log_binary_pos_neg_{}/'.format(column))\n",
    "\n",
    "#     LOG_PATH = Path('/scratch/da2734/twitter/jobs/running_on_200Msamples/logs/log_binary_pos_neg_{}/'.format(column))\n",
    "#     print('LOG_PATH', LOG_PATH)\n",
    "#     DATA_PATH = Path('/scratch/da2734/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced/')\n",
    "#     LABEL_PATH = Path('/scratch/da2734/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced/')\n",
    "#     OUTPUT_PATH = Path('/scratch/da2734/twitter/jobs/training_binary/models_may20_9Klabels/output_{}'.format(column))\n",
    "#     FINETUNED_PATH = None\n",
    "\n",
    "#     args = Box({\n",
    "#         \"run_text\": \"100Msamples\",\n",
    "#         \"train_size\": -1,\n",
    "#         \"val_size\": -1,\n",
    "#         \"log_path\": LOG_PATH,\n",
    "#         \"full_data_dir\": DATA_PATH,\n",
    "#         \"data_dir\": DATA_PATH,\n",
    "#         \"task_name\": \"labor_market_classification\",\n",
    "#         \"no_cuda\": False,\n",
    "#         #     \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "#         \"output_dir\": OUTPUT_PATH,\n",
    "#         \"max_seq_length\": 512,\n",
    "#         \"do_train\": True,\n",
    "#         \"do_eval\": True,\n",
    "#         \"do_lower_case\": True,\n",
    "#         \"train_batch_size\": 8,\n",
    "#         \"eval_batch_size\": 16,\n",
    "#         \"learning_rate\": 5e-5,\n",
    "#         \"num_train_epochs\": 100,\n",
    "#         \"warmup_proportion\": 0.0,\n",
    "#         \"no_cuda\": False,\n",
    "#         \"local_rank\": -1,\n",
    "#         \"seed\": 42,\n",
    "#         \"gradient_accumulation_steps\": 1,\n",
    "#         \"optimize_on_cpu\": False,\n",
    "#         \"fp16\": False,\n",
    "#         \"fp16_opt_level\": \"O1\",\n",
    "#         \"weight_decay\": 0.0,\n",
    "#         \"adam_epsilon\": 1e-8,\n",
    "#         \"max_grad_norm\": 1.0,\n",
    "#         \"max_steps\": -1,\n",
    "#         \"warmup_steps\": 500,\n",
    "#         \"logging_steps\": 50,\n",
    "#         \"eval_all_checkpoints\": True,\n",
    "#         \"overwrite_output_dir\": True,\n",
    "#         \"overwrite_cache\": True,\n",
    "#         \"seed\": 42,\n",
    "#         \"loss_scale\": 128,\n",
    "#         \"task_name\": 'intent',\n",
    "#         \"model_name\": 'bert-base-uncased',\n",
    "#         \"model_type\": 'bert'\n",
    "#     })\n",
    "\n",
    "#     import logging\n",
    "\n",
    "#     logfile = str(LOG_PATH / 'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "#     logging.basicConfig(\n",
    "#         level=logging.INFO,\n",
    "#         format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#         datefmt='%m/%d/%Y %H:%M:%S',\n",
    "#         handlers=[\n",
    "#             logging.FileHandler(logfile),\n",
    "#             logging.StreamHandler(sys.stdout)\n",
    "#         ])\n",
    "\n",
    "#     logger = logging.getLogger()\n",
    "\n",
    "#     logger.info(args)\n",
    "\n",
    "#     device = torch.device('cuda')\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         args.multi_gpu = True\n",
    "#     else:\n",
    "#         args.multi_gpu = False\n",
    "\n",
    "#     label_cols = ['class']\n",
    "\n",
    "#     databunch = BertDataBunch(\n",
    "#         args['data_dir'],\n",
    "#         LABEL_PATH,\n",
    "#         args.model_name,\n",
    "#         train_file='train_{}.csv'.format(column),\n",
    "#         val_file='val_{}.csv'.format(column),\n",
    "#         label_file='label_{}.csv'.format(column),\n",
    "#         # test_data='test.csv',\n",
    "#         text_col=\"text\",  # this is the name of the column in the train file that containts the tweet text\n",
    "#         label_col=label_cols,\n",
    "#         batch_size_per_gpu=args['train_batch_size'],\n",
    "#         max_seq_length=args['max_seq_length'],\n",
    "#         multi_gpu=args.multi_gpu,\n",
    "#         multi_label=False,\n",
    "#         model_type=args.model_type)\n",
    "\n",
    "#     num_labels = len(databunch.labels)\n",
    "#     print('num_labels', num_labels)\n",
    "\n",
    "#     print('time taken to load all this stuff:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "#     # metrics defined: https://github.com/kaushaltrivedi/fast-bert/blob/d89e2aa01d948d6d3cdea7ad106bf5792fea7dfa/fast_bert/metrics.py\n",
    "#     metrics = []\n",
    "#     # metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\n",
    "#     # metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "#     # metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "#     metrics.append({'name': 'accuracy', 'function': accuracy})\n",
    "#     metrics.append({'name': 'roc_auc_save_to_plot_binary', 'function': roc_auc_save_to_plot_binary})\n",
    "#     # metrics.append({'name': 'accuracy_multilabel', 'function': accuracy_multilabel})\n",
    "\n",
    "#     learner = BertLearner.from_pretrained_model(\n",
    "#         databunch,\n",
    "#         pretrained_path='/scratch/da2734/twitter/jobs/training_binary/models_may20_9Klabels/output_{}/model_out_{}/'.format(column, epoch),\n",
    "#         metrics=metrics,\n",
    "#         device=device,\n",
    "#         logger=logger,\n",
    "#         output_dir=args.output_dir,\n",
    "#         finetuned_wgts_path=FINETUNED_PATH,\n",
    "#         warmup_steps=args.warmup_steps,\n",
    "#         multi_gpu=args.multi_gpu,\n",
    "#         is_fp16=args.fp16,\n",
    "#         multi_label=False,\n",
    "#         logging_steps=0)\n",
    "\n",
    "#     return learner\n",
    "\n",
    "\n",
    "# best_epochs = {\n",
    "#     'is_hired_1mo':8,\n",
    "#     'lost_job_1mo':5,\n",
    "#     'job_offer':4,\n",
    "#     'is_unemployed':3,\n",
    "#     'job_search':6\n",
    "# }\n",
    "\n",
    "# best_epochs = {\n",
    "#     'is_hired_1mo':6,\n",
    "#     'lost_job_1mo':4,\n",
    "#     'job_offer':3,\n",
    "#     'is_unemployed':3,\n",
    "#     'job_search':4\n",
    "# }\n",
    "\n",
    "\n",
    "def get_env_var(varname, default):\n",
    "    if os.environ.get(varname) != None:\n",
    "        var = int(os.environ.get(varname))\n",
    "        print(varname, ':', var)\n",
    "    else:\n",
    "        var = default\n",
    "        print(varname, ':', var, '(Default)')\n",
    "    return var\n",
    "\n",
    "\n",
    "# Choose Number of Nodes To Distribute Credentials: e.g. jobarray=0-4, cpu_per_task=20, credentials = 90 (<100)\n",
    "# SLURM_JOB_ID = get_env_var('SLURM_JOB_ID', 0)\n",
    "# SLURM_ARRAY_TASK_ID = get_env_var('SLURM_ARRAY_TASK_ID', 0)\n",
    "# SLURM_ARRAY_TASK_COUNT = get_env_var('SLURM_ARRAY_TASK_COUNT', 1)\n",
    "\n",
    "SLURM_JOB_ID = 123123123\n",
    "SLURM_ARRAY_TASK_ID = 10\n",
    "SLURM_ARRAY_TASK_COUNT = 500\n",
    "\n",
    "\n",
    "print('SLURM_JOB_ID', SLURM_JOB_ID)\n",
    "print('SLURM_ARRAY_TASK_ID', SLURM_ARRAY_TASK_ID)\n",
    "print('SLURM_ARRAY_TASK_COUNT', SLURM_ARRAY_TASK_COUNT)\n",
    "\n",
    "\n",
    "# ####################################################################################################################################\n",
    "# # loading data\n",
    "# ####################################################################################################################################\n",
    "\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path_to_data='/scratch/spf248/twitter/data/classification/US/'\n",
    "\n",
    "\n",
    "# print('Load Filtered Tweets:')\n",
    "# # filtered contains 8G of data!!\n",
    "# start_time = time.time()\n",
    "\n",
    "# paths_to_filtered=list(np.array_split(\n",
    "#                         # glob(os.path.join(path_to_data,'filtered','*.parquet')),\n",
    "#                         # glob(os.path.join(path_to_data,'filtered_10perct_sample','*.parquet')),\n",
    "#                         glob(os.path.join(path_to_data,'filtered_1perct_sample','*.parquet')),\n",
    "#                         SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID]\n",
    "#                        )\n",
    "# print('#files:', len(paths_to_filtered))\n",
    "\n",
    "# tweets_filtered=pd.DataFrame()\n",
    "# for file in paths_to_filtered:\n",
    "#     print(file)\n",
    "#     tweets_filtered=pd.concat([tweets_filtered,pd.read_parquet(file)[['tweet_id','text']]])\n",
    "#     print(tweets_filtered.shape)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# # tweets_filtered = tweets_filtered[:100]\n",
    "\n",
    "# print('time taken to load keyword filtered sample:', str(time.time() - start_time), 'seconds')\n",
    "# print(tweets_filtered.shape)\n",
    "\n",
    "\n",
    "print('Load Random Tweets:')\n",
    "# random contains 7.3G of data!!\n",
    "start_time = time.time()\n",
    "\n",
    "paths_to_random=list(np.array_split(\n",
    "                        # glob(os.path.join(path_to_data,'random','*.parquet')),\n",
    "                        # glob(os.path.join(path_to_data,'random_10perct_sample','*.parquet')),\n",
    "                        glob(os.path.join(path_to_data,'random_1perct_sample','*.parquet')),\n",
    "                        SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID])\n",
    "print('#files:', len(paths_to_random))\n",
    "\n",
    "tweets_random=pd.DataFrame()\n",
    "for file in paths_to_random:\n",
    "    print(file)\n",
    "    tweets_random=pd.concat([tweets_random,pd.read_parquet(file)[['tweet_id','text']]])\n",
    "    print(tweets_random.shape)\n",
    "\n",
    "    break\n",
    "\n",
    "tweets_random = tweets_random[:100]\n",
    "\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_random.shape)\n",
    "\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec\n",
    "\n",
    "import os\n",
    "print('GENSIM_DATA_DIR', os.environ['GENSIM_DATA_DIR'] )\n",
    "    \n",
    "start_time = time.time()\n",
    "print('loading glove')\n",
    "glove_twitter = api.load(\"glove-twitter-200\")\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')    \n",
    "\n",
    "start_time = time.time()\n",
    "print('calculating embeddings')    \n",
    "random_data_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in tweets_random[\"text\"]]))\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "print('per tweet:', (time.time() - start_time)/tweets_random.shape[0], 'seconds')\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "\n",
    "    print('\\n\\n!!!!!', column)\n",
    "\n",
    "#     start = time.time()\n",
    "#     learner = create_model(column, best_epochs[column])\n",
    "#     print('load model:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "#     print('Predictions of Filtered Tweets:')\n",
    "#     start_time = time.time()\n",
    "#     predictions_filtered = learner.predict_batch(tweets_filtered['text'].values.tolist())\n",
    "#     print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "#     print('per tweet:', (time.time() - start_time)/tweets_filtered.shape[0], 'seconds')\n",
    "\n",
    "#     # In[ ]:\n",
    "\n",
    "    data_path = \"/scratch/da2734/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced/\"\n",
    "    print(\"************ {} ************\".format(column))\n",
    "\n",
    "    train_file_name = \"train_{}.csv\".format(column)\n",
    "    val_file_name = \"val_{}.csv\".format(column)\n",
    "    #download data\n",
    "    df_train = pd.read_csv(os.path.join(data_path, train_file_name))\n",
    "    print(df_train.head())\n",
    "    df_val = pd.read_csv(os.path.join(data_path, val_file_name))\n",
    "    #create embeddings\n",
    "    train_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_train[\"text\"]]))\n",
    "    validation_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_val[\"text\"]]))\n",
    "    #train\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(train_vecs_glove_mean,df_train[\"class\"])\n",
    "    #evaluate\n",
    "    df_val[\"class_predict\"] = clf.predict(validation_vecs_glove_mean)\n",
    "    TP, FP, TN, FN = perf_measure(df_val[\"class\"], df_val[\"class_predict\"])\n",
    "    print(\"Precision: \", TP/(TP+FP))\n",
    "    print(\"Recall: \", TP/(TP+FN))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Predictions of Random Tweets:')\n",
    "    start_time = time.time()\n",
    "    #     predictions_random = learner.predict_batch(tweets_random['text'].values.tolist())\n",
    "    predictions_random = clf.predict_proba(random_data_vecs_glove_mean)\n",
    "\n",
    "    print(type(predictions_random))\n",
    "    # print(predictions_random)\n",
    "\n",
    "    print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "    print('per tweet:', (time.time() - start_time)/tweets_random.shape[0], 'seconds')\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    #     print('Save Predictions of Filtered Tweets:')\n",
    "    #     start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    #     df_filtered = predictions_filtered.set_index(tweets_filtered.tweet_id).rename(columns={\n",
    "    #             '0':'pos_model',\n",
    "    #             '1':'neg_model',\n",
    "    #     })\n",
    "\n",
    "    if not os.path.exists(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column)):\n",
    "        print('>>>> directory doesnt exists')\n",
    "        os.makedirs(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column))\n",
    "\n",
    "    #     # if not os.path.exists(os.path.join(root_path,'pred_output_10pct_sample', column)):\n",
    "    #     #     os.makedirs(os.path.join(root_path,'pred_output_10pct_sample', column))\n",
    "\n",
    "    #     # if not os.path.exists(os.path.join(root_path,'pred_output_full', column)):\n",
    "    #     #     os.makedirs(os.path.join(root_path,'pred_output_full', column))\n",
    "\n",
    "    #     df_filtered.to_csv(\n",
    "    #             # os.path.join(root_path,'pred_output', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "    #             os.path.join(root_path,'pred_output_1pct_sample', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "    #         )\n",
    "\n",
    "    #     print(os.path.join(root_path,'pred_output_1pct_sample', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'), 'saved')\n",
    "\n",
    "    #     print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Save Predictions of Random Tweets:')\n",
    "    start_time = time.time()\n",
    "    # predictions_random_df = pd.DataFrame(data=predictions_random, columns = ['neg', 'pos'])\n",
    "    predictions_random_df = pd.DataFrame(data=predictions_random, columns = ['neg_model', 'pos_model'])\n",
    "    df_random = predictions_random_df.set_index(tweets_random.tweet_id)\n",
    "    # df_random = predictions_random.set_index(tweets_random.tweet_id).rename(columns={\n",
    "    #         '0':'pos_model',\n",
    "    #         '1':'neg_model',\n",
    "    # })\n",
    "\n",
    "    # if not os.path.exists(os.path.join(root_path,'pred_output_10pct_sample', column)):\n",
    "    #     os.makedirs(os.path.join(root_path,'pred_output_10pct_sample', column))\n",
    "\n",
    "    df_random.to_csv(\n",
    "        # os.path.join(root_path,'pred_output', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "        os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "        )\n",
    "\n",
    "    print(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'), 'saved')\n",
    "\n",
    "    print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:63: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_JOB_ID 123123123\n",
      "SLURM_ARRAY_TASK_ID 10\n",
      "SLURM_ARRAY_TASK_COUNT 500\n",
      "Load Random Tweets:\n",
      "#files: 1\n",
      "/scratch/spf248/twitter/data/classification/US/random_1perct_sample/part-00974-1c1e6466-49fa-411b-beb0-276d14cdffab-c000.snappy.parquet\n",
      "(42521, 2)\n",
      "time taken to load random sample: 0.05513167381286621 seconds\n",
      "(100, 2)\n",
      "GENSIM_DATA_DIR /scratch/da2734/twitter/code/11-baseline/logit_glove/downloaded\n",
      "loading glove\n",
      "time taken: 220.26266598701477 seconds\n",
      "calculating embeddings\n",
      "time taken: 0.0072174072265625 seconds\n",
      "per tweet: 7.601737976074218e-05 seconds\n",
      "\n",
      "\n",
      "!!!!! is_unemployed\n",
      "\n",
      "\n",
      "!!!!! lost_job_1mo\n",
      "\n",
      "\n",
      "!!!!! job_search\n",
      "\n",
      "\n",
      "!!!!! is_hired_1mo\n",
      "\n",
      "\n",
      "!!!!! job_offer\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# run using: sbatch --array=0-9 7.9-get-predictions-from-BERT.sh\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# column = sys.argv[1]\n",
    "# column = 'is_unemployed'\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# loading the model\n",
    "####################################################################################################################################\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# import sys\n",
    "import random\n",
    "import numpy as np\n",
    "# import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "# sys.path.append('../')\n",
    "\n",
    "# from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "# from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, \\\n",
    "#     convert_examples_to_features\n",
    "# from fast_bert.learner_cls import BertLearner\n",
    "# # from fast_bert.metrics import accuracy_multilabel, accuracy_thresh, fbeta, roc_auc, accuracy\n",
    "# from fast_bert.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "root_path='/scratch/da2734/twitter/jobs/running_on_200Msamples/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def create_model(columnm, epoch):\n",
    "#     if not os.path.exists('/scratch/da2734/twitter/jobs/running_on_200Msamples/logs/log_binary_pos_neg_{}/'.format(column)):\n",
    "#         os.makedirs('/scratch/da2734/twitter/jobs/running_on_200Msamples/logs/log_binary_pos_neg_{}/'.format(column))\n",
    "\n",
    "#     LOG_PATH = Path('/scratch/da2734/twitter/jobs/running_on_200Msamples/logs/log_binary_pos_neg_{}/'.format(column))\n",
    "#     print('LOG_PATH', LOG_PATH)\n",
    "#     DATA_PATH = Path('/scratch/da2734/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced/')\n",
    "#     LABEL_PATH = Path('/scratch/da2734/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced/')\n",
    "#     OUTPUT_PATH = Path('/scratch/da2734/twitter/jobs/training_binary/models_may20_9Klabels/output_{}'.format(column))\n",
    "#     FINETUNED_PATH = None\n",
    "\n",
    "#     args = Box({\n",
    "#         \"run_text\": \"100Msamples\",\n",
    "#         \"train_size\": -1,\n",
    "#         \"val_size\": -1,\n",
    "#         \"log_path\": LOG_PATH,\n",
    "#         \"full_data_dir\": DATA_PATH,\n",
    "#         \"data_dir\": DATA_PATH,\n",
    "#         \"task_name\": \"labor_market_classification\",\n",
    "#         \"no_cuda\": False,\n",
    "#         #     \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "#         \"output_dir\": OUTPUT_PATH,\n",
    "#         \"max_seq_length\": 512,\n",
    "#         \"do_train\": True,\n",
    "#         \"do_eval\": True,\n",
    "#         \"do_lower_case\": True,\n",
    "#         \"train_batch_size\": 8,\n",
    "#         \"eval_batch_size\": 16,\n",
    "#         \"learning_rate\": 5e-5,\n",
    "#         \"num_train_epochs\": 100,\n",
    "#         \"warmup_proportion\": 0.0,\n",
    "#         \"no_cuda\": False,\n",
    "#         \"local_rank\": -1,\n",
    "#         \"seed\": 42,\n",
    "#         \"gradient_accumulation_steps\": 1,\n",
    "#         \"optimize_on_cpu\": False,\n",
    "#         \"fp16\": False,\n",
    "#         \"fp16_opt_level\": \"O1\",\n",
    "#         \"weight_decay\": 0.0,\n",
    "#         \"adam_epsilon\": 1e-8,\n",
    "#         \"max_grad_norm\": 1.0,\n",
    "#         \"max_steps\": -1,\n",
    "#         \"warmup_steps\": 500,\n",
    "#         \"logging_steps\": 50,\n",
    "#         \"eval_all_checkpoints\": True,\n",
    "#         \"overwrite_output_dir\": True,\n",
    "#         \"overwrite_cache\": True,\n",
    "#         \"seed\": 42,\n",
    "#         \"loss_scale\": 128,\n",
    "#         \"task_name\": 'intent',\n",
    "#         \"model_name\": 'bert-base-uncased',\n",
    "#         \"model_type\": 'bert'\n",
    "#     })\n",
    "\n",
    "#     import logging\n",
    "\n",
    "#     logfile = str(LOG_PATH / 'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "#     logging.basicConfig(\n",
    "#         level=logging.INFO,\n",
    "#         format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "#         datefmt='%m/%d/%Y %H:%M:%S',\n",
    "#         handlers=[\n",
    "#             logging.FileHandler(logfile),\n",
    "#             logging.StreamHandler(sys.stdout)\n",
    "#         ])\n",
    "\n",
    "#     logger = logging.getLogger()\n",
    "\n",
    "#     logger.info(args)\n",
    "\n",
    "#     device = torch.device('cuda')\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         args.multi_gpu = True\n",
    "#     else:\n",
    "#         args.multi_gpu = False\n",
    "\n",
    "#     label_cols = ['class']\n",
    "\n",
    "#     databunch = BertDataBunch(\n",
    "#         args['data_dir'],\n",
    "#         LABEL_PATH,\n",
    "#         args.model_name,\n",
    "#         train_file='train_{}.csv'.format(column),\n",
    "#         val_file='val_{}.csv'.format(column),\n",
    "#         label_file='label_{}.csv'.format(column),\n",
    "#         # test_data='test.csv',\n",
    "#         text_col=\"text\",  # this is the name of the column in the train file that containts the tweet text\n",
    "#         label_col=label_cols,\n",
    "#         batch_size_per_gpu=args['train_batch_size'],\n",
    "#         max_seq_length=args['max_seq_length'],\n",
    "#         multi_gpu=args.multi_gpu,\n",
    "#         multi_label=False,\n",
    "#         model_type=args.model_type)\n",
    "\n",
    "#     num_labels = len(databunch.labels)\n",
    "#     print('num_labels', num_labels)\n",
    "\n",
    "#     print('time taken to load all this stuff:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "#     # metrics defined: https://github.com/kaushaltrivedi/fast-bert/blob/d89e2aa01d948d6d3cdea7ad106bf5792fea7dfa/fast_bert/metrics.py\n",
    "#     metrics = []\n",
    "#     # metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\n",
    "#     # metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "#     # metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "#     metrics.append({'name': 'accuracy', 'function': accuracy})\n",
    "#     metrics.append({'name': 'roc_auc_save_to_plot_binary', 'function': roc_auc_save_to_plot_binary})\n",
    "#     # metrics.append({'name': 'accuracy_multilabel', 'function': accuracy_multilabel})\n",
    "\n",
    "#     learner = BertLearner.from_pretrained_model(\n",
    "#         databunch,\n",
    "#         pretrained_path='/scratch/da2734/twitter/jobs/training_binary/models_may20_9Klabels/output_{}/model_out_{}/'.format(column, epoch),\n",
    "#         metrics=metrics,\n",
    "#         device=device,\n",
    "#         logger=logger,\n",
    "#         output_dir=args.output_dir,\n",
    "#         finetuned_wgts_path=FINETUNED_PATH,\n",
    "#         warmup_steps=args.warmup_steps,\n",
    "#         multi_gpu=args.multi_gpu,\n",
    "#         is_fp16=args.fp16,\n",
    "#         multi_label=False,\n",
    "#         logging_steps=0)\n",
    "\n",
    "#     return learner\n",
    "\n",
    "\n",
    "# best_epochs = {\n",
    "#     'is_hired_1mo':8,\n",
    "#     'lost_job_1mo':5,\n",
    "#     'job_offer':4,\n",
    "#     'is_unemployed':3,\n",
    "#     'job_search':6\n",
    "# }\n",
    "\n",
    "# best_epochs = {\n",
    "#     'is_hired_1mo':6,\n",
    "#     'lost_job_1mo':4,\n",
    "#     'job_offer':3,\n",
    "#     'is_unemployed':3,\n",
    "#     'job_search':4\n",
    "# }\n",
    "\n",
    "\n",
    "def get_env_var(varname, default):\n",
    "    if os.environ.get(varname) != None:\n",
    "        var = int(os.environ.get(varname))\n",
    "        print(varname, ':', var)\n",
    "    else:\n",
    "        var = default\n",
    "        print(varname, ':', var, '(Default)')\n",
    "    return var\n",
    "\n",
    "\n",
    "# Choose Number of Nodes To Distribute Credentials: e.g. jobarray=0-4, cpu_per_task=20, credentials = 90 (<100)\n",
    "# SLURM_JOB_ID = get_env_var('SLURM_JOB_ID', 0)\n",
    "# SLURM_ARRAY_TASK_ID = get_env_var('SLURM_ARRAY_TASK_ID', 0)\n",
    "# SLURM_ARRAY_TASK_COUNT = get_env_var('SLURM_ARRAY_TASK_COUNT', 1)\n",
    "\n",
    "SLURM_JOB_ID = 123123123\n",
    "SLURM_ARRAY_TASK_ID = 10\n",
    "SLURM_ARRAY_TASK_COUNT = 500\n",
    "\n",
    "\n",
    "print('SLURM_JOB_ID', SLURM_JOB_ID)\n",
    "print('SLURM_ARRAY_TASK_ID', SLURM_ARRAY_TASK_ID)\n",
    "print('SLURM_ARRAY_TASK_COUNT', SLURM_ARRAY_TASK_COUNT)\n",
    "\n",
    "\n",
    "# ####################################################################################################################################\n",
    "# # loading data\n",
    "# ####################################################################################################################################\n",
    "\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path_to_data='/scratch/spf248/twitter/data/classification/US/'\n",
    "\n",
    "\n",
    "# print('Load Filtered Tweets:')\n",
    "# # filtered contains 8G of data!!\n",
    "# start_time = time.time()\n",
    "\n",
    "# paths_to_filtered=list(np.array_split(\n",
    "#                         # glob(os.path.join(path_to_data,'filtered','*.parquet')),\n",
    "#                         # glob(os.path.join(path_to_data,'filtered_10perct_sample','*.parquet')),\n",
    "#                         glob(os.path.join(path_to_data,'filtered_1perct_sample','*.parquet')),\n",
    "#                         SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID]\n",
    "#                        )\n",
    "# print('#files:', len(paths_to_filtered))\n",
    "\n",
    "# tweets_filtered=pd.DataFrame()\n",
    "# for file in paths_to_filtered:\n",
    "#     print(file)\n",
    "#     tweets_filtered=pd.concat([tweets_filtered,pd.read_parquet(file)[['tweet_id','text']]])\n",
    "#     print(tweets_filtered.shape)\n",
    "\n",
    "#     # break\n",
    "\n",
    "# # tweets_filtered = tweets_filtered[:100]\n",
    "\n",
    "# print('time taken to load keyword filtered sample:', str(time.time() - start_time), 'seconds')\n",
    "# print(tweets_filtered.shape)\n",
    "\n",
    "\n",
    "print('Load Random Tweets:')\n",
    "# random contains 7.3G of data!!\n",
    "start_time = time.time()\n",
    "\n",
    "paths_to_random=list(np.array_split(\n",
    "                        # glob(os.path.join(path_to_data,'random','*.parquet')),\n",
    "                        # glob(os.path.join(path_to_data,'random_10perct_sample','*.parquet')),\n",
    "                        glob(os.path.join(path_to_data,'random_1perct_sample','*.parquet')),\n",
    "                        SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID])\n",
    "print('#files:', len(paths_to_random))\n",
    "\n",
    "tweets_random=pd.DataFrame()\n",
    "for file in paths_to_random:\n",
    "    print(file)\n",
    "    tweets_random=pd.concat([tweets_random,pd.read_parquet(file)[['tweet_id','text']]])\n",
    "    print(tweets_random.shape)\n",
    "\n",
    "    break\n",
    "\n",
    "tweets_random = tweets_random[:100]\n",
    "\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_random.shape)\n",
    "\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec\n",
    "\n",
    "import os\n",
    "print('GENSIM_DATA_DIR', os.environ['GENSIM_DATA_DIR'] )\n",
    "    \n",
    "start_time = time.time()\n",
    "print('loading glove')\n",
    "glove_twitter = api.load(\"glove-twitter-200\")\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')    \n",
    "\n",
    "start_time = time.time()\n",
    "print('calculating embeddings')    \n",
    "random_data_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in tweets_random[\"text\"]]))\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "print('per tweet:', (time.time() - start_time)/tweets_random.shape[0], 'seconds')\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "\n",
    "    print('\\n\\n!!!!!', column)\n",
    "\n",
    "#     start = time.time()\n",
    "#     learner = create_model(column, best_epochs[column])\n",
    "#     print('load model:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "#     print('Predictions of Filtered Tweets:')\n",
    "#     start_time = time.time()\n",
    "#     predictions_filtered = learner.predict_batch(tweets_filtered['text'].values.tolist())\n",
    "#     print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "#     print('per tweet:', (time.time() - start_time)/tweets_filtered.shape[0], 'seconds')\n",
    "\n",
    "#     # In[ ]:\n",
    "\n",
    "    data_path = \"/scratch/da2734/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced/\"\n",
    "    print(\"************ {} ************\".format(column))\n",
    "\n",
    "    train_file_name = \"train_{}.csv\".format(column)\n",
    "    val_file_name = \"val_{}.csv\".format(column)\n",
    "    #download data\n",
    "    df_train = pd.read_csv(os.path.join(data_path, train_file_name))\n",
    "    print(df_train.head())\n",
    "    df_val = pd.read_csv(os.path.join(data_path, val_file_name))\n",
    "    #create embeddings\n",
    "    train_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_train[\"text\"]]))\n",
    "    validation_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_val[\"text\"]]))\n",
    "    #train\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(train_vecs_glove_mean,df_train[\"class\"])\n",
    "    #evaluate\n",
    "    df_val[\"class_predict\"] = clf.predict(validation_vecs_glove_mean)\n",
    "    TP, FP, TN, FN = perf_measure(df_val[\"class\"], df_val[\"class_predict\"])\n",
    "    print(\"Precision: \", TP/(TP+FP))\n",
    "    print(\"Recall: \", TP/(TP+FN))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Predictions of Random Tweets:')\n",
    "    start_time = time.time()\n",
    "    #     predictions_random = learner.predict_batch(tweets_random['text'].values.tolist())\n",
    "    predictions_random = clf.predict_proba(random_data_vecs_glove_mean)\n",
    "\n",
    "    print(type(predictions_random))\n",
    "    # print(predictions_random)\n",
    "\n",
    "    print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "    print('per tweet:', (time.time() - start_time)/tweets_random.shape[0], 'seconds')\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    #     print('Save Predictions of Filtered Tweets:')\n",
    "    #     start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    #     df_filtered = predictions_filtered.set_index(tweets_filtered.tweet_id).rename(columns={\n",
    "    #             '0':'pos_model',\n",
    "    #             '1':'neg_model',\n",
    "    #     })\n",
    "\n",
    "    if not os.path.exists(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column)):\n",
    "        print('>>>> directory doesnt exists')\n",
    "        os.makedirs(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column))\n",
    "\n",
    "    #     # if not os.path.exists(os.path.join(root_path,'pred_output_10pct_sample', column)):\n",
    "    #     #     os.makedirs(os.path.join(root_path,'pred_output_10pct_sample', column))\n",
    "\n",
    "    #     # if not os.path.exists(os.path.join(root_path,'pred_output_full', column)):\n",
    "    #     #     os.makedirs(os.path.join(root_path,'pred_output_full', column))\n",
    "\n",
    "    #     df_filtered.to_csv(\n",
    "    #             # os.path.join(root_path,'pred_output', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "    #             os.path.join(root_path,'pred_output_1pct_sample', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "    #         )\n",
    "\n",
    "    #     print(os.path.join(root_path,'pred_output_1pct_sample', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'), 'saved')\n",
    "\n",
    "    #     print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Save Predictions of Random Tweets:')\n",
    "    start_time = time.time()\n",
    "    # predictions_random_df = pd.DataFrame(data=predictions_random, columns = ['neg', 'pos'])\n",
    "    predictions_random_df = pd.DataFrame(data=predictions_random, columns = ['neg_model', 'pos_model'])\n",
    "    df_random = predictions_random_df.set_index(tweets_random.tweet_id)\n",
    "    # df_random = predictions_random.set_index(tweets_random.tweet_id).rename(columns={\n",
    "    #         '0':'pos_model',\n",
    "    #         '1':'neg_model',\n",
    "    # })\n",
    "\n",
    "    # if not os.path.exists(os.path.join(root_path,'pred_output_10pct_sample', column)):\n",
    "    #     os.makedirs(os.path.join(root_path,'pred_output_10pct_sample', column))\n",
    "\n",
    "    df_random.to_csv(\n",
    "        # os.path.join(root_path,'pred_output', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "        os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "        )\n",
    "\n",
    "    print(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'), 'saved')\n",
    "\n",
    "    print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "    # break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ job_offer ************\n",
      "     id  \\\n",
      "0  3256   \n",
      "1  6090   \n",
      "2  5811   \n",
      "3  1085   \n",
      "4  2383   \n",
      "\n",
      "                                                                                                                                       text  \\\n",
      "0  Just fired up my GPS from 1999. New batteries and in 30 seconds it found a position indoors! http://t.co/gWZXkx2L2w                        \n",
      "1  @MysteryColorist its a fuckin CRIME!! They just charge batts turn the thing on &amp; go. YET STILL GET HIRED FOR SLOPPY WORK!              \n",
      "2  WAIT WHAT ? WHO QUITS THEIR JOB BEFORE THEY GET AN INTERVIEW WITH THE NEXT COMPANY?....                                                    \n",
      "3  Searching For a LinkedIn Marketing Superstar! - oDesk: I am seeking an experience LinkedIn ... http://t.co/tK73BjLJ6H #sales #mktg #jobs   \n",
      "4  Who is hiring???                                                                                                                           \n",
      "\n",
      "   class  \n",
      "0  0      \n",
      "1  0      \n",
      "2  0      \n",
      "3  1      \n",
      "4  0      \n",
      "Precision:  0.8904899135446686\n",
      "Recall:  0.865546218487395\n",
      "Predictions of Random Tweets:\n",
      "<class 'numpy.ndarray'>\n",
      "time taken: 0.0005924701690673828 seconds\n",
      "per tweet: 9.701251983642578e-06 seconds\n",
      "Save Predictions of Random Tweets:\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_1pct_sample_GLOVE/job_offer/random-123123123-10.csv saved\n",
      "time taken: 0.015444278717041016 seconds\n"
     ]
    }
   ],
   "source": [
    "# huh\n",
    "data_path = \"/scratch/da2734/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced/\"\n",
    "print(\"************ {} ************\".format(column))\n",
    "\n",
    "train_file_name = \"train_{}.csv\".format(column)\n",
    "val_file_name = \"val_{}.csv\".format(column)\n",
    "#download data\n",
    "df_train = pd.read_csv(os.path.join(data_path, train_file_name))\n",
    "print(df_train.head())\n",
    "df_val = pd.read_csv(os.path.join(data_path, val_file_name))\n",
    "#create embeddings\n",
    "train_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_train[\"text\"]]))\n",
    "validation_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_val[\"text\"]]))\n",
    "#train\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_vecs_glove_mean,df_train[\"class\"])\n",
    "#evaluate\n",
    "df_val[\"class_predict\"] = clf.predict(validation_vecs_glove_mean)\n",
    "TP, FP, TN, FN = perf_measure(df_val[\"class\"], df_val[\"class_predict\"])\n",
    "print(\"Precision: \", TP/(TP+FP))\n",
    "print(\"Recall: \", TP/(TP+FN))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Predictions of Random Tweets:')\n",
    "start_time = time.time()\n",
    "#     predictions_random = learner.predict_batch(tweets_random['text'].values.tolist())\n",
    "predictions_random = clf.predict_proba(random_data_vecs_glove_mean)\n",
    "\n",
    "print(type(predictions_random))\n",
    "# print(predictions_random)\n",
    "\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "print('per tweet:', (time.time() - start_time)/tweets_random.shape[0], 'seconds')\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#     print('Save Predictions of Filtered Tweets:')\n",
    "#     start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "#     df_filtered = predictions_filtered.set_index(tweets_filtered.tweet_id).rename(columns={\n",
    "#             '0':'pos_model',\n",
    "#             '1':'neg_model',\n",
    "#     })\n",
    "\n",
    "if not os.path.exists(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column)):\n",
    "    print('>>>> directory doesnt exists')\n",
    "    os.makedirs(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column))\n",
    "\n",
    "#     # if not os.path.exists(os.path.join(root_path,'pred_output_10pct_sample', column)):\n",
    "#     #     os.makedirs(os.path.join(root_path,'pred_output_10pct_sample', column))\n",
    "\n",
    "#     # if not os.path.exists(os.path.join(root_path,'pred_output_full', column)):\n",
    "#     #     os.makedirs(os.path.join(root_path,'pred_output_full', column))\n",
    "\n",
    "#     df_filtered.to_csv(\n",
    "#             # os.path.join(root_path,'pred_output', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "#             os.path.join(root_path,'pred_output_1pct_sample', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "#         )\n",
    "\n",
    "#     print(os.path.join(root_path,'pred_output_1pct_sample', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'), 'saved')\n",
    "\n",
    "#     print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Save Predictions of Random Tweets:')\n",
    "start_time = time.time()\n",
    "# predictions_random_df = pd.DataFrame(data=predictions_random, columns = ['neg', 'pos'])\n",
    "predictions_random_df = pd.DataFrame(data=predictions_random, columns = ['neg_model', 'pos_model'])\n",
    "df_random = predictions_random_df.set_index(tweets_random.tweet_id)\n",
    "# df_random = predictions_random.set_index(tweets_random.tweet_id).rename(columns={\n",
    "#         '0':'pos_model',\n",
    "#         '1':'neg_model',\n",
    "# })\n",
    "\n",
    "# if not os.path.exists(os.path.join(root_path,'pred_output_10pct_sample', column)):\n",
    "#     os.makedirs(os.path.join(root_path,'pred_output_10pct_sample', column))\n",
    "\n",
    "df_random.to_csv(\n",
    "    # os.path.join(root_path,'pred_output', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "    os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "    )\n",
    "\n",
    "print(os.path.join(root_path,'pred_output_1pct_sample_GLOVE', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'), 'saved')\n",
    "\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg_model</th>\n",
       "      <th>pos_model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>554664286443028480</th>\n",
       "      <td>0.807784</td>\n",
       "      <td>0.192216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554666350086737920</th>\n",
       "      <td>0.718504</td>\n",
       "      <td>0.281496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554666446463438848</th>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554667234845806592</th>\n",
       "      <td>0.014462</td>\n",
       "      <td>0.985538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554668283765354498</th>\n",
       "      <td>0.878246</td>\n",
       "      <td>0.121754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554797904561332225</th>\n",
       "      <td>0.253242</td>\n",
       "      <td>0.746758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554797921049120768</th>\n",
       "      <td>0.203928</td>\n",
       "      <td>0.796072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554798437204779008</th>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.999479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554798516602564608</th>\n",
       "      <td>0.046232</td>\n",
       "      <td>0.953768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554799397650046976</th>\n",
       "      <td>0.465285</td>\n",
       "      <td>0.534715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    neg_model  pos_model\n",
       "tweet_id                                \n",
       "554664286443028480  0.807784   0.192216 \n",
       "554666350086737920  0.718504   0.281496 \n",
       "554666446463438848  0.999905   0.000095 \n",
       "554667234845806592  0.014462   0.985538 \n",
       "554668283765354498  0.878246   0.121754 \n",
       "...                      ...        ... \n",
       "554797904561332225  0.253242   0.746758 \n",
       "554797921049120768  0.203928   0.796072 \n",
       "554798437204779008  0.000521   0.999479 \n",
       "554798516602564608  0.046232   0.953768 \n",
       "554799397650046976  0.465285   0.534715 \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.215886</td>\n",
       "      <td>0.784114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.389961</td>\n",
       "      <td>0.610039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.005759</td>\n",
       "      <td>0.994241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998955</td>\n",
       "      <td>0.001045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.141725</td>\n",
       "      <td>0.858275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.399320</td>\n",
       "      <td>0.600680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.541336</td>\n",
       "      <td>0.458664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.149652</td>\n",
       "      <td>0.850348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.991346</td>\n",
       "      <td>0.008654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.417493</td>\n",
       "      <td>0.582507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         neg       pos\n",
       "0   0.215886  0.784114\n",
       "1   0.389961  0.610039\n",
       "2   0.005759  0.994241\n",
       "3   0.998955  0.001045\n",
       "4   0.141725  0.858275\n",
       "..       ...       ...\n",
       "95  0.399320  0.600680\n",
       "96  0.541336  0.458664\n",
       "97  0.149652  0.850348\n",
       "98  0.991346  0.008654\n",
       "99  0.417493  0.582507\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=predictions_random, columns = ['neg', 'pos'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
