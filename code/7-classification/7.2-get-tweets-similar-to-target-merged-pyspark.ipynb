{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark-submit --master yarn --deploy-mode cluster  --conf spark.yarn.submit.waitAppCompletion=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.speculation=false --conf spark.executorEnv.LANG=en_US.UTF-8 --conf spark.yarn.appMasterEnv.LANG=en_US.UTF-8 --driver-cores 20 --driver-memory 50G --num-executors 20 --executor-cores 25 --executor-memory 30G pyspark-merge-tweets-similarity-scores.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Time (in sec):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2660.511"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Running Time (in sec):')\n",
    "(1579015886163-1579013225652)/(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import socket\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,dayofmonth,dayofweek,to_timestamp,size,isnan,lower,rand, lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: Samuels-MacBook-Pro.local\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.177:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x112937a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Hostname:', socket.gethostname())\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    if 'samuel' in socket.gethostname().lower():\n",
    "        print('Create Local SparkSession')\n",
    "        spark = SparkSession.builder.config(\n",
    "        \"spark.driver.host\", \"localhost\").appName(\n",
    "        \"merge-tweets-similarity-scores\").getOrCreate()\n",
    "    else:\n",
    "        print('Create Cluster SparkSession')\n",
    "        spark = SparkSession.builder.appName(\n",
    "        \"merge-tweets-similarity-scores\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: US\n",
      "Path to data: ../../data/classification/US\n"
     ]
    }
   ],
   "source": [
    "country_code = \"US\"\n",
    "print('Country:', country_code)\n",
    "\n",
    "# Local\n",
    "if  'samuel' in socket.gethostname().lower():\n",
    "    path_to_data = os.path.join('../../data/classification',country_code)\n",
    "# Cluster\n",
    "else:\n",
    "    path_to_data = os.path.join('/user/spf248/twitter/data/classification',country_code)\n",
    "print('Path to data:',path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import tweets containing keywords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, text: string, fired: boolean, hired: boolean, job: boolean, laid_off: boolean, position: boolean, quit: boolean, unemployed: boolean, work: boolean, keyword: boolean]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import tweets containing keywords')\n",
    "filtered = spark.read.parquet(os.path.join(path_to_data,'filtered'))\n",
    "filtered.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import random tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, text: string, fired: boolean, hired: boolean, job: boolean, laid_off: boolean, position: boolean, quit: boolean, unemployed: boolean, work: boolean, keyword: boolean]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import random tweets')\n",
    "random = spark.read.parquet(os.path.join(path_to_data,'random'))\n",
    "random.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, score: float, target: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import scores')\n",
    "\n",
    "schema = StructType([StructField('tweet_id', StringType(), False),\n",
    "                     StructField('score', FloatType(), False),\n",
    "                     StructField('target', StringType(), False)])\n",
    "\n",
    "scores = spark.read.option('header','true').schema(schema).csv(os.path.join(path_to_data,'similarity'))\n",
    "scores.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop Duplicated Scores (Random Sample Could Contain Keywords)\n"
     ]
    }
   ],
   "source": [
    "print('Drop Duplicated Scores (Random Sample Could Contain Keywords)')\n",
    "scores = scores.drop_duplicates(subset=['tweet_id','target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Tweets With Their Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "\n",
      "I lost my job today\n"
     ]
    }
   ],
   "source": [
    "targets=sorted(scores.select(\"target\").distinct().rdd.map(lambda r: r[0]).collect())\n",
    "print('Targets:\\n')\n",
    "print('\\n'.join(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Tweets With Their Similarity Scores:\n",
      "I lost my job today\n"
     ]
    }
   ],
   "source": [
    "print('Merge Tweets With Their Similarity Scores:')\n",
    "for target in targets:\n",
    "    \n",
    "    print(target)\n",
    "    \n",
    "    filtered=filtered.join(\n",
    "    scores.filter(scores['target']==target).selectExpr(\n",
    "    \"tweet_id\", \n",
    "    \"score as target_\"+target.replace(' ','_').replace('?','').lower()),on='tweet_id')\n",
    "    \n",
    "    random=random.join(\n",
    "    scores.filter(scores['target']==target).selectExpr(\n",
    "    \"tweet_id\", \n",
    "    \"score as target_\"+target.replace(' ','_').replace('?','').lower()),on='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save\n"
     ]
    }
   ],
   "source": [
    "print('Save')\n",
    "filtered.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,'filtered-scored'))\n",
    "random.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,'random-scored'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
