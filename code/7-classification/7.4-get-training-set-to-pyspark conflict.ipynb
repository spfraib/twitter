{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark-submit --master yarn --deploy-mode cluster  --conf spark.yarn.submit.waitAppCompletion=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.speculation=false --conf spark.executorEnv.LANG=en_US.UTF-8 --conf spark.yarn.appMasterEnv.LANG=en_US.UTF-8 --driver-cores 20 --driver-memory 50G --num-executors 30 --executor-cores 20 --executor-memory 20G pyspark-sample-training-set.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Time (in sec):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2590.501"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Computing Time (in sec):')\n",
    "(1579053833248-1579051242747)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import socket\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,desc,row_number,col,year,month,dayofmonth,dayofweek,to_timestamp,size,isnan,lower,rand, lit\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField, FloatType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: Samuels-MacBook-Pro.local\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a9f4a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Hostname:', socket.gethostname())\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    if 'samuel' in socket.gethostname().lower():\n",
    "        print('Create Local SparkSession')\n",
    "        spark = SparkSession.builder.config(\n",
    "        \"spark.driver.host\", \"localhost\").appName(\n",
    "        \"sample-tweets-for-labeling\").getOrCreate()\n",
    "    else:\n",
    "        print('Create Cluster SparkSession')\n",
    "        spark = SparkSession.builder.appName(\n",
    "        \"sample-tweets-for-labeling\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: US\n",
      "Path to data: ../../data/classification/US\n"
     ]
    }
   ],
   "source": [
    "country_code = \"US\"\n",
    "print('Country:', country_code)\n",
    "\n",
    "# Local\n",
    "if  'samuel' in socket.gethostname().lower():\n",
    "    path_to_data = os.path.join('../../data/classification',country_code)\n",
    "# Cluster\n",
    "else:\n",
    "    path_to_data = os.path.join('/user/spf248/twitter/data/classification',country_code)\n",
    "print('Path to data:',path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import tweets containing keywords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, text: string, fired: boolean, hired: boolean, job: boolean, laid_off: boolean, position: boolean, quit: boolean, unemployed: boolean, work: boolean, keyword: boolean, target_anyone_hiring: float, target_here_is_a_job_opportunity_you_might_be_interested_in: float, target_i_am_currently_not_working: float, target_i_am_searching_for_a_new_position: float, target_i_got_hired_today: float, target_i_lost_my_job_today: float, target_i_recently_started_working_at_my_new_job: float, target_i_was_fired_earlier_this_week: float, target_looking_for_a_new_position: float, target_now_i_am_unemployed: float]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import tweets containing keywords')\n",
    "filtered = spark.read.parquet(os.path.join(path_to_data,'filtered-scored'))\n",
    "filtered.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import random tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[tweet_id: string, text: string, fired: boolean, hired: boolean, job: boolean, laid_off: boolean, position: boolean, quit: boolean, unemployed: boolean, work: boolean, keyword: boolean, target_anyone_hiring: float, target_here_is_a_job_opportunity_you_might_be_interested_in: float, target_i_am_currently_not_working: float, target_i_am_searching_for_a_new_position: float, target_i_got_hired_today: float, target_i_lost_my_job_today: float, target_i_recently_started_working_at_my_new_job: float, target_i_was_fired_earlier_this_week: float, target_looking_for_a_new_position: float, target_now_i_am_unemployed: float]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Import random tweets')\n",
    "random = spark.read.parquet(os.path.join(path_to_data,'random-scored'))\n",
    "random.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop duplicated texts\n"
     ]
    }
   ],
   "source": [
    "print('Drop duplicated texts')\n",
    "filtered = filtered.drop_duplicates(subset=['text'])\n",
    "random = random.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:\n",
      "\n",
      "fired\n",
      "hired\n",
      "job\n",
      "laid_off\n",
      "position\n",
      "quit\n",
      "unemployed\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "keywords=sorted([keyword for keyword in filtered.columns \n",
    "                 if keyword not in ['tweet_id','text','keyword'] \n",
    "                 and 'target_' not in keyword])\n",
    "print('Keywords:\\n')\n",
    "print('\\n'.join(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "\n",
      "target_anyone_hiring\n",
      "target_here_is_a_job_opportunity_you_might_be_interested_in\n",
      "target_i_am_currently_not_working\n",
      "target_i_am_searching_for_a_new_position\n",
      "target_i_got_hired_today\n",
      "target_i_lost_my_job_today\n",
      "target_i_recently_started_working_at_my_new_job\n",
      "target_i_was_fired_earlier_this_week\n",
      "target_looking_for_a_new_position\n",
      "target_now_i_am_unemployed\n"
     ]
    }
   ],
   "source": [
    "targets=sorted([target for target in filtered.columns if 'target_' in target])\n",
    "print('Targets:\\n')\n",
    "print('\\n'.join(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sample For Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sampled tweets per group: 100\n",
      "# labels: 9800\n"
     ]
    }
   ],
   "source": [
    "n_sample=100\n",
    "n_labels=n_sample*((len(targets)+1)*(len(keywords)+1)-1)\n",
    "print('# sampled tweets per group:', n_sample)\n",
    "print('# labels:', n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Sample for Labeling\n"
     ]
    }
   ],
   "source": [
    "print('Create Sample for Labeling')\n",
    "sampled_ids=[]\n",
    "sampled_tweets=[]\n",
    "sampled_keywords=[]\n",
    "sampled_targets=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fired\n",
      "hired\n",
      "job\n",
      "laid_off\n",
      "position\n",
      "quit\n",
      "unemployed\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "    \n",
    "    print(keyword)\n",
    "    \n",
    "    # Take Random Sample of Size n_sample from Non-sampled Tweets Containing Specific Keyword\n",
    "    tmp=filtered.where(\n",
    "    (filtered[keyword]==True)&(~filtered['tweet_id'].isin(sampled_ids))).sample(\n",
    "    False,0.1,seed=0).limit(n_sample).select('tweet_id','text').rdd.map(lambda x: (x[0],x[1])).collect()\n",
    "    \n",
    "    # Keep Track of Sampled Ids\n",
    "    sampled_ids.extend(list(list(zip(*tmp))[0]))\n",
    "    \n",
    "    # Keep Track of Sampled Tweets\n",
    "    sampled_tweets.extend(list(list(zip(*tmp))[1]))\n",
    "    \n",
    "    # Keep Track of Sampling Properties\n",
    "    sampled_keywords.extend([keyword]*len(tmp))\n",
    "    sampled_targets.extend(['random']*len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fired\n",
      "target_anyone_hiring\n",
      "\n",
      "hired\n",
      "target_anyone_hiring\n",
      "\n",
      "job\n",
      "target_anyone_hiring\n",
      "\n",
      "laid_off\n",
      "target_anyone_hiring\n",
      "\n",
      "position\n",
      "target_anyone_hiring\n",
      "\n",
      "quit\n",
      "target_anyone_hiring\n",
      "\n",
      "unemployed\n",
      "target_anyone_hiring\n",
      "\n",
      "work\n",
      "target_anyone_hiring\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "\n",
    "    print(keyword)\n",
    "    \n",
    "    for target in targets:\n",
    "        \n",
    "        print(target)\n",
    "\n",
    "        # Take n_sample Tweets Most Similar with Target Sentence From Non-sampled Tweets Containing Specific Keyword\n",
    "        tmp=filtered.where(\n",
    "        (filtered[keyword]==True)&(~filtered['tweet_id'].isin(sampled_ids))).orderBy(\n",
    "        desc(target)).limit(n_sample).select('tweet_id','text').rdd.map(lambda x: (x[0],x[1])).collect()\n",
    "    \n",
    "        # Keep Track of Sampled Ids\n",
    "        sampled_ids.extend(list(list(zip(*tmp))[0]))\n",
    "\n",
    "        # Keep Track of Sampled Tweets\n",
    "        sampled_tweets.extend(list(list(zip(*tmp))[1]))\n",
    "\n",
    "        # Keep Track of Sampling Properties\n",
    "        sampled_keywords.extend([keyword]*len(tmp))\n",
    "        sampled_targets.extend([target]*len(tmp))\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_anyone_hiring\n",
      "target_here_is_a_job_opportunity_you_might_be_interested_in\n",
      "target_i_am_currently_not_working\n",
      "target_i_am_searching_for_a_new_position\n",
      "target_i_got_hired_today\n",
      "target_i_lost_my_job_today\n",
      "target_i_recently_started_working_at_my_new_job\n",
      "target_i_was_fired_earlier_this_week\n",
      "target_looking_for_a_new_position\n",
      "target_now_i_am_unemployed\n"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    \n",
    "    print(target)\n",
    "    \n",
    "    # Take n_sample from Non-sampled Tweets Most Similar with Target Sentence\n",
    "    tmp=random.where(~random['tweet_id'].isin(sampled_ids)).orderBy(\n",
    "    desc(target)).limit(n_sample).select('tweet_id','text').rdd.map(lambda x: (x[0],x[1])).collect()\n",
    "    \n",
    "    # Keep Track of Sampled Ids\n",
    "    sampled_ids.extend(list(list(zip(*tmp))[0]))\n",
    "\n",
    "    # Keep Track of Sampled Tweets\n",
    "    sampled_tweets.extend(list(list(zip(*tmp))[1]))\n",
    "\n",
    "    # Keep Track of Sampling Properties\n",
    "    sampled_keywords.extend(['random']*len(tmp))\n",
    "    sampled_targets.extend([target]*len(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Save')\n",
    "\n",
    "tweets_for_labeling = spark.createDataFrame(\n",
    "zip(sampled_ids, sampled_tweets, sampled_keywords, sampled_targets), \n",
    "schema=['tweet_id', 'text', 'keyword', 'target'])\n",
    "\n",
    "tweets_for_labeling.write.mode(\"overwrite\").parquet(os.path.join(path_to_data,'labeling'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
