{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://holoviews.org/user_guide/Network_Graphs.html\n",
    "    \n",
    "https://pyvis.readthedocs.io/en/latest/tutorial.html\n",
    "    \n",
    "https://towardsdatascience.com/python-interactive-network-visualization-using-networkx-plotly-and-dash-e44749161ed7\n",
    "    \n",
    "https://github.com/tweepy/tweepy/issues/627\n",
    "    \n",
    "https://blog.f-secure.com/how-to-get-twitter-follower-data-using-python-and-tweepy/\n",
    "    \n",
    "https://stackoverflow.com/questions/31000178/how-to-get-large-list-of-followers-tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import sys\n",
    "import socket\n",
    "import uuid\n",
    "from glob import glob\n",
    "import json\n",
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 10000\n",
    "print('Save Data After Downloading',cutoff,'Timelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes=[\n",
    "'US',\n",
    "'ID',\n",
    "'BR',\n",
    "'TR',\n",
    "'MX',\n",
    "'AR',\n",
    "'PH',\n",
    "'CO',\n",
    "'MY',\n",
    "'VE',\n",
    "'TH',\n",
    "'PK',\n",
    "]\n",
    "\n",
    "country_code = \"US\"\n",
    "print('Country:', country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_var(varname,default):\n",
    "    \n",
    "    if os.environ.get(varname) != None:\n",
    "        var = int(os.environ.get(varname))\n",
    "        print(varname,':', var)\n",
    "    else:\n",
    "        var = default\n",
    "        print(varname,':', var,'(Default)')\n",
    "    return var\n",
    "\n",
    "# Choose Number of Nodes To Distribute Credentials: e.g. jobarray=0-4, cpu_per_task=20, credentials = 90 (<100)\n",
    "SLURM_JOB_ID            = get_env_var('SLURM_JOB_ID',0)\n",
    "SLURM_ARRAY_TASK_ID     = get_env_var('SLURM_ARRAY_TASK_ID',0)\n",
    "SLURM_ARRAY_TASK_COUNT  = get_env_var('SLURM_ARRAY_TASK_COUNT',1)\n",
    "SLURM_JOB_CPUS_PER_NODE = get_env_var('SLURM_JOB_CPUS_PER_NODE',mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'samuel' in socket.gethostname().lower():\n",
    "    path_to_data='../../data'\n",
    "else:\n",
    "    path_to_data='/scratch/spf248/twitter/data'\n",
    "\n",
    "path_to_keys = os.path.join(path_to_data,'keys','twitter')\n",
    "path_to_users = os.path.join(path_to_data,'users')\n",
    "path_to_locations = os.path.join(path_to_data,'locations','profiles')\n",
    "path_to_friends = os.path.join(path_to_data,'friends','API',country_code)\n",
    "os.makedirs(path_to_friends, exist_ok=True)\n",
    "print(path_to_keys)\n",
    "print(path_to_users)\n",
    "print(path_to_locations)\n",
    "print(path_to_friends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_files(SLURM_ARRAY_TASK_ID,SLURM_ARRAY_TASK_COUNT,SLURM_JOB_CPUS_PER_NODE):\n",
    "\n",
    "    # Randomize set of key files using constant seed\n",
    "    np.random.seed(0)\n",
    "    all_key_files = np.random.permutation(glob(os.path.join(path_to_keys,'*.json')))\n",
    "    \n",
    "    # Split file list by node\n",
    "    key_files = np.array_split(all_key_files,SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID]\n",
    "    \n",
    "    # Check that node has more CPU than key file \n",
    "    if len(key_files) <= SLURM_JOB_CPUS_PER_NODE:\n",
    "        print('# Credentials Allocated To Node:', len(key_files)) \n",
    "    else:\n",
    "        print('# Credentials (',len(key_files),') > # CPU (', SLURM_JOB_CPUS_PER_NODE,')')\n",
    "        print('Only Keeping', SLURM_JOB_CPUS_PER_NODE, 'Credentials')\n",
    "        key_files = key_files[:SLURM_JOB_CPUS_PER_NODE]\n",
    "        \n",
    "    return key_files\n",
    "\n",
    "key_files = get_key_files(SLURM_ARRAY_TASK_ID,SLURM_ARRAY_TASK_COUNT,SLURM_JOB_CPUS_PER_NODE)\n",
    "print('\\n'.join(key_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auth(key_file):\n",
    "    \n",
    "    # Import Key\n",
    "    with open(key_file) as f:\n",
    "        key = json.load(f)\n",
    "\n",
    "    # OAuth process, using the keys and tokens\n",
    "    auth = tweepy.OAuthHandler(key['consumer_key'], key['consumer_secret'])\n",
    "    auth.set_access_token(key['access_token'], key['access_token_secret'])\n",
    "\n",
    "    # Creation of the actual interface, using authentication\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    try:\n",
    "        api.verify_credentials()\n",
    "#         print(key_file,\": Authentication OK\")\n",
    "    except:\n",
    "        print(key_file,\": error during authentication\")\n",
    "    \n",
    "    return api\n",
    "\n",
    "for key_file in np.random.permutation(glob(os.path.join(path_to_keys,'*.json'))):\n",
    "    get_auth(key_file)\n",
    "print('Credentials Checked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Import Users By Account Locations')\n",
    "start = timer()\n",
    "\n",
    "l = []\n",
    "for filename in sorted(glob(os.path.join(path_to_users,'user-ids-by-account-location-verified/*.json'))):\n",
    "    try:\n",
    "        df = pd.read_json(filename,lines=True)\n",
    "        l.append(df)\n",
    "    except:\n",
    "        print('error importing', filename)\n",
    "        \n",
    "users_by_account_location=pd.concat(l, axis=0, ignore_index=True)\n",
    "users_by_account_location=users_by_account_location.set_index('user_location')['user_id']\n",
    "users_by_account_location=users_by_account_location.apply(eval).apply(lambda x:[str(y) for y in x])\n",
    "print('# Locations:', len(users_by_account_location))\n",
    "print('# Users:', users_by_account_location.apply(len).sum())\n",
    "\n",
    "end = timer()\n",
    "print('Computing Time:', round(end - start), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_by_account_location.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Import Locations')\n",
    "user_locations=pd.read_csv(os.path.join(path_to_locations,'user_locations_geocoded.csv'),index_col=0) \n",
    "print('# Locations:', len(user_locations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Select Users...')\n",
    "start = timer()\n",
    "\n",
    "# Sorted list of users in selected countries\n",
    "users=users_by_account_location.reindex(user_locations.loc[user_locations['country_short']==country_code,'user_location']).dropna().explode().reset_index(drop=True)\n",
    "\n",
    "# Randomize users\n",
    "users=users.sample(frac=1,random_state=0)\n",
    "\n",
    "del users_by_account_location\n",
    "del user_locations\n",
    "\n",
    "print('# Users :', len(users)) \n",
    "print('First user:', users.index[0])\n",
    "\n",
    "end = timer()\n",
    "print('Computing Time:', round(end - start), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Split Users Across Nodes...')\n",
    "start = timer()\n",
    "\n",
    "users=np.array_split(users,SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID]\n",
    "print('# Users for this node:', len(users)) \n",
    "print('First user for this node:', users.index[0])\n",
    "\n",
    "end = timer()\n",
    "print('Computing Time:', round(end - start), 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Remove Existing Users:')\n",
    "start = timer()\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_friends,'success')):\n",
    "    existing_users=set(pd.read_csv(os.path.join(path_to_friends,'success'),names=['user_id','filename'],dtype='str',sep='\\t')['user_id'])\n",
    "    users=set(users).difference(existing_users)\n",
    "    \n",
    "np.random.seed(0)\n",
    "users=np.random.permutation(list(users))\n",
    "print('# Remaining Users:', len(users))\n",
    "\n",
    "end = timer()\n",
    "print('Computing Time:', round(end - start), 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friends_ids(api,user_id):\n",
    "    \n",
    "    friends = []\n",
    "\n",
    "    try:\n",
    "        cursor = tweepy.Cursor(api.friends_ids, user_id=user_id, count=5000).items()\n",
    "        for friend in cursor:\n",
    "            friends.append(friend)\n",
    "        return friends\n",
    "    \n",
    "    except tweepy.error.TweepError as e:\n",
    "        print(e)\n",
    "        with open(os.path.join(path_to_friends,'errors'), 'a', encoding='utf-8') as file:\n",
    "            file.write(user_id+'\\tfriends_ids\\terror\\t'+str(e)+'\\n')\n",
    "\n",
    "friends = friends_ids(get_auth(key_file),user_id=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_by_block(index_key):\n",
    "\n",
    "    # Create Access For Block of Users\n",
    "    api = get_auth(key_files[index_key])\n",
    "    \n",
    "    # Select Block of Users\n",
    "    users_block = np.array_split(users,len(key_files))[index_key]\n",
    "    \n",
    "    # Initialize Output File ID\n",
    "    output_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Initialize DataFrame\n",
    "    users_friends = pd.DataFrame()\n",
    "    \n",
    "    # Initialize Downloaded User List\n",
    "    downloaded_ids = []\n",
    "    counter_ids = 0\n",
    "    \n",
    "    for i,user_id in enumerate(users_block):\n",
    "        \n",
    "        # Try Downloading Friends\n",
    "        friends=friends_ids(api,user_id)\n",
    "            \n",
    "        if friends==None:\n",
    "            print('Error:', user_id)\n",
    "            continue\n",
    "            \n",
    "        # Append\n",
    "        users_friends = pd.concat([users_friends,pd.DataFrame([(user_id,friends)],columns=['user_id','friends'])],sort=False)\n",
    "        downloaded_ids.append(user_id)\n",
    "            \n",
    "        # Save after <cutoff> timelines or when reaching last user\n",
    "        if len(downloaded_ids) == cutoff or user_id == users_block[-1][0]:\n",
    "            \n",
    "            counter_ids += len(downloaded_ids)\n",
    "            \n",
    "            filename = \\\n",
    "            'friends-'+\\\n",
    "            str(SLURM_JOB_ID)+'-'+\\\n",
    "            str(SLURM_ARRAY_TASK_ID)+'-'+\\\n",
    "            str(index_key)+'-'+\\\n",
    "            str(len(downloaded_ids))+'-'+\\\n",
    "            output_id+'.json.bz2'\n",
    "            \n",
    "            print('Process', index_key, 'downloaded', counter_ids, 'friends list with most recent output file:', \n",
    "            os.path.join(path_to_friends,filename))\n",
    "            \n",
    "            # Save as list of dict discarding index\n",
    "            users_friends.to_json(os.path.join(path_to_friends,filename),orient='records')\n",
    "            \n",
    "            # Save User Id and File In Which Its Timeline Was Saved\n",
    "            with open(os.path.join(path_to_friends,'success'), 'a', encoding='utf-8') as file:\n",
    "                for downloaded_id in downloaded_ids:\n",
    "                    file.write(downloaded_id+'\\t'+filename+'\\n')\n",
    "            \n",
    "            # Reset Output File ID, Data, and Downloaded Users\n",
    "            del users_friends, downloaded_ids\n",
    "            output_id = str(uuid.uuid4())\n",
    "            users_friends = pd.DataFrame()\n",
    "            downloaded_ids = []\n",
    "            \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Extract Data By Block...\\n')\n",
    "start = timer()\n",
    "\n",
    "with mp.Pool() as pool:\n",
    "    \n",
    "    pool.map(get_data_by_block, range(len(key_files)))\n",
    "\n",
    "end = timer()\n",
    "print('Computing Time:', round(end - start), 'sec')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
