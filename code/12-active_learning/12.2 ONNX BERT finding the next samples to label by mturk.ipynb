{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure you have all libraries installed. \n",
    "use conda environment: /scratch/da2734/twitter/worldbank_twitter_environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading filtered samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to load filtered sample: 386.1313638687134 seconds\n",
      "(92121093, 11)\n"
     ]
    }
   ],
   "source": [
    "# filtered contains 0.8G of data!!\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "country_code = 'US'\n",
    "month = '2012-1'\n",
    "path_to_data = '/scratch/spf248/twitter/data/classification/US/filtered/'\n",
    "tweets_filtered=pq.ParquetDataset(glob(os.path.join(path_to_data,                                           \n",
    "#                                            country_code,\n",
    "#                                            month,\n",
    "                                           '*.parquet'))).read().to_pandas()\n",
    "tweets_filtered['tweet_id'] = tweets_filtered['tweet_id'].astype(int)\n",
    "print('time taken to load filtered sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop columns: 5.684360504150391 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tweets_filtered = tweets_filtered[['tweet_id', 'text']]\n",
    "print('drop columns:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276933934124765184</td>\n",
       "      <td>Damn i have to much homework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>277143632232992768</td>\n",
       "      <td>Does a bedazzler work on leather? Serious ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>277150490276540416</td>\n",
       "      <td>RT @porcelain10: Washington Post D Milbank bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>277151157208637440</td>\n",
       "      <td>Finally off work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277175194454462465</td>\n",
       "      <td>Irrational: No BioShock PS Vita until Sony and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text\n",
       "0  276933934124765184                       Damn i have to much homework\n",
       "1  277143632232992768  Does a bedazzler work on leather? Serious ques...\n",
       "2  277150490276540416  RT @porcelain10: Washington Post D Milbank bas...\n",
       "3  277151157208637440                                   Finally off work\n",
       "4  277175194454462465  Irrational: No BioShock PS Vita until Sony and..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to load random sample: 194.6565079689026 seconds\n",
      "(92114009, 11)\n"
     ]
    }
   ],
   "source": [
    "# random contains 0.8G of data!!\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "country_code = 'US'\n",
    "month = '2012-1'\n",
    "path_to_data = '/scratch/spf248/twitter/data/classification/US/random/'\n",
    "tweets_random=pq.ParquetDataset(glob(os.path.join(path_to_data,                                           \n",
    "#                                            country_code,\n",
    "#                                            month,\n",
    "                                           '*.parquet'))).read().to_pandas()\n",
    "tweets_random['tweet_id'] = tweets_random['tweet_id'].astype(int)\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_random.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop columns: 5.201020002365112 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tweets_random = tweets_random[['tweet_id', 'text']]\n",
    "print('drop columns:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>367881326273105920</td>\n",
       "      <td>@shoebydoo32 I only left to go back home for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367881326281519105</td>\n",
       "      <td>oh my god bina id idnt read that u played GTA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367882185916702722</td>\n",
       "      <td>I Have To Make What I Think Is The Best Decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367883553121394690</td>\n",
       "      <td>@elizrod_ that's from hard work ðŸ˜‰</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>367883867719348224</td>\n",
       "      <td>RT @MileenaSucks: Can I just lay out in the gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text\n",
       "0  367881326273105920  @shoebydoo32 I only left to go back home for t...\n",
       "1  367881326281519105  oh my god bina id idnt read that u played GTA ...\n",
       "2  367882185916702722  I Have To Make What I Think Is The Best Decisi...\n",
       "3  367883553121394690                  @elizrod_ that's from hard work ðŸ˜‰\n",
       "4  367883867719348224  RT @MileenaSucks: Can I just lay out in the gr..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_random.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " is_unemployed\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/is_unemployed/\n",
      "time taken to load filtered sample: 80.48429036140442 seconds (75999592, 3)\n",
      "drop columns: 0.5115442276000977 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tweets_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2423abeae079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'is_unemployed'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmerged_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tweet_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time taken to merge:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seconds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "# outputting 1M (pinky finger up) tweets by column\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print('\\n\\n', column)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/{}/'.format(column)\n",
    "    print(model_output_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_output_filtered = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'filtered*.csv')], ignore_index = True)\n",
    "    print('time taken to load filtered sample:', str(time.time() - start_time), 'seconds', model_output_filtered.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_filtered = model_output_filtered[['tweet_id', 'second']]\n",
    "    model_output_filtered.columns = ['tweet_id', column]\n",
    "    print('drop columns:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "\n",
    "    if column == 'is_unemployed':\n",
    "        start_time = time.time()\n",
    "        merged_filtered = pd.merge(model_output_filtered, tweets_filtered, how='inner', on = 'tweet_id')\n",
    "        print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        merged_filtered = pd.merge(model_output_filtered, merged_filtered, how='inner', on = 'tweet_id')\n",
    "        print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "            \n",
    "    print(merged_filtered.tail(10))\n",
    "            \n",
    "#     break\n",
    "    \n",
    "    \n",
    "start_time = time.time()\n",
    "merged_filtered = merged_filtered.drop_duplicates(['text'])\n",
    "print('drop dup:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "\n",
    "start_time = time.time()\n",
    "pickle.dump( merged_filtered, open( \"./ONNXturk100M/ALL_ONNX_BERT_ST_merged_filtered_100m_jun24.pkl\", \"wb\" ) )\n",
    "print('time taken to dump:', str(time.time() - start_time), 'seconds')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# merging filtered tweets and taking top 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boundary_list [0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.0]\n",
      "\n",
      "\n",
      " is_unemployed\n",
      "\n",
      "\n",
      " lost_job_1mo\n",
      "\n",
      "\n",
      " job_search\n",
      "\n",
      "\n",
      " is_hired_1mo\n",
      "\n",
      "\n",
      " job_offer\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/job_offer/\n",
      "time taken to load filtered sample: 29.686917066574097 seconds (75198670, 3)\n",
      "drop columns: 0.5181810855865479 seconds\n",
      "time taken to merge: 93.91530275344849 seconds (75198670, 3)\n",
      "drop dup: 116.30996441841125 seconds (65461336, 3)\n",
      "sort: 46.12218117713928 seconds (65461336, 3)\n",
      "top 10M truncate: 0.0006086826324462891 seconds (65461336, 3)\n",
      "                    tweet_id    second  \\\n",
      "52995136  425300563577044992  0.001509   \n",
      "36788608  797919678882402305  0.001508   \n",
      "18997625  888857163732590592  0.001508   \n",
      "25206348  800899324926427136  0.001508   \n",
      "21249400  690004041087213571  0.001508   \n",
      "48565908  754686475032297474  0.001508   \n",
      "30178463  371150526143950848  0.001508   \n",
      "27092336  645982618584924160  0.001508   \n",
      "59052375  577320419453243392  0.001508   \n",
      "70549732  360630225983377409  0.001508   \n",
      "40881255  360031807791566849  0.001508   \n",
      "50564790  402858658813317120  0.001507   \n",
      "60874169  290224630688526336  0.001507   \n",
      "7157297   726455961066426368  0.001507   \n",
      "31219739  374606251046338560  0.001507   \n",
      "70975876  558773647582572545  0.001506   \n",
      "66945484  629258284252315648  0.001506   \n",
      "65793139  576003843902685185  0.001506   \n",
      "21399449  320765761436143617  0.001505   \n",
      "62318191  654777113740886016  0.001505   \n",
      "29062067  359365181849088000  0.001505   \n",
      "2968327   591296255625666560  0.001505   \n",
      "74422124  747212560684855296  0.001504   \n",
      "30352115  630794913882484736  0.001504   \n",
      "71977985  863556417361457153  0.001504   \n",
      "3875681   334771750304874498  0.001503   \n",
      "32215048  409893752501964800  0.001503   \n",
      "58866070  756992666852663296  0.001503   \n",
      "1265805   498694357189668864  0.001503   \n",
      "33517377  497838098529542144  0.001502   \n",
      "30079603  291403670619312128  0.001502   \n",
      "40375899  342480411571650560  0.001502   \n",
      "31578404  502647929166131202  0.001501   \n",
      "14895221  799471487317065729  0.001501   \n",
      "22068057  856646234433400832  0.001500   \n",
      "5752759   432614804771962881  0.001500   \n",
      "4173892   586255779818242048  0.001499   \n",
      "7980218   357696184577564672  0.001498   \n",
      "54075184  431224267296866304  0.001498   \n",
      "16892639  407324724679544833  0.001498   \n",
      "13050075  342969794536034304  0.001498   \n",
      "39146971  348529713339318273  0.001497   \n",
      "48061562  380177557619879936  0.001496   \n",
      "20378748  646801828999507969  0.001496   \n",
      "34292508  420745129784115200  0.001495   \n",
      "68714840  649740429605253121  0.001495   \n",
      "57553383  370626628113543168  0.001494   \n",
      "74722776  465998755008098304  0.001494   \n",
      "68630588  419371642372882432  0.001492   \n",
      "12787647  538823792932290560  0.001490   \n",
      "\n",
      "                                                       text  \n",
      "52995136  Can I just sleep all day and not unpack and no...  \n",
      "36788608  ðŸ˜‚ I hate when I'm so hyper I can't even focus ...  \n",
      "18997625  So annoying when I'm so up in my own head that...  \n",
      "25206348  I'll be trying to power nap then I remember I ...  \n",
      "21249400              I be at work sleepy Af wanting to nap  \n",
      "48565908  Like idk cause I spazz to easily &amp; im not ...  \n",
      "30178463  I work early and i dont wanna be yawning all t...  \n",
      "27092336  I also lost track of time and couldn't shower ...  \n",
      "59052375  I could stay up for hours on my phone and not ...  \n",
      "70549732                Im trying to drink before work haha  \n",
      "40881255  I hate when I work and I be sleepy cause I jus...  \n",
      "50564790  Need to stop smokin before work shit gets me a...  \n",
      "60874169  But every time I try to be productive (do some...  \n",
      "7157297    I'm trying to cuddle and watch movies after work  \n",
      "31219739  It's so hard for me to concentrate on my homew...  \n",
      "70975876  Every time I start homework, I get super sleep...  \n",
      "66945484          I'm at work trying not to fall asleep lol  \n",
      "65793139  I love working out but I hate how tired I get ...  \n",
      "21399449  Dont need a workout partner cause ill get dist...  \n",
      "62318191  I just wanna cuddle and watch movies and drink...  \n",
      "29062067        My eyes don't work so I can't concentrate ðŸ‘€  \n",
      "2968327   I gotta clean but I wanna cuddle with my baby ...  \n",
      "74422124  I need to start drinking and working out rathe...  \n",
      "30352115  I'm really tryna cuddle and watch movies when ...  \n",
      "71977985           Tryna get beers and snug after work lmao  \n",
      "3875681   After a hard workout I get really hyper and th...  \n",
      "32215048  I must be eating candy or snack on something t...  \n",
      "58866070                    Im dumb tired smh work flow tho  \n",
      "1265805   I can't drink on Saturdays cuz Sunday night at...  \n",
      "33517377  I need to shower and clean my room before work...  \n",
      "30079603  I should start working out in the mornings but...  \n",
      "40375899  I was asleep this time last night but I still ...  \n",
      "31578404  I still have to shower and do my homework but ...  \n",
      "14895221  I wanna cuddle tonight but shawty gotta go to ...  \n",
      "22068057               I wanna workout but I feel like shit  \n",
      "5752759        I need to workout and not lose my motivation  \n",
      "4173892   I just haven't been eating bc I work all the time  \n",
      "7980218   I want to workout but i have no self motivatio...  \n",
      "54075184  I didn't workout this morning and I was mad al...  \n",
      "16892639                Workout got me lightheaded and shit  \n",
      "13050075       I can be at work and still eat and chill lol  \n",
      "39146971                  Working and napping im sleepy lol  \n",
      "48061562  I need to workout more often tho cause I got weak  \n",
      "20378748                   I'm grouchy when I have homework  \n",
      "34292508  Hate being sick and not being able to workout ...  \n",
      "68714840  I wish I could get high and watch cartoons all...  \n",
      "57553383  My parents and I are lazy and don't workout an...  \n",
      "74722776  I just like working out after showers and nake...  \n",
      "68630588  I gotta workout more\\nI haven't  done shit and...  \n",
      "12787647  I always get really tired and start yawning ri...  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d4e4d292ea76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmerged_filtered_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"./ONNXturk100M/{}_ONNX_BERT_ST_merged_filtered_100m_jun22.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time taken to dump:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;31m# Pickle Methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_attributes_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# outputting 1M (pinky finger up) tweets by column\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "boundary_list = [i/20.0 for i in range(11)]\n",
    "boundary_list.reverse()\n",
    "print('boundary_list', boundary_list)\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print('\\n\\n', column)\n",
    "    \n",
    "    if column != 'job_offer': continue\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/{}/'.format(column)\n",
    "    print(model_output_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_output_filtered = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'filtered*.csv')], ignore_index = True)\n",
    "    print('time taken to load filtered sample:', str(time.time() - start_time), 'seconds', model_output_filtered.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_filtered = model_output_filtered[['tweet_id', 'second']]\n",
    "    print('drop columns:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    merged_filtered = pd.merge(model_output_filtered, tweets_filtered, how='inner', on = 'tweet_id')\n",
    "    print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "\n",
    "#     print(merged_filtered.head())\n",
    "\n",
    "    start_time = time.time()\n",
    "    merged_filtered = merged_filtered.drop_duplicates(['text'])\n",
    "    print('drop dup:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    merged_filtered = merged_filtered.sort_values(by=['second'], ascending=False)\n",
    "    print('sort:', str(time.time() - start_time), 'seconds', merged_filtered.shape)    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    merged_filtered_temp = merged_filtered[:30000000]\n",
    "    print('top 10M truncate:', str(time.time() - start_time), 'seconds', merged_filtered.shape)   \n",
    "    \n",
    "    print(merged_filtered.tail(50))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pickle.dump( merged_filtered_temp, open( \"./ONNXturk100M/{}_ONNX_BERT_ST_merged_filtered_100m_jun22.pkl\".format(column), \"wb\" ) )\n",
    "    print('time taken to dump:', str(time.time() - start_time), 'seconds')\n",
    "        \n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22519252    #nurse #jobs Charge Entry Specialist PRN, Midw...\n",
      "67533711    â˜… JOB â˜… #hiring #job # Orlando - MED SURG Nurs...\n",
      "61238633    #Sales #Job in #Murfreesboro, TN: Event Specia...\n",
      "46007175    #Job Administrative Assistant II / Receptionis...\n",
      "47273325    #Job #Spokane Lead General Engineer: Spokane V...\n",
      "21470115    #CareerArc #Retail #Job alert: Prepared Food C...\n",
      "57759454    #internship #Job alert: Accountant- Entry Leve...\n",
      "13895362    We're #hiring! Click to apply: Shift Superviso...\n",
      "49103383    Can you recommend anyone for this #job? Physic...\n",
      "38065965    Social Media &amp; Community Management Intern...\n",
      "36577254    #Gadsden #Jobs Shift Leader: Boaz Description*...\n",
      "55358699    24 Seven #Marketing #Job: Brand (Product) Mana...\n",
      "15203445    #Job Alert: #Hawaii #Jobs GENERAL OFFICE CLERK...\n",
      "3743242     JobsinBoston_ #Boston #Job Data Entry Coordina...\n",
      "64953159    We're #hiring! Click to apply: CDL-A Team Comp...\n",
      "22480598    Nintendo #Marketing #Job: Assistant Manager, R...\n",
      "38957127    This #Sales #Job might be a great fit for you:...\n",
      "68074175    We're #hiring! Click to apply: Housekeeping P....\n",
      "47321233    New #job opening at York Memorial Hospital in ...\n",
      "74055174    This #Hospitality #job might be a great fit fo...\n",
      "14344753    Can you recommend anyone for this #job? Passen...\n",
      "50831437    Soliant Health #Healthcare #Job: Permanent Dir...\n",
      "13676826    Can you recommend anyone for this #job? Financ...\n",
      "52969798    Construction Jobs - Now Hiring Licensed Journe...\n",
      "21715650    Oracle #Sales #Job: Hardware Sales Representat...\n",
      "26499080    #Job #Columbus (USA-OH-COLUMBUS) Fundraising C...\n",
      "29943109    #CareerArc #Clerical #Job alert: Receptionist ...\n",
      "22519928    Los Angeles Jobs $ Data Management - Process R...\n",
      "58781773    â˜… JOB â˜… #hiring #job #Mobile - Over-the-Road (...\n",
      "22838018    Customer Service Associate Weekend Salesfloor ...\n",
      "Name: text, dtype: object\n",
      "69313114    RT @DertyWorkGlou: #ASUvsFVSUWeekend \\nFRI FEB...\n",
      "8076750     Filming quite the controversial YT video today...\n",
      "2313639     RT @artiequitter: Sept 14th I'll be at NOMADS ...\n",
      "8015093     RT @rhccools: Exciting stuff!! Introduction to...\n",
      "37158773    Hello everyone, have a great Saturday! looking...\n",
      "1669227     RT @DGilliesNetwork: Sending best wishes to th...\n",
      "4837742     RT @WTSP10News: Holiday? Quarter of American w...\n",
      "8331482     The appreciation we get from our customers mak...\n",
      "37785433    @e1n The network I'm on is blocking the link! ...\n",
      "26107330    So im about to go see if i get the job (: \\nI ...\n",
      "31206107    Always a pleasure working with #MattSorum and ...\n",
      "68938250    Hillary says there's 11 Million Gainfully Empl...\n",
      "61913829    Last time I checked YOU have a job because of ...\n",
      "15726767    Double Time Blowjob Fun: #head Double Time Blo...\n",
      "4567156     RT @FHSGlads: Another Thomas sighting. Excelle...\n",
      "30890402    RT @SplatFennec: Ahhhhhh first commission is c...\n",
      "34294207    UAW expects to be in VW's Tennessee plant by J...\n",
      "57899858    Check out this great artwork by Kerri  Blackma...\n",
      "16946051    Clean up your #network with #virtualization fr...\n",
      "45152510    RT @joshsroka: $5000 (plus 8 games) is the cos...\n",
      "52949790    Wake up from #winter with #Flourishin15 #yogat...\n",
      "7176903     I'm earning #mPLUSRewards in My Quit Coach. ht...\n",
      "56285238    Wow, Great Job Rachel Jackson, you are a true ...\n",
      "62517794         @team_tunji I will once we get back workingðŸ‘Œ\n",
      "12767572    Work Part-Time, Live Full-Time â€“ Why I Choose ...\n",
      "12928876    RT @pleasingharry: THIS IS YOUR LAST CHANCE TO...\n",
      "69791711    @ddiamond @PhaedraParks right. Jobs for people...\n",
      "7111552     RT @TheDailyEdge: Inequality: Wall St-ers earn...\n",
      "10322987                YESS! Great job boys!! #LETSGORANGERS\n",
      "47416051                    and work  https://t.co/KXYP7w4Q2D\n",
      "Name: text, dtype: object\n",
      "time taken to dump: 16.519010066986084 seconds\n"
     ]
    }
   ],
   "source": [
    "print(merged_filtered[1300000:20000000].head(30)['text'])\n",
    "print(merged_filtered[1300000:20000000].tail(30)['text'])\n",
    "start_time = time.time()\n",
    "pickle.dump( merged_filtered[1300000:20000000], open( \"./ONNXturk100M/job_offer_select_ONNX_BERT_ST_merged_filtered_100m_jun22.pkl\".format(column), \"wb\" ) )\n",
    "print('time taken to dump:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merging random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " is_unemployed\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/is_unemployed/\n",
      "time taken to load random sample: 110.52857494354248 seconds (91060413, 3)\n",
      "drop columns: 0.5876379013061523 seconds\n",
      "time taken to merge: 89.58239603042603 seconds (91060413, 3)\n",
      "                    tweet_id  is_unemployed  \\\n",
      "91060403  480800302723903489       0.005023   \n",
      "91060404  480800424732413952       0.009104   \n",
      "91060405  480800554541912064       0.010539   \n",
      "91060406  480801041223401472       0.004538   \n",
      "91060407  480801104012120064       0.003330   \n",
      "91060408  480801754334756866       0.005141   \n",
      "91060409  480801993267503104       0.006567   \n",
      "91060410  480803981493420033       0.038735   \n",
      "91060411  480805851922325505       0.004640   \n",
      "91060412  480805906477613056       0.006376   \n",
      "\n",
      "                                                       text  \n",
      "91060403  RT @alexmorgan13: 2 more hours!! Me and @melea...  \n",
      "91060404           a fuck nigga, thats the shit i dont like  \n",
      "91060405  Interface Builder in Xcode 6 is still hella fr...  \n",
      "91060406  HERE IS THE BEST LINK TO A COUPLE OF NEW BEATS...  \n",
      "91060407  RT @MattSebek: 12 years ago today, Darryl Kile...  \n",
      "91060408  @VanWeasel I think you are getting the worst m...  \n",
      "91060409  @PeggyAllen2 @BMPena13 #wuhs #sga #lobsterfest...  \n",
      "91060410                RT @jbayleigh_: I need new sandals.  \n",
      "91060411  @utt_jamie just left #HOBYMN2014 and I'm alrea...  \n",
      "91060412  @MedicMaiden523 @NYRTown27 Yea there's some gu...  \n",
      "\n",
      "\n",
      " lost_job_1mo\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/lost_job_1mo/\n",
      "time taken to load random sample: 120.70354390144348 seconds (90124423, 3)\n",
      "drop columns: 0.5893905162811279 seconds\n",
      "time taken to merge: 92.17652630805969 seconds (90125923, 4)\n",
      "                    tweet_id  lost_job_1mo  is_unemployed  \\\n",
      "90125913  480800302723903489      0.008009       0.005023   \n",
      "90125914  480800424732413952      0.013568       0.009104   \n",
      "90125915  480800554541912064      0.006173       0.010539   \n",
      "90125916  480801041223401472      0.007936       0.004538   \n",
      "90125917  480801104012120064      0.010842       0.003330   \n",
      "90125918  480801754334756866      0.013154       0.005141   \n",
      "90125919  480801993267503104      0.009631       0.006567   \n",
      "90125920  480803981493420033      0.006503       0.038735   \n",
      "90125921  480805851922325505      0.019823       0.004640   \n",
      "90125922  480805906477613056      0.009891       0.006376   \n",
      "\n",
      "                                                       text  \n",
      "90125913  RT @alexmorgan13: 2 more hours!! Me and @melea...  \n",
      "90125914           a fuck nigga, thats the shit i dont like  \n",
      "90125915  Interface Builder in Xcode 6 is still hella fr...  \n",
      "90125916  HERE IS THE BEST LINK TO A COUPLE OF NEW BEATS...  \n",
      "90125917  RT @MattSebek: 12 years ago today, Darryl Kile...  \n",
      "90125918  @VanWeasel I think you are getting the worst m...  \n",
      "90125919  @PeggyAllen2 @BMPena13 #wuhs #sga #lobsterfest...  \n",
      "90125920                RT @jbayleigh_: I need new sandals.  \n",
      "90125921  @utt_jamie just left #HOBYMN2014 and I'm alrea...  \n",
      "90125922  @MedicMaiden523 @NYRTown27 Yea there's some gu...  \n",
      "\n",
      "\n",
      " job_search\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/job_search/\n",
      "time taken to load random sample: 116.48342108726501 seconds (89955879, 3)\n",
      "drop columns: 0.5891711711883545 seconds\n",
      "time taken to merge: 92.73473954200745 seconds (89961879, 5)\n",
      "                    tweet_id  job_search  lost_job_1mo  is_unemployed  \\\n",
      "89961869  480800302723903489    0.001325      0.008009       0.005023   \n",
      "89961870  480800424732413952    0.000520      0.013568       0.009104   \n",
      "89961871  480800554541912064    0.000430      0.006173       0.010539   \n",
      "89961872  480801041223401472    0.000429      0.007936       0.004538   \n",
      "89961873  480801104012120064    0.000562      0.010842       0.003330   \n",
      "89961874  480801754334756866    0.000601      0.013154       0.005141   \n",
      "89961875  480801993267503104    0.000455      0.009631       0.006567   \n",
      "89961876  480803981493420033    0.249783      0.006503       0.038735   \n",
      "89961877  480805851922325505    0.000363      0.019823       0.004640   \n",
      "89961878  480805906477613056    0.000713      0.009891       0.006376   \n",
      "\n",
      "                                                       text  \n",
      "89961869  RT @alexmorgan13: 2 more hours!! Me and @melea...  \n",
      "89961870           a fuck nigga, thats the shit i dont like  \n",
      "89961871  Interface Builder in Xcode 6 is still hella fr...  \n",
      "89961872  HERE IS THE BEST LINK TO A COUPLE OF NEW BEATS...  \n",
      "89961873  RT @MattSebek: 12 years ago today, Darryl Kile...  \n",
      "89961874  @VanWeasel I think you are getting the worst m...  \n",
      "89961875  @PeggyAllen2 @BMPena13 #wuhs #sga #lobsterfest...  \n",
      "89961876                RT @jbayleigh_: I need new sandals.  \n",
      "89961877  @utt_jamie just left #HOBYMN2014 and I'm alrea...  \n",
      "89961878  @MedicMaiden523 @NYRTown27 Yea there's some gu...  \n",
      "\n",
      "\n",
      " is_hired_1mo\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/is_hired_1mo/\n",
      "time taken to load random sample: 116.68918585777283 seconds (89955879, 3)\n",
      "drop columns: 0.6117243766784668 seconds\n",
      "time taken to merge: 94.29408144950867 seconds (89975379, 6)\n",
      "                    tweet_id  is_hired_1mo  job_search  lost_job_1mo  \\\n",
      "89975369  480800302723903489      0.009955    0.001325      0.008009   \n",
      "89975370  480800424732413952      0.008273    0.000520      0.013568   \n",
      "89975371  480800554541912064      0.001798    0.000430      0.006173   \n",
      "89975372  480801041223401472      0.002336    0.000429      0.007936   \n",
      "89975373  480801104012120064      0.001650    0.000562      0.010842   \n",
      "89975374  480801754334756866      0.001592    0.000601      0.013154   \n",
      "89975375  480801993267503104      0.001851    0.000455      0.009631   \n",
      "89975376  480803981493420033      0.016251    0.249783      0.006503   \n",
      "89975377  480805851922325505      0.004400    0.000363      0.019823   \n",
      "89975378  480805906477613056      0.002099    0.000713      0.009891   \n",
      "\n",
      "          is_unemployed                                               text  \n",
      "89975369       0.005023  RT @alexmorgan13: 2 more hours!! Me and @melea...  \n",
      "89975370       0.009104           a fuck nigga, thats the shit i dont like  \n",
      "89975371       0.010539  Interface Builder in Xcode 6 is still hella fr...  \n",
      "89975372       0.004538  HERE IS THE BEST LINK TO A COUPLE OF NEW BEATS...  \n",
      "89975373       0.003330  RT @MattSebek: 12 years ago today, Darryl Kile...  \n",
      "89975374       0.005141  @VanWeasel I think you are getting the worst m...  \n",
      "89975375       0.006567  @PeggyAllen2 @BMPena13 #wuhs #sga #lobsterfest...  \n",
      "89975376       0.038735                RT @jbayleigh_: I need new sandals.  \n",
      "89975377       0.004640  @utt_jamie just left #HOBYMN2014 and I'm alrea...  \n",
      "89975378       0.006376  @MedicMaiden523 @NYRTown27 Yea there's some gu...  \n",
      "\n",
      "\n",
      " job_offer\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/job_offer/\n",
      "time taken to load random sample: 116.04946303367615 seconds (89955879, 3)\n",
      "drop columns: 0.5847036838531494 seconds\n",
      "time taken to merge: 98.73852729797363 seconds (90015879, 7)\n",
      "                    tweet_id  job_offer  is_hired_1mo  job_search  \\\n",
      "90015869  480800302723903489   0.003656      0.009955    0.001325   \n",
      "90015870  480800424732413952   0.001941      0.008273    0.000520   \n",
      "90015871  480800554541912064   0.001850      0.001798    0.000430   \n",
      "90015872  480801041223401472   0.224086      0.002336    0.000429   \n",
      "90015873  480801104012120064   0.002457      0.001650    0.000562   \n",
      "90015874  480801754334756866   0.002708      0.001592    0.000601   \n",
      "90015875  480801993267503104   0.003538      0.001851    0.000455   \n",
      "90015876  480803981493420033   0.001968      0.016251    0.249783   \n",
      "90015877  480805851922325505   0.002698      0.004400    0.000363   \n",
      "90015878  480805906477613056   0.005279      0.002099    0.000713   \n",
      "\n",
      "          lost_job_1mo  is_unemployed  \\\n",
      "90015869      0.008009       0.005023   \n",
      "90015870      0.013568       0.009104   \n",
      "90015871      0.006173       0.010539   \n",
      "90015872      0.007936       0.004538   \n",
      "90015873      0.010842       0.003330   \n",
      "90015874      0.013154       0.005141   \n",
      "90015875      0.009631       0.006567   \n",
      "90015876      0.006503       0.038735   \n",
      "90015877      0.019823       0.004640   \n",
      "90015878      0.009891       0.006376   \n",
      "\n",
      "                                                       text  \n",
      "90015869  RT @alexmorgan13: 2 more hours!! Me and @melea...  \n",
      "90015870           a fuck nigga, thats the shit i dont like  \n",
      "90015871  Interface Builder in Xcode 6 is still hella fr...  \n",
      "90015872  HERE IS THE BEST LINK TO A COUPLE OF NEW BEATS...  \n",
      "90015873  RT @MattSebek: 12 years ago today, Darryl Kile...  \n",
      "90015874  @VanWeasel I think you are getting the worst m...  \n",
      "90015875  @PeggyAllen2 @BMPena13 #wuhs #sga #lobsterfest...  \n",
      "90015876                RT @jbayleigh_: I need new sandals.  \n",
      "90015877  @utt_jamie just left #HOBYMN2014 and I'm alrea...  \n",
      "90015878  @MedicMaiden523 @NYRTown27 Yea there's some gu...  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop dup: 129.14112615585327 seconds (82905552, 7)\n",
      "time taken to dump: 58.25591516494751 seconds\n"
     ]
    }
   ],
   "source": [
    "# outputting 1M (pinky finger up) tweets by column\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print('\\n\\n', column)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_ONNX_optimized/{}/'.format(column)\n",
    "    print(model_output_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_output_random = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'random*.csv')], ignore_index = True)\n",
    "    print('time taken to load random sample:', str(time.time() - start_time), 'seconds', model_output_random.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_random = model_output_random[['tweet_id', 'second']]\n",
    "    model_output_random.columns = ['tweet_id', column]\n",
    "    print('drop columns:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "\n",
    "    if column == 'is_unemployed':\n",
    "        start_time = time.time()\n",
    "        merged_random = pd.merge(model_output_random, tweets_random, how='inner', on = 'tweet_id')\n",
    "        print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_random.shape)\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        merged_random = pd.merge(model_output_random, merged_random, how='inner', on = 'tweet_id')\n",
    "        print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_random.shape)\n",
    "            \n",
    "    print(merged_random.tail(10))\n",
    "            \n",
    "#     break\n",
    "    \n",
    "    \n",
    "start_time = time.time()\n",
    "merged_random = merged_random.drop_duplicates(['text'])\n",
    "print('drop dup:', str(time.time() - start_time), 'seconds', merged_random.shape)\n",
    "\n",
    "start_time = time.time()\n",
    "pickle.dump( merged_random, open( \"./ONNXturk100M/ALL_ONNX_BERT_ST_merged_random_100m_jun24.pkl\", \"wb\" ) )\n",
    "print('time taken to dump:', str(time.time() - start_time), 'seconds')\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
