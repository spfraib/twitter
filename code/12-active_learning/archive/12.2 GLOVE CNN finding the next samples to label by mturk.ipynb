{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure you have all libraries installed. \n",
    "use conda environment: /scratch/da2734/twitter/worldbank_twitter_environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading filtered samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to load filtered sample: 309.1688368320465 seconds\n",
      "(92121093, 11)\n"
     ]
    }
   ],
   "source": [
    "# filtered contains 0.8G of data!!\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "country_code = 'US'\n",
    "month = '2012-1'\n",
    "path_to_data = '/scratch/spf248/twitter/data/classification/US/filtered/'\n",
    "tweets_filtered=pq.ParquetDataset(glob(os.path.join(path_to_data,                                           \n",
    "#                                            country_code,\n",
    "#                                            month,\n",
    "                                           '*.parquet'))).read().to_pandas()\n",
    "tweets_filtered['tweet_id'] = tweets_filtered['tweet_id'].astype(int)\n",
    "print('time taken to load filtered sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop columns: 6.185930490493774 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tweets_filtered = tweets_filtered[['tweet_id', 'text']]\n",
    "print('drop columns:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276933934124765184</td>\n",
       "      <td>Damn i have to much homework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>277143632232992768</td>\n",
       "      <td>Does a bedazzler work on leather? Serious ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>277150490276540416</td>\n",
       "      <td>RT @porcelain10: Washington Post D Milbank bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>277151157208637440</td>\n",
       "      <td>Finally off work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>277175194454462465</td>\n",
       "      <td>Irrational: No BioShock PS Vita until Sony and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text\n",
       "0  276933934124765184                       Damn i have to much homework\n",
       "1  277143632232992768  Does a bedazzler work on leather? Serious ques...\n",
       "2  277150490276540416  RT @porcelain10: Washington Post D Milbank bas...\n",
       "3  277151157208637440                                   Finally off work\n",
       "4  277175194454462465  Irrational: No BioShock PS Vita until Sony and..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to load random sample: 354.7586364746094 seconds\n",
      "(92114009, 11)\n"
     ]
    }
   ],
   "source": [
    "# random contains 0.8G of data!!\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "country_code = 'US'\n",
    "month = '2012-1'\n",
    "path_to_data = '/scratch/spf248/twitter/data/classification/US/random/'\n",
    "tweets_random=pq.ParquetDataset(glob(os.path.join(path_to_data,                                           \n",
    "#                                            country_code,\n",
    "#                                            month,\n",
    "                                           '*.parquet'))).read().to_pandas()\n",
    "tweets_random['tweet_id'] = tweets_random['tweet_id'].astype(int)\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_random.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop columns: 5.405134201049805 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tweets_random = tweets_random[['tweet_id', 'text']]\n",
    "print('drop columns:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>367881326273105920</td>\n",
       "      <td>@shoebydoo32 I only left to go back home for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>367881326281519105</td>\n",
       "      <td>oh my god bina id idnt read that u played GTA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>367882185916702722</td>\n",
       "      <td>I Have To Make What I Think Is The Best Decisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367883553121394690</td>\n",
       "      <td>@elizrod_ that's from hard work 😉</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>367883867719348224</td>\n",
       "      <td>RT @MileenaSucks: Can I just lay out in the gr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text\n",
       "0  367881326273105920  @shoebydoo32 I only left to go back home for t...\n",
       "1  367881326281519105  oh my god bina id idnt read that u played GTA ...\n",
       "2  367882185916702722  I Have To Make What I Think Is The Best Decisi...\n",
       "3  367883553121394690                  @elizrod_ that's from hard work 😉\n",
       "4  367883867719348224  RT @MileenaSucks: Can I just lay out in the gr..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_random.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filtered top 10M files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65461336, 3)\n"
     ]
    }
   ],
   "source": [
    "print(merged_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " is_unemployed\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/is_unemployed/\n",
      "time taken to load filtered sample: 26.308045148849487 seconds (40809972, 2)\n",
      "drop columns: 0.28781652450561523 seconds\n",
      "time taken to merge: 77.44315648078918 seconds (40809972, 3)\n",
      "(40809972, 3)\n",
      "drop dup: 55.0835325717926 seconds (36333272, 3)\n",
      "sort: 22.951133966445923 seconds (36333272, 3)\n",
      "top 5M truncate: 2.2111856937408447 seconds (36333272, 3)\n",
      "24732266    #JobSearch Brand Ambassador XM - Brand Activat...\n",
      "34812390    Best Boston Careers CFO Cambridge Operating Gr...\n",
      "1178028     RT @OldSpice: Ray Lewis taught us real success...\n",
      "6276315     When your coworker plays your favorite song at...\n",
      "22157731    Illinois New Jobs $ Company Truck Drivers - Fl...\n",
      "1569147     Las Vegas New Jobs $ Pharmaceutical Customer S...\n",
      "21933438    New #job opening at Richemont in #Houston - #B...\n",
      "1865894     Computers &amp; Networking #5 : http://t.co/cA...\n",
      "2275735     Telecommute Jobs $$$ Business Analysis Consult...\n",
      "1472146     #Arlington #TX #Job Company Truck Drivers - Fl...\n",
      "33323914    Top Pay Houston Jobs Pharmaceutical Customer S...\n",
      "17648867    Supposed to be working but can't stop staring ...\n",
      "15397022    Company Truck Drivers - Flatbed &amp; Van - CD...\n",
      "6725094     Honored to announce former Secretaries of U.S....\n",
      "2238086     Arizona jobs $$ Business Intelligence Consulta...\n",
      "27745290    Can you recommend anyone for this #job? ENVIRO...\n",
      "28151282    RT @CouncilwomanBRB: Looking forward to the wo...\n",
      "29170984    Top NYC Brooklyn Job Clinical Performance Solu...\n",
      "34725186    Engel &amp; Völkers and Turnquist Partners Ann...\n",
      "35662872    Top Laggards: United States Steel Corp. (NYSE:...\n",
      "37841149    Job Silicon Valley $ Pharmaceutical Representa...\n",
      "16619406    Las Vegas New Jobs $ Pharmacuetical Customer S...\n",
      "28634703    New Post - EQ Labs, Inc. and Crown Equity Hold...\n",
      "39362568    RT @CamTucker_Metro: Click on the Mike Babcock...\n",
      "33685505    RT @EvieBelievie: This totally erases consenti...\n",
      "38847191    New job post: Sales Associate at Chrysler Jeep...\n",
      "21538571    RT @FreedomWorks: Here’s How Goldman Sachs CEO...\n",
      "31600047    Las Vegas New Jobs $ Sales Representative - Di...\n",
      "21085663    Genuine Pandora 14k gold Ltd Ed open work hear...\n",
      "29896379    Los Angeles Jobs $$ Sales Representative, Busi...\n",
      "26597106    #Job #Gadsden Pharmaceutical Customer Service ...\n",
      "14298044    Illinois New Jobs $ Vice President, Business D...\n",
      "4707171     Networking &amp; Deals #110: http://t.co/vGLpp...\n",
      "6051285     Networking &amp; Deals #110: http://t.co/iUd6f...\n",
      "23723724    You have to see ODU's Ron Whitcomb at work at ...\n",
      "29379490    #Dallas #Job Sales Operations Representative -...\n",
      "3359011     Computers &amp; Networking #5 : http://t.co/9i...\n",
      "10464180    RT @DjKingAssassin: Dr. Dre &amp; Jimmy Iovine...\n",
      "14335314    Tablets &amp; Networking : https://t.co/UAwmRD...\n",
      "9084531     Tablets &amp; Networking : http://t.co/KtLN4PL...\n",
      "11826122    Tablets &amp; Networking : https://t.co/U34eNp...\n",
      "5410059     Tablets &amp; Networking : https://t.co/VwFMLN...\n",
      "22152046    Up-to-Minute #JobSearch Truck Driver - Owner O...\n",
      "6516016     RT @RTNBA: Wolves are interviewing Jeff Van Gu...\n",
      "32540121    Networking #101 : https://t.co/jgNBbqFgiV HP -...\n",
      "30036732    #pharmaceutical Respiratory Care Specialist - ...\n",
      "37324013    Geithner to join Warburg Pincus - Former US Tr...\n",
      "23607070    $MDT - Jefferies Group Equities Analysts Lower...\n",
      "9081445     Computers &amp; Networking #5 : http://t.co/yl...\n",
      "15890696    New Bose SoundLink On-Ear Bluetooth Wireless H...\n",
      "Name: text, dtype: object\n",
      "time taken to dump: 4.195570468902588 seconds\n",
      "\n",
      "\n",
      " lost_job_1mo\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/lost_job_1mo/\n",
      "time taken to load filtered sample: 33.09776997566223 seconds (40809972, 2)\n",
      "drop columns: 0.2920806407928467 seconds\n",
      "time taken to merge: 78.39689087867737 seconds (40809972, 3)\n",
      "(40809972, 3)\n",
      "drop dup: 55.18895649909973 seconds (36333272, 3)\n",
      "sort: 23.959326028823853 seconds (36333272, 3)\n",
      "top 5M truncate: 2.174835681915283 seconds (36333272, 3)\n",
      "25706970    The fact that we are biologically wired for mu...\n",
      "27132730    Make-up artists who work with stars like Amy S...\n",
      "31624559    VA Customer Jobs $ Customer Relationship Manag...\n",
      "30205994    The best supporting actress winner was asked b...\n",
      "21962353    Check out how data is used to assign students ...\n",
      "14124763    Entrepreneur: The CEO and co-founder of SeatGe...\n",
      "28381745    RT @newtechnetwork: Research shows #PBL in hig...\n",
      "34925297    RT @JohnWesleyShipp: Actors Equity Association...\n",
      "35279921    JOB; Manchester NH - Regional Teacher - Child ...\n",
      "21831726    .@afroboi @WorkFr0mHome_ @AfroBadoo Tips for c...\n",
      "22972485    @eday110 @Jobsconnected2 @xs25xon Tips for cra...\n",
      "16439088    5 Ways to Negotiate at Work and Win - Elizabet...\n",
      "5244905     RT @hells_mafia: Girls work on their looks but...\n",
      "37082170    RT @jfonder10: Dad yelling at his approx 6 yr ...\n",
      "15073553    RT @CNNPolitics: Senate Minority Leader Chuck ...\n",
      "40573958    Political Trivia Fun Fact:\\nRepublican strateg...\n",
      "34936416    Business Roundtable CEOs sing in chorus: we wa...\n",
      "1221597     RT @MiaFarrow: Why does CNN not refute Trump s...\n",
      "16122690    RT @MichelleRhee: Kesha is creating pop-up lib...\n",
      "5454084     RT @AASInvestigates: Gov. Rick Perry has hired...\n",
      "23771792    Always fascinating overhearing conversations o...\n",
      "34650657    Companies and countries alike are jockeying fo...\n",
      "24374386    The extrovert group is loud &amp; laughing. Th...\n",
      "1881388     RT @TheDCVince: Uranium One hired Podesta Grou...\n",
      "8666793     Why big brands are hiring mental-health aides ...\n",
      "19091242    RT @PreceptMinistry: \"Be steadfast immovable a...\n",
      "37422131    Trump supporters are grasping at straws and ca...\n",
      "2211778     RT @PolitiFact: Pro-Dem group claims Koch brot...\n",
      "5197995     RT @TFemomist: John Kasich admits protesters a...\n",
      "19012810    Don't Call It That: A Naming Workbook — A step...\n",
      "22264823    How Too Many Independent Workers Squander Thei...\n",
      "22265913    How Too Many Independent Workers Squander Thei...\n",
      "28211867    Girls work on their looks but not their minds ...\n",
      "1399402     1D fans are mad asf that Kpop groups are beati...\n",
      "33994032    RT @Bonfiredesigns: Goldman Sachs endorses the...\n",
      "7539534     RT @michelllllada: Why do Mexican moms nag at ...\n",
      "35339343    RT @realpolitics7: Actors hired to applaud her...\n",
      "20939671    Are you looking for a candidate to fill a posi...\n",
      "20060094    “Workforce development programs are effective ...\n",
      "36565662    #DigitalMedia: Infographic: How quick keys mak...\n",
      "22769595    EVAs are one my favorite example of human coll...\n",
      "37062458    RT @CNN: \"He just says what's convenient at th...\n",
      "38245007    RT @annie_sparrow: 1/2 While Ebola crisis grew...\n",
      "14524181    \"be steadfast immovable always abounding n the...\n",
      "14479919    RT @OmanReagan: Even when Hillary herself says...\n",
      "1213285     #ContentMarketing The CEO and co-founder of Se...\n",
      "14350291    Make-up artists who work with stars like Amy S...\n",
      "19516680    Make-up artists who work with stars like Amy S...\n",
      "27501768    RT @DanFowler05: A1: most effective principals...\n",
      "815555      @DanielBurrus Thank you for producing share-wo...\n",
      "Name: text, dtype: object\n",
      "time taken to dump: 3.786142349243164 seconds\n",
      "\n",
      "\n",
      " job_search\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/job_search/\n",
      "time taken to load filtered sample: 29.8760347366333 seconds (40809972, 2)\n",
      "drop columns: 0.2848677635192871 seconds\n",
      "time taken to merge: 83.13595342636108 seconds (40809972, 3)\n",
      "(40809972, 3)\n",
      "drop dup: 55.129724979400635 seconds (36333272, 3)\n",
      "sort: 24.333839654922485 seconds (36333272, 3)\n",
      "top 5M truncate: 1.9919648170471191 seconds (36333272, 3)\n",
      "5208070     Free 1PCS Gold Sand Dichroic Green Flower Lamp...\n",
      "39696659    Hong Kong protesters pepper sprayed - Hong Kon...\n",
      "33264116    RT @Morgitaa: Lol at ignorant ass ppl who thin...\n",
      "35416496    Beach Necklace Ocean Necklace Wave Necklace La...\n",
      "4101100     Now using the all natural face lip and cheeks ...\n",
      "22860907    RT @JesseVintage: White Aqua Blue Black Rondel...\n",
      "7564218     RT @SammysBeadworks: Mushroom Charm Dangle Ear...\n",
      "31061926    RT @SammysBeadworks: Mushroom Charm Dangle Ear...\n",
      "22875155    Sailor Moon Princess Serenity Crystal Cabochon...\n",
      "1776478     Sage Brown GRANITE SILVER European Charm Handm...\n",
      "22863995    RT @LandofFayelon: RT JesseVintage: White Aqua...\n",
      "37520475    󾓯 Turquoise and Brick Lampwork Bracelet - Hand...\n",
      "23745169    Collectible Decorated Old Handwork Tibet Silve...\n",
      "3213831     Swarovski Pearl Earrings Beadwork Bezel Sterli...\n",
      "13215123    White Aqua Blue Black Rondelle Lampwork Bead A...\n",
      "11990287    White Aqua Blue Black Rondelle Lampwork Bead A...\n",
      "3837401     White Aqua Blue Black Rondelle Lampwork Bead A...\n",
      "2083069     White Aqua Blue Black Rondelle Lampwork Bead A...\n",
      "13022862    Heart Pendant - Mosaic - Dichroic Fused Glass ...\n",
      "13288270    Glass and Brass Bracelet Turquoise Lampwork Bl...\n",
      "30887680    Adjustable charm bracelet - orange glass beads...\n",
      "23203139    Contemporary Stained Glass Jewelry Box – Dichr...\n",
      "38874801    RT @arkayscreations: Beadwork Bracelet - Earth...\n",
      "7545629     Blue and Black Earrings - Blue Bead Earrings -...\n",
      "16050562    Be the #one to #follow the #instagram \" http:/...\n",
      "7465278     Violet Flower Earrings - Lampwork Glass Earrin...\n",
      "31709402    RT @hjvdesigns: Copper Torch Fired Enamel Disc...\n",
      "36716837    Beach Necklace Ocean Necklace Wave Necklace La...\n",
      "8680815     RT @RitaFrontese: European charm bracelet - ki...\n",
      "8100665     RT @pippinbarr: ▊\\n▊ ▊ LIFE HACK\\n▊ ▊ ▊ ▊ ▊ ▊\\...\n",
      "11240519    RT @mysticgemz77: Owl Bracelet - Owl Charm Bra...\n",
      "15125036    Raw: Pepper Spray Fired at El Cajon Protesters...\n",
      "10453694    Bright Green over Blue - DRAGON TEARS Silver W...\n",
      "3457689     #Job 【RETWEET】 #Followback #Follow #TFB #TFBJP...\n",
      "14068865    #Job 【RETWEET】 #Followback #Follow #TFB #TFBJP...\n",
      "12349305    #Job 【RETWEET】 #Followback #Follow #TFB #TFBJP...\n",
      "7348820     Druzy agate necklace - purple agate pendant - ...\n",
      "11752098    RT @hjvdesigns: Blue Hoop Style Earrings\\nCopp...\n",
      "18571761    HALLOWEEN SALE Clockwork Spider pendant - blue...\n",
      "33334083    Chandelier Earrings - Dichroic Fused Glass Ear...\n",
      "17228897    Get Your Chilled Wine On - NewAir Thermoelectr...\n",
      "11474822    RT @Mylingy: Aqua Fired Agate Gemstone Lampwor...\n",
      "12669214    Enter to #win a Lowepro Flipside Sport &amp; J...\n",
      "17215510    Enter to #win a Lowepro Flipside Sport &amp; J...\n",
      "23067580    RT @SammysBeadworks: Mushroom Charm Dangle Ear...\n",
      "12262139    Dichroic Fused Glass - Brooch - Dichroic Glass...\n",
      "29930008    Pugster Blue Beautiful Fish Lampwork Silver Mu...\n",
      "24679840    Biceps and Bows - Ruffles with Love - Racerbac...\n",
      "8336535     Horse Necklace - Pony Necklace - Swarovski Cry...\n",
      "22629150    Lampwork Glass Earrings - Periwinkle Earrings ...\n",
      "Name: text, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to dump: 3.8571767807006836 seconds\n",
      "\n",
      "\n",
      " is_hired_1mo\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/is_hired_1mo/\n",
      "time taken to load filtered sample: 30.5049307346344 seconds (40809972, 2)\n",
      "drop columns: 0.29251646995544434 seconds\n",
      "time taken to merge: 77.83843755722046 seconds (40809972, 3)\n",
      "(40809972, 3)\n",
      "drop dup: 54.788901567459106 seconds (36333272, 3)\n",
      "sort: 23.370448112487793 seconds (36333272, 3)\n",
      "top 5M truncate: 2.6819756031036377 seconds (36333272, 3)\n",
      "39013089    @flocktard @seanhannity @UrbanAchievr What are...\n",
      "14786170    @chiu0nthls no significant network problems th...\n",
      "3386967     Hypnotist helps woman quit smoking - KPRC Hous...\n",
      "31963888    Sen. Jeff Sessions Sends Memo Urging GOP Colle...\n",
      "15147608    RT @sallykohn: Please please please please PLE...\n",
      "29040816    HOW ANY MAJOR ELECTION REALLY WORKS: https://t...\n",
      "1105230     Fired Texas trooper who probed women's genital...\n",
      "9308064     TEENAGERS HATE: ? Homework ? Fake friends ? Sc...\n",
      "27724749    RT @NBCNews: Teen girl avoids serious injury w...\n",
      "28642682    4 Strategies to Sharpen Your Focus: Simple tip...\n",
      "18351436    RT @gamma_ray239: This Corrupt HACK ↪ @Marilyn...\n",
      "30784297    RT @AnyaKlepakk: Dirty-minded Brunette Slut Gi...\n",
      "33934571    Skeleton Key malware steals credentials to inf...\n",
      "12898148    RT @BradRodu: FDA tobacco chief ignores govern...\n",
      "27026056    Alarmed By Planned Immigration ´Surge´, Civil ...\n",
      "22140340    ‘Environmental genocide’: Native Americans qui...\n",
      "34263629    RT @politicalwire: Huckabee Threatens to Quit ...\n",
      "3871849     You hurt Steph. Why can't you quit fucking wit...\n",
      "18810696    What happened with all the Election Fraud @POT...\n",
      "7139137     Why do folks think they should work so hard to...\n",
      "24472502    Do you spit or swallow cum — Spitters are quit...\n",
      "12055406    Law clerk says court ignored sexual harassment...\n",
      "39759324    Everyone please quit retweeting all these fake...\n",
      "23302211    RT @coachfisher_rp: Q3: How does the offensive...\n",
      "18979307    #Job Sr Mgr Appraisal - Esurance - Richardson,...\n",
      "2580291     Find this #Job &amp;More Inbound Sales Rep - E...\n",
      "3910055     Integrated Marketing Communications Manager - ...\n",
      "33926593    #Job #Gadsden ▉ Real Estate Closing Agents-- B...\n",
      "29109166    HELL NO TO AMNESTY. Fire all those that suppor...\n",
      "15820308    Mayank Gandhi threatens to quit AAP - Niticent...\n",
      "2420856     Mayank Gandhi threatens to quit AAP - Niticent...\n",
      "8630616     San Antonio officer caught on video body slamm...\n",
      "12696784    loan officer jobs Connecticut fraud investigat...\n",
      "10205161    RT @splcenter: SPLC Files Federal Safety Compl...\n",
      "1178697     RT @ciciperritt: RT People should really quit ...\n",
      "1774167     When a co-worker blatantly refuses to help a m...\n",
      "2164470     RT @PackardSusan: EQ is important for work and...\n",
      "9575323     RT @shhanology: \"instead of asking women why t...\n",
      "15139701    RT @WITS_NKitKat: You ↑ You ↗ You ↙ You → You ...\n",
      "6587331     @imfuxkingcook Our app and text messages are f...\n",
      "39858636    #Dallas #Jobs Claims Assistant - Esurance -  R...\n",
      "21726501    RT @NRSC: Mother of overdose victim criticizes...\n",
      "758847      #security #Job alert: Fraud Intelligence Inves...\n",
      "2705677     ‘Environmental genocide’: Native Americans qui...\n",
      "1183322     Casualty Liability Adjuster - Esurance - Alpha...\n",
      "31811306    RT @_metafizik: Sick of cowardly cons decrying...\n",
      "39975368    @6_Elijah how you screenshot that ? Psp app ? ...\n",
      "34711685    RT @USMarineCorps: Marines never quit because ...\n",
      "17929605    RT @PRESlDENTBANNON: Liberal journalists who s...\n",
      "8384059     RT @motherboard: Survey of Facebook quitters f...\n",
      "Name: text, dtype: object\n",
      "time taken to dump: 3.906129837036133 seconds\n",
      "\n",
      "\n",
      " job_offer\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/job_offer/\n",
      "time taken to load filtered sample: 11.829874038696289 seconds (40809972, 2)\n",
      "drop columns: 0.28623294830322266 seconds\n",
      "time taken to merge: 78.53052020072937 seconds (40809972, 3)\n",
      "(40809972, 3)\n",
      "drop dup: 54.74268579483032 seconds (36333272, 3)\n",
      "sort: 25.874905586242676 seconds (36333272, 3)\n",
      "top 5M truncate: 2.702890634536743 seconds (36333272, 3)\n",
      "6013399     RT @melbssss: i literally just said i quit alc...\n",
      "12407869         4 years ago I quit smoking cigarettes #goals\n",
      "24444717    I neeeeed to quit smoking cigarettes BUT I DON...\n",
      "37623088    After I said I quit drinking earlier today.......\n",
      "31015642    Adele thinks quitting smoking weakened her voi...\n",
      "18389088    @5AMpancakes quite frankly i am disturbed bc i...\n",
      "12633569    RT @YungL25: I quit smoking easy AF Lol now wh...\n",
      "31957997    Ever since I quit drinking Diet Coke my swolle...\n",
      "27973870    Niggas think I fell off. I was workin 60. (HUS...\n",
      "13802068        I swear I said I was gunna quit tobacco 🤦🏻‍♂️\n",
      "31790703    I remember when i was on probation haha kinda ...\n",
      "40450653                    I quit smoking weed when I was 11\n",
      "17876872    @Jah_vee_yuh i heard that sis!! i think i spra...\n",
      "24828114    first i was collecting unemployment because i ...\n",
      "10022506    Still can't believe I quit smoking cigarettes ...\n",
      "19947170    I said I was gonna quit smoking but I haven't ...\n",
      "9518787          Now I remember why I quit smoking cigarettes\n",
      "3835030     RT @JODYHiGHROLLER: i STOPPED DOiNG DRUGS AND ...\n",
      "7400624     Supposed to quit smoking but i relapsed after ...\n",
      "37279182             Why did I quit smoking cigarettes again?\n",
      "17468195    I was sooooo tired after work, I just ate and ...\n",
      "1118384     Im laying down hopefully I dont sleep....since...\n",
      "11879565    my cousin said he thought i said snort homewor...\n",
      "30560625    @ProphetLV Yes. I did not move until I see his...\n",
      "13779156    Lmfao Tobacco quit just favorited that. I DONT...\n",
      "32820695    @QUEEN_Sx0 i was told not to quit just cut bac...\n",
      "5804500     RT @MKGenest: So, I quit smoking cold turkey a...\n",
      "11195707    I quit smoking Middletons now im chain smoking...\n",
      "2922239     @JacobCouvillon well technically I got fired b...\n",
      "12896235    @KateCopeseeley lol it was brutal but I hated ...\n",
      "19344339    The amount of friends I've lost ever since I q...\n",
      "33333570    Just bought 9 grams smh I quit smoking after this\n",
      "4384384     I been tryna quit smokin since I started #stru...\n",
      "24735462    Since i forreal quit smoking cigarettes I feel...\n",
      "35906819    @ABronxThing I have emphysema I don't smoke ci...\n",
      "15336363    Funny as shit I said I was gunna quit smoking ...\n",
      "2089507     it sux i quit smoking cuz that's how i hid fro...\n",
      "4851741     Today I am quitting cigarettes cold turkey! I ...\n",
      "21352098    @davenavarro I am lucky I never even tried str...\n",
      "18513872    RT @audcoonz: Since I quit smoking cigarettes ...\n",
      "9290637     RT @yunawinter: i had to quit smoking cigarett...\n",
      "15019153    so fuckin frustrated cuz its my fault im joble...\n",
      "28202207    @Callme_Scruffy Noo he said if I kept eatin tw...\n",
      "38386338    i just remembered that when i told a coworker ...\n",
      "29580116    Ever since I quit smoking cigarettes my teeth ...\n",
      "15374741    Omg now I remember why I quit smoking weed whe...\n",
      "33381925      Why did I quit smoking weed now that was unwise\n",
      "1548219               Ever since I quit smoking cigs I feel 💯\n",
      "16189580    I never knew how absolutely disgusting smoking...\n",
      "24888843    @ninaax0x im tryna quit smoking tobacco i have...\n",
      "Name: text, dtype: object\n",
      "time taken to dump: 4.414255380630493 seconds\n"
     ]
    }
   ],
   "source": [
    "# outputting 1M (pinky finger up) tweets by column\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# boundary_list = [i/20.0 for i in range(11)]\n",
    "# boundary_list.reverse()\n",
    "# print('boundary_list', boundary_list)\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print('\\n\\n', column)\n",
    "    \n",
    "#     if column == 'job_offer': continue\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/{}/'.format(column)\n",
    "    print(model_output_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_output_filtered = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'filtered*.csv')], ignore_index = True)\n",
    "    print('time taken to load filtered sample:', str(time.time() - start_time), 'seconds', model_output_filtered.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_filtered = model_output_filtered[['tweet_id', 'proba']]\n",
    "    print('drop columns:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    merged_filtered = pd.merge(model_output_filtered, tweets_filtered, how='inner', on = 'tweet_id')\n",
    "    print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "\n",
    "    print(merged_filtered.shape)\n",
    "\n",
    "    start_time = time.time()\n",
    "    merged_filtered = merged_filtered.drop_duplicates(['text'])\n",
    "    print('drop dup:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    merged_filtered = merged_filtered.sort_values(by=['proba'], ascending=False)\n",
    "    print('sort:', str(time.time() - start_time), 'seconds', merged_filtered.shape)    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    merged_filtered_temp = merged_filtered[:5000000]\n",
    "    print('top 5M truncate:', str(time.time() - start_time), 'seconds', merged_filtered.shape)   \n",
    "    \n",
    "    print(merged_filtered_temp.tail(50)['text'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pickle.dump( merged_filtered_temp, open( \"./ONNXturk100M/{}_GLOVE_CNN_merged_filtered_100m_jun22.pkl\".format(column), \"wb\" ) )\n",
    "    print('time taken to dump:', str(time.time() - start_time), 'seconds')\n",
    "        \n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5M truncate: 0.0004470348358154297 seconds (36333272, 3)\n",
      "333802      @USPS Bang up job so far guys.. Think you miss...\n",
      "40415006                    RT @Obey_Katii: Instagram working\n",
      "12124043    Too much sweetness in a relationship leads to ...\n",
      "4671441     Extremely humbled &amp; honored to announce I ...\n",
      "27341155    RT @cl_atlanta: Atlanta jobs agency gets full-...\n",
      "18499039    RT @JudithKBrennan: @HellgateOsprey The nest i...\n",
      "25467470    Frustrations of a Job Seeker by Ed Tsyitee Jr ...\n",
      "35856640    \"@LamontBender: Cooking and Working Out! #myth...\n",
      "20454317    Prayer and devotion are a great way to start t...\n",
      "9798804     Quit being cheap Oakland and build another sta...\n",
      "30134276    RT @3Mpowerment: Ladies and Gentlemen Please W...\n",
      "2175210             @Pabloowwjobb si ahorita voy. Yeah right!\n",
      "8356666     #AbandonedTreasures Jan 2016 #Vintiques #Paint...\n",
      "22128914    Give your ego the day off. Your job today is t...\n",
      "39647160    Oh hey, I got an interview for a job at Target...\n",
      "27483899    Thank you for taking the @arnisrestaurant Retr...\n",
      "7797759     RT @XLACED: #CDV now open for submissions!\\nSe...\n",
      "35215091    @meowzermix The white bits always require extr...\n",
      "34146905    RT @ContentCouncil: Shout out to @MagnetMediaI...\n",
      "34375304    What Doesn't Work: Literacy Practices We Shoul...\n",
      "28796829    cool video: top reasons u should work here @Au...\n",
      "32239720    RT @CapehartJ: READ ---&gt; Behind chaotic pre...\n",
      "34074153    jobs Build a Unity Game Prototype by Nectar1: ...\n",
      "10317160    Having trouble deciding what career path is be...\n",
      "27726118    DeMarco Murray's truck stick button is stuck i...\n",
      "4205684     Having trouble deciding what career path is be...\n",
      "17395302                          hopefully were busy at work\n",
      "12307244                     You had 1 job Sam!! GoT season 3\n",
      "12071092    Everything you desire lies in YOUR #NETWORK.\\n...\n",
      "25814602    “Agamer applied for the North Dakota football ...\n",
      "8362454     Be Prepared: 3 Questions to Expect During a Ma...\n",
      "35299027    New #Job #Omaha Public Power District Presiden...\n",
      "10747269    “Surgeons are best positioned to actually lead...\n",
      "35503993    An African-American riveter at work in Burbank...\n",
      "26810163    Don't want to go to work. Need more sleep!!! H...\n",
      "36569746    @UnrealEngine @EpicGames I’m currently making ...\n",
      "29370054    RT @JENESISMagazine: On #JENESIS: Industry Job...\n",
      "35690196    @jimmyfallon good job today with the @AppMeerk...\n",
      "37537313    Cool feature in the March edition of @FoodNetw...\n",
      "38011597    @TweetPGA \\nCheck out Bulova Quartz Golf Theme...\n",
      "14398357    Commie &amp;#9773; Mints RT @queenaquileens no...\n",
      "9674506     @RusevBUL Reacts To @KalistoWWE's US Title Win...\n",
      "9382193     RT @LACANetwork: Much love and gratitude @Nigg...\n",
      "25246751    RT @USWomenVeterans: \"Recently, Menlo Park-bas...\n",
      "7346273     RT @vichupedia: GYM - only place where people ...\n",
      "28827828    RT @Johnatsrs1949: go get a job --------------...\n",
      "21725532                              Another slow day @ work\n",
      "5236061            #thingsidratherdothanwork lay on the beach\n",
      "30619883    RT @mljamrisko: #Yellen jobs dashboard a bit b...\n",
      "30690203    Thinking about starting a business?  Want to i...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "merged_filtered_temp = merged_filtered[5000000:12000000]\n",
    "print('top 5M truncate:', str(time.time() - start_time), 'seconds', merged_filtered.shape)   \n",
    "\n",
    "# print(merged_filtered_temp.tail(50)['text'])\n",
    "\n",
    "start_time = time.time()\n",
    "pickle.dump( merged_filtered_temp, open( \"./ONNXturk100M/{}_GLOVE_CNN_merged_filtered_100m_jun22_5_to_12M.pkl\".format(column), \"wb\" ) )\n",
    "print('time taken to dump:', str(time.time() - start_time), 'seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random top 10M files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " is_unemployed\n",
      "\n",
      "\n",
      " lost_job_1mo\n",
      "\n",
      "\n",
      " job_search\n",
      "\n",
      "\n",
      " is_hired_1mo\n",
      "\n",
      "\n",
      " job_offer\n",
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/job_offer/\n",
      "time taken to load random sample: 66.41552114486694 seconds (91210308, 2)\n",
      "drop columns: 0.9590597152709961 seconds\n",
      "time taken to merge: 99.23714971542358 seconds (91210308, 3)\n",
      "drop dup: 117.09680914878845 seconds (84034162, 3)\n",
      "sort: 59.81334638595581 seconds (84034162, 3)\n"
     ]
    }
   ],
   "source": [
    "# outputting 1M (pinky finger up) tweets by column\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# boundary_list = [i/20.0 for i in range(11)]\n",
    "# boundary_list.reverse()\n",
    "# print('boundary_list', boundary_list)\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print('\\n\\n', column)\n",
    "    \n",
    "    if column != 'job_offer': continue\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/{}/'.format(column)\n",
    "    print(model_output_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_output_random = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'random*.csv')], ignore_index = True)\n",
    "    print('time taken to load random sample:', str(time.time() - start_time), 'seconds', model_output_random.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_random = model_output_random[['tweet_id', 'proba']]\n",
    "    print('drop columns:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    merged_random = pd.merge(model_output_random, tweets_random, how='inner', on = 'tweet_id')\n",
    "    print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_random.shape)\n",
    "\n",
    "#     print(merged_random.head())\n",
    "\n",
    "    start_time = time.time()\n",
    "    merged_random = merged_random.drop_duplicates(['text'])\n",
    "    print('drop dup:', str(time.time() - start_time), 'seconds', merged_random.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    merged_random = merged_random.sort_values(by=['proba'], ascending=False)\n",
    "    print('sort:', str(time.time() - start_time), 'seconds', merged_random.shape)    \n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     merged_random_temp = merged_random[:5000000]\n",
    "#     print('top 5M truncate:', str(time.time() - start_time), 'seconds', merged_random.shape)   \n",
    "    \n",
    "#     print(merged_random_temp.tail(50)['text'])\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     pickle.dump( merged_random_temp, open( \"./ONNXturk100M/{}_GLOVE_CNN_merged_random_100m_jun22.pkl\".format(column), \"wb\" ) )\n",
    "#     print('time taken to dump:', str(time.time() - start_time), 'seconds')\n",
    "        \n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting 1M (pinky finger up) tweets by column\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print('\\n\\n', column)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/{}/'.format(column)\n",
    "    print(model_output_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_output_filtered = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'filtered*.csv')], ignore_index = True)\n",
    "    print('time taken to load filtered sample:', str(time.time() - start_time), 'seconds', model_output_filtered.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_filtered = model_output_filtered[['tweet_id', 'proba']]\n",
    "    model_output_filtered.columns = ['tweet_id', column]\n",
    "    print('drop columns:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "\n",
    "    if column == 'is_unemployed':\n",
    "        start_time = time.time()\n",
    "        merged_filtered = pd.merge(model_output_filtered, tweets_filtered, how='inner', on = 'tweet_id')\n",
    "        print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        merged_filtered = pd.merge(model_output_filtered, merged_filtered, how='inner', on = 'tweet_id')\n",
    "        print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "            \n",
    "    print(merged_filtered.tail(10))\n",
    "            \n",
    "#     break\n",
    "    \n",
    "    \n",
    "start_time = time.time()\n",
    "merged_filtered = merged_filtered.drop_duplicates(['text'])\n",
    "print('drop dup:', str(time.time() - start_time), 'seconds', merged_filtered.shape)\n",
    "\n",
    "start_time = time.time()\n",
    "pickle.dump( merged_filtered, open( \"./ONNXturk100M/ALL_ONNX_BERT_ST_merged_filtered_100m_jun24.pkl\", \"wb\" ) )\n",
    "print('time taken to dump:', str(time.time() - start_time), 'seconds')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merging random tweets into one big file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# outputting 1M (pinky finger up) tweets by column\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "    print('\\n\\n', column)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/pred_output_100M_GLOVE_CNN_optimized/{}/'.format(column)\n",
    "    print(model_output_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_output_random = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'random*.csv')], ignore_index = True)\n",
    "    print('time taken to load random sample:', str(time.time() - start_time), 'seconds', model_output_random.shape)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_output_random = model_output_random[['tweet_id', 'proba']]\n",
    "    model_output_random.columns = ['tweet_id', column]\n",
    "    print('drop columns:', str(time.time() - start_time), 'seconds')\n",
    "    \n",
    "\n",
    "    if column == 'is_unemployed':\n",
    "        start_time = time.time()\n",
    "        merged_random = pd.merge(model_output_random, tweets_random, how='inner', on = 'tweet_id')\n",
    "        print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_random.shape)\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        merged_random = pd.merge(model_output_random, merged_random, how='inner', on = 'tweet_id')\n",
    "        print('time taken to merge:', str(time.time() - start_time), 'seconds', merged_random.shape)\n",
    "            \n",
    "    print(merged_random.tail(10))\n",
    "            \n",
    "#     break\n",
    "    \n",
    "    \n",
    "start_time = time.time()\n",
    "merged_random = merged_random.drop_duplicates(['text'])\n",
    "print('drop dup:', str(time.time() - start_time), 'seconds', merged_random.shape)\n",
    "\n",
    "start_time = time.time()\n",
    "pickle.dump( merged_random, open( \"./ONNXturk100M/ALL_GLOVE_CNN_merged_random_100m_jun24.pkl\", \"wb\" ) )\n",
    "print('time taken to dump:', str(time.time() - start_time), 'seconds')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_random.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
