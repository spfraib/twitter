{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_unemployed creating model and loading..\n",
      "03/30/2020 15:59:13 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_is_unemployed'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_is_unemployed'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 1, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da2734/miniconda3/envs/worldbank/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/30/2020 15:59:13 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/30/2020 15:59:13 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_is_unemployed.csv\n",
      "03/30/2020 15:59:14 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_dev_multi_class_512_val_is_unemployed.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 0.8670964241027832 seconds\n",
      "03/30/2020 15:59:14 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/da2734/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/30/2020 15:59:14 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/30/2020 15:59:14 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/da2734/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/30/2020 15:59:17 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/30/2020 15:59:17 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "03/30/2020 15:59:18 - INFO - root -   ***** Running training *****\n",
      "03/30/2020 15:59:18 - INFO - root -     Num examples = 4407\n",
      "03/30/2020 15:59:18 - INFO - root -     Num Epochs = 1\n",
      "03/30/2020 15:59:18 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "03/30/2020 15:59:18 - INFO - root -     Gradient Accumulation steps = 1\n",
      "03/30/2020 15:59:18 - INFO - root -     Total optimization steps = 551\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to file\n",
      "/scratch/da2734/twitter/mturk_mar6/output_is_unemployed/model_out_0\n",
      "03/30/2020 15:59:18 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_is_unemployed/model_out_0/config.json\n",
      "03/30/2020 15:59:20 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_is_unemployed/model_out_0/pytorch_model.bin\n",
      "03/30/2020 16:03:15 - INFO - root -   Running evaluation\n",
      "03/30/2020 16:03:15 - INFO - root -     Num examples = 1101\n",
      "03/30/2020 16:03:15 - INFO - root -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='69' class='' max='69', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [69/69 00:15<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/30/2020 16:03:31 - INFO - root -   eval_loss after epoch 1: 0.5354319494487583: \n",
      "03/30/2020 16:03:31 - INFO - root -   eval_accuracy after epoch 1: 0.7801998183469573: \n",
      ">> saving to..  /scratch/da2734/twitter/mturk_mar6/output_is_unemployed/results.json\n",
      "03/30/2020 16:03:31 - INFO - root -   lr after epoch 1: 0.0\n",
      "03/30/2020 16:03:31 - INFO - root -   train_loss after epoch 1: 0.43081418589812226\n",
      "03/30/2020 16:03:31 - INFO - root -   \n",
      "\n",
      "lost_job_1mo creating model and loading..\n",
      "03/30/2020 16:03:31 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_lost_job_1mo'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_lost_job_1mo'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 1, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n",
      "03/30/2020 16:03:31 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "cached_features_file: /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_lost_job_1mo.csv\n",
      ">>> /scratch/da2734/twitter/mturk_mar6/data_binary/train_lost_job_1mo.csv\n",
      "03/30/2020 16:03:34 - INFO - root -   Writing example 0 of 4415\n",
      "03/30/2020 16:03:37 - INFO - root -   Saving features into cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_lost_job_1mo.csv\n",
      "03/30/2020 16:03:41 - INFO - root -   Writing example 0 of 1103\n",
      "03/30/2020 16:03:42 - INFO - root -   Saving features into cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_dev_multi_class_512_val_lost_job_1mo.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 269.29431223869324 seconds\n",
      "03/30/2020 16:03:42 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/da2734/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/30/2020 16:03:42 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/30/2020 16:03:42 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/da2734/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/30/2020 16:03:46 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/30/2020 16:03:46 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "03/30/2020 16:03:46 - INFO - root -   ***** Running training *****\n",
      "03/30/2020 16:03:46 - INFO - root -     Num examples = 4415\n",
      "03/30/2020 16:03:46 - INFO - root -     Num Epochs = 1\n",
      "03/30/2020 16:03:46 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "03/30/2020 16:03:46 - INFO - root -     Gradient Accumulation steps = 1\n",
      "03/30/2020 16:03:46 - INFO - root -     Total optimization steps = 552\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to file\n",
      "/scratch/da2734/twitter/mturk_mar6/output_lost_job_1mo/model_out_0\n",
      "03/30/2020 16:03:46 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_lost_job_1mo/model_out_0/config.json\n",
      "03/30/2020 16:03:48 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_lost_job_1mo/model_out_0/pytorch_model.bin\n",
      "03/30/2020 16:07:44 - INFO - root -   Running evaluation\n",
      "03/30/2020 16:07:44 - INFO - root -     Num examples = 1103\n",
      "03/30/2020 16:07:44 - INFO - root -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='69' class='' max='69', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [69/69 00:16<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/30/2020 16:08:00 - INFO - root -   eval_loss after epoch 1: 0.3221706247524075: \n",
      "03/30/2020 16:08:00 - INFO - root -   eval_accuracy after epoch 1: 0.898458748866727: \n",
      ">> saving to..  /scratch/da2734/twitter/mturk_mar6/output_lost_job_1mo/results.json\n",
      "03/30/2020 16:08:00 - INFO - root -   lr after epoch 1: 0.0\n",
      "03/30/2020 16:08:00 - INFO - root -   train_loss after epoch 1: 0.44141741756997677\n",
      "03/30/2020 16:08:00 - INFO - root -   \n",
      "\n",
      "job_search creating model and loading..\n",
      "03/30/2020 16:08:00 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_job_search'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_job_search'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 1, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n",
      "03/30/2020 16:08:00 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "cached_features_file: /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_job_search.csv\n",
      ">>> /scratch/da2734/twitter/mturk_mar6/data_binary/train_job_search.csv\n",
      "03/30/2020 16:08:03 - INFO - root -   Writing example 0 of 4431\n",
      "03/30/2020 16:08:06 - INFO - root -   Saving features into cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_job_search.csv\n",
      "03/30/2020 16:08:10 - INFO - root -   Writing example 0 of 1107\n",
      "03/30/2020 16:08:10 - INFO - root -   Saving features into cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_dev_multi_class_512_val_job_search.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 538.1599481105804 seconds\n",
      "03/30/2020 16:08:11 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/da2734/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/30/2020 16:08:11 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/30/2020 16:08:11 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/da2734/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/30/2020 16:08:15 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/30/2020 16:08:15 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "03/30/2020 16:08:15 - INFO - root -   ***** Running training *****\n",
      "03/30/2020 16:08:15 - INFO - root -     Num examples = 4431\n",
      "03/30/2020 16:08:15 - INFO - root -     Num Epochs = 1\n",
      "03/30/2020 16:08:15 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "03/30/2020 16:08:15 - INFO - root -     Gradient Accumulation steps = 1\n",
      "03/30/2020 16:08:15 - INFO - root -     Total optimization steps = 554\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to file\n",
      "/scratch/da2734/twitter/mturk_mar6/output_job_search/model_out_0\n",
      "03/30/2020 16:08:15 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_job_search/model_out_0/config.json\n",
      "03/30/2020 16:08:17 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_job_search/model_out_0/pytorch_model.bin\n",
      "03/30/2020 16:12:14 - INFO - root -   Running evaluation\n",
      "03/30/2020 16:12:14 - INFO - root -     Num examples = 1107\n",
      "03/30/2020 16:12:14 - INFO - root -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='70' class='' max='70', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [70/70 00:16<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/30/2020 16:12:30 - INFO - root -   eval_loss after epoch 1: 0.3901121497154236: \n",
      "03/30/2020 16:12:30 - INFO - root -   eval_accuracy after epoch 1: 0.8654019873532068: \n",
      ">> saving to..  /scratch/da2734/twitter/mturk_mar6/output_job_search/results.json\n",
      "03/30/2020 16:12:30 - INFO - root -   lr after epoch 1: 0.0\n",
      "03/30/2020 16:12:30 - INFO - root -   train_loss after epoch 1: 0.5369812721379827\n",
      "03/30/2020 16:12:30 - INFO - root -   \n",
      "\n",
      "is_hired_1mo creating model and loading..\n",
      "03/30/2020 16:12:30 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_is_hired_1mo'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_is_hired_1mo'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 1, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n",
      "03/30/2020 16:12:30 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "cached_features_file: /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_is_hired_1mo.csv\n",
      ">>> /scratch/da2734/twitter/mturk_mar6/data_binary/train_is_hired_1mo.csv\n",
      "03/30/2020 16:12:33 - INFO - root -   Writing example 0 of 4419\n",
      "03/30/2020 16:12:36 - INFO - root -   Saving features into cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_is_hired_1mo.csv\n",
      "03/30/2020 16:12:40 - INFO - root -   Writing example 0 of 1104\n",
      "03/30/2020 16:12:41 - INFO - root -   Saving features into cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_dev_multi_class_512_val_is_hired_1mo.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 808.3061938285828 seconds\n",
      "03/30/2020 16:12:42 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/da2734/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/30/2020 16:12:42 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/30/2020 16:12:42 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/da2734/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/30/2020 16:12:45 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/30/2020 16:12:45 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "03/30/2020 16:12:45 - INFO - root -   ***** Running training *****\n",
      "03/30/2020 16:12:45 - INFO - root -     Num examples = 4419\n",
      "03/30/2020 16:12:45 - INFO - root -     Num Epochs = 1\n",
      "03/30/2020 16:12:45 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "03/30/2020 16:12:45 - INFO - root -     Gradient Accumulation steps = 1\n",
      "03/30/2020 16:12:45 - INFO - root -     Total optimization steps = 553\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to file\n",
      "/scratch/da2734/twitter/mturk_mar6/output_is_hired_1mo/model_out_0\n",
      "03/30/2020 16:12:45 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_is_hired_1mo/model_out_0/config.json\n",
      "03/30/2020 16:12:48 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_is_hired_1mo/model_out_0/pytorch_model.bin\n",
      "03/30/2020 16:16:44 - INFO - root -   Running evaluation\n",
      "03/30/2020 16:16:44 - INFO - root -     Num examples = 1104\n",
      "03/30/2020 16:16:44 - INFO - root -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='69' class='' max='69', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [69/69 00:16<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/30/2020 16:17:00 - INFO - root -   eval_loss after epoch 1: 0.23548285317593726: \n",
      "03/30/2020 16:17:00 - INFO - root -   eval_accuracy after epoch 1: 0.9365942028985508: \n",
      ">> saving to..  /scratch/da2734/twitter/mturk_mar6/output_is_hired_1mo/results.json\n",
      "03/30/2020 16:17:00 - INFO - root -   lr after epoch 1: 0.0\n",
      "03/30/2020 16:17:00 - INFO - root -   train_loss after epoch 1: 0.44552475091400023\n",
      "03/30/2020 16:17:00 - INFO - root -   \n",
      "\n",
      "job_offer creating model and loading..\n",
      "03/30/2020 16:17:00 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_job_offer'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_job_offer'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 1, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n",
      "03/30/2020 16:17:00 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "cached_features_file: /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_job_offer.csv\n",
      ">>> /scratch/da2734/twitter/mturk_mar6/data_binary/train_job_offer.csv\n",
      "03/30/2020 16:17:03 - INFO - root -   Writing example 0 of 4422\n",
      "03/30/2020 16:17:06 - INFO - root -   Saving features into cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_job_offer.csv\n",
      "03/30/2020 16:17:10 - INFO - root -   Writing example 0 of 1105\n",
      "03/30/2020 16:17:10 - INFO - root -   Saving features into cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_dev_multi_class_512_val_job_offer.csv\n",
      "num_labels 2\n",
      "time taken to load all this stuff: 1078.2041556835175 seconds\n",
      "03/30/2020 16:17:11 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/da2734/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/30/2020 16:17:11 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/30/2020 16:17:11 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/da2734/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/30/2020 16:17:15 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/30/2020 16:17:15 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "03/30/2020 16:17:15 - INFO - root -   ***** Running training *****\n",
      "03/30/2020 16:17:15 - INFO - root -     Num examples = 4422\n",
      "03/30/2020 16:17:15 - INFO - root -     Num Epochs = 1\n",
      "03/30/2020 16:17:15 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "03/30/2020 16:17:15 - INFO - root -     Gradient Accumulation steps = 1\n",
      "03/30/2020 16:17:15 - INFO - root -     Total optimization steps = 553\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to file\n",
      "/scratch/da2734/twitter/mturk_mar6/output_job_offer/model_out_0\n",
      "03/30/2020 16:17:15 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_job_offer/model_out_0/config.json\n",
      "03/30/2020 16:17:17 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_job_offer/model_out_0/pytorch_model.bin\n",
      "03/30/2020 16:21:13 - INFO - root -   Running evaluation\n",
      "03/30/2020 16:21:13 - INFO - root -     Num examples = 1105\n",
      "03/30/2020 16:21:13 - INFO - root -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='70' class='' max='70', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [70/70 00:16<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/30/2020 16:21:30 - INFO - root -   eval_loss after epoch 1: 0.23045690836650984: \n",
      "03/30/2020 16:21:30 - INFO - root -   eval_accuracy after epoch 1: 0.9149321266968325: \n",
      ">> saving to..  /scratch/da2734/twitter/mturk_mar6/output_job_offer/results.json\n",
      "03/30/2020 16:21:30 - INFO - root -   lr after epoch 1: 0.0\n",
      "03/30/2020 16:21:30 - INFO - root -   train_loss after epoch 1: 0.4242126832549438\n",
      "03/30/2020 16:21:30 - INFO - root -   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gets all this setup\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "# import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, \\\n",
    "    convert_examples_to_features\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import *\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "\n",
    "    print(column, 'creating model and loading..')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    if not os.path.exists('/scratch/da2734/twitter/mturk_mar6/log_{}/'.format(column)):\n",
    "        os.makedirs('/scratch/da2734/twitter/mturk_mar6/log_{}/'.format(column))\n",
    "\n",
    "    if not os.path.exists('/scratch/da2734/twitter/mturk_mar6/output_binary_{}'.format(column)):\n",
    "        os.makedirs('/scratch/da2734/twitter/mturk_mar6/output_binary_{}'.format(column))\n",
    "\n",
    "    LOG_PATH = Path('/scratch/da2734/twitter/mturk_mar6/log_{}/'.format(column))\n",
    "    DATA_PATH = Path('/scratch/da2734/twitter/mturk_mar6/data_binary/')\n",
    "    LABEL_PATH = Path('/scratch/da2734/twitter/mturk_mar6/data_binary/')\n",
    "    OUTPUT_PATH = Path('/scratch/da2734/twitter/mturk_mar6/output_binary_{}'.format(column))\n",
    "    FINETUNED_PATH = None\n",
    "\n",
    "    args = Box({\n",
    "        \"run_text\": \"labor mturk ar 6 binary\",\n",
    "        \"train_size\": -1,\n",
    "        \"val_size\": -1,\n",
    "        \"log_path\": LOG_PATH,\n",
    "        \"full_data_dir\": DATA_PATH,\n",
    "        \"data_dir\": DATA_PATH,\n",
    "        \"task_name\": \"labor_market_classification\",\n",
    "        \"no_cuda\": False,\n",
    "        #     \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "        \"output_dir\": OUTPUT_PATH,\n",
    "        \"max_seq_length\": 512,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"do_lower_case\": True,\n",
    "        \"train_batch_size\": 8,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"warmup_proportion\": 0.0,\n",
    "        \"no_cuda\": False,\n",
    "        \"local_rank\": -1,\n",
    "        \"seed\": 42,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"optimize_on_cpu\": False,\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"max_steps\": -1,\n",
    "        \"warmup_steps\": 500,\n",
    "        \"logging_steps\": 50,\n",
    "        \"eval_all_checkpoints\": True,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"overwrite_cache\": True,\n",
    "        \"seed\": 42,\n",
    "        \"loss_scale\": 128,\n",
    "        \"task_name\": 'intent',\n",
    "        \"model_name\": 'bert-base-uncased',\n",
    "        \"model_type\": 'bert'\n",
    "    })\n",
    "\n",
    "    import logging\n",
    "\n",
    "    logfile = str(LOG_PATH / 'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "        handlers=[\n",
    "            logging.FileHandler(logfile),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ])\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    logger.info(args)\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        args.multi_gpu = True\n",
    "    else:\n",
    "        args.multi_gpu = False\n",
    "\n",
    "    # label_cols = [\"job_loss\",\"is_unemployed\",\"job_search\",\"is_hired\",\"job_offer\"]\n",
    "    # label_cols = ['pos', 'neg']\n",
    "    label_cols = ['pos']\n",
    "\n",
    "    databunch = BertDataBunch(\n",
    "        args['data_dir'],\n",
    "        LABEL_PATH,\n",
    "        args.model_name,\n",
    "        train_file='train_{}.csv'.format(column),\n",
    "        val_file='val_{}.csv'.format(column),\n",
    "        label_file='label_{}.csv'.format(column),\n",
    "        # test_data='test.csv',\n",
    "        text_col=\"text\",  # this is the name of the column in the train file that containts the tweet text\n",
    "        label_col=label_cols,\n",
    "        batch_size_per_gpu=args['train_batch_size'],\n",
    "        max_seq_length=args['max_seq_length'],\n",
    "        multi_gpu=args.multi_gpu,\n",
    "        multi_label=False,\n",
    "        model_type=args.model_type)\n",
    "\n",
    "    num_labels = len(databunch.labels)\n",
    "    print('num_labels', num_labels)\n",
    "\n",
    "    print('time taken to load all this stuff:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "    # metrics defined: https://github.com/kaushaltrivedi/fast-bert/blob/d89e2aa01d948d6d3cdea7ad106bf5792fea7dfa/fast_bert/metrics.py\n",
    "    metrics = []\n",
    "    # metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\n",
    "    # metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "    # metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "    metrics.append({'name': 'accuracy', 'function': accuracy})\n",
    "    # metrics.append({'name': 'accuracy_multilabel', 'function': accuracy_multilabel})\n",
    "\n",
    "    learner = BertLearner.from_pretrained_model(\n",
    "        databunch,\n",
    "        pretrained_path=args.model_name,\n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "        logger=logger,\n",
    "        output_dir=args.output_dir,\n",
    "        finetuned_wgts_path=FINETUNED_PATH,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        multi_gpu=args.multi_gpu,\n",
    "        is_fp16=args.fp16,\n",
    "        multi_label=False,\n",
    "        logging_steps=0)\n",
    "\n",
    "    learner.fit(args.num_train_epochs, args.learning_rate, validate=True)  # this trains the model\n",
    "\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/06/2020 23:41:11 - INFO - root -   Running evaluation\n",
      "03/06/2020 23:41:11 - INFO - root -     Num examples = 737\n",
      "03/06/2020 23:41:11 - INFO - root -     Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='47' class='' max='47', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [47/47 00:10<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.23552671804073008,\n",
       " 'accuracy_thresh': 0.9175034165382385,\n",
       " 'roc_auc': 0.8943037311014089,\n",
       " 'fbeta': 0.1971413791179657,\n",
       " 'accuracy': 0.0,\n",
       " 'accuracy_multilabel': 0.6390773405698779}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/06/2020 23:41:38 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_100/model_out/config.json\n",
      "03/06/2020 23:41:41 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_100/model_out/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "learner.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/06/2020 23:41:52 - INFO - root -   Writing example 0 of 1\n",
      "[('is_hired_1mo', 0.35232582688331604), ('lost_job_1mo', 0.31937041878700256), ('is_unemployed', 0.31286755204200745), ('job_search', 0.23360265791416168), ('job_offer\"', 0.056336939334869385)]\n"
     ]
    }
   ],
   "source": [
    "texts = ['I just received a job offer']\n",
    "predictions = learner.predict_batch(texts)\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
