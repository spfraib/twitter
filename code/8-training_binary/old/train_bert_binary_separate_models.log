is_unemployed creating model and loading..
training_binary.py:38: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.
  pd.set_option('display.max_colwidth', -1)
03/30/2020 17:27:20 - INFO - root -   {'run_text': 'labor mturk ar 6 binary', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('/scratch/da2734/twitter/mturk_mar6/log_is_unemployed'), 'full_data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'data_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/data_binary'), 'task_name': 'intent', 'no_cuda': False, 'output_dir': PosixPath('/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 100, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': True, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}
03/30/2020 17:27:20 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/da2734/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
03/30/2020 17:27:20 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_train_multi_class_512_train_is_unemployed.csv
03/30/2020 17:27:21 - INFO - root -   Loading features from cached file /scratch/da2734/twitter/mturk_mar6/data_binary/cache/cached_bert_dev_multi_class_512_val_is_unemployed.csv
num_labels 2
time taken to load all this stuff: 40.190072536468506 seconds
03/30/2020 17:27:21 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/da2734/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685
03/30/2020 17:27:21 - INFO - transformers.configuration_utils -   Model config {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

03/30/2020 17:27:21 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/da2734/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
03/30/2020 17:27:26 - INFO - transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
03/30/2020 17:27:26 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
03/30/2020 17:27:34 - INFO - root -   ***** Running training *****
03/30/2020 17:27:34 - INFO - root -     Num examples = 4407
03/30/2020 17:27:34 - INFO - root -     Num Epochs = 100
03/30/2020 17:27:34 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8
03/30/2020 17:27:34 - INFO - root -     Gradient Accumulation steps = 1
03/30/2020 17:27:34 - INFO - root -     Total optimization steps = 55100
█saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_0
03/30/2020 17:27:34 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_0/config.json
03/30/2020 17:27:35 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_0/pytorch_model.bin
█03/30/2020 17:33:30 - INFO - root -   Running evaluation
03/30/2020 17:33:30 - INFO - root -     Num examples = 1101
03/30/2020 17:33:30 - INFO - root -     Batch size = 16
█03/30/2020 17:33:55 - INFO - root -   eval_loss after epoch 1: 0.544746898546599: 
03/30/2020 17:33:55 - INFO - root -   eval_accuracy after epoch 1: 0.7801998183469573: 
/home/da2734/miniconda3/envs/worldbank/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 17:33:55 - INFO - root -   lr after epoch 1: 4.999989236227414e-05
03/30/2020 17:33:55 - INFO - root -   train_loss after epoch 1: 0.6204901889723788
03/30/2020 17:33:55 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_1
03/30/2020 17:33:55 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_1/config.json
03/30/2020 17:33:55 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_1/pytorch_model.bin
█03/30/2020 17:39:50 - INFO - root -   Running evaluation
03/30/2020 17:39:50 - INFO - root -     Num examples = 1101
03/30/2020 17:39:50 - INFO - root -     Batch size = 16
█03/30/2020 17:40:14 - INFO - root -   eval_loss after epoch 2: 0.48318625290108763: 
03/30/2020 17:40:14 - INFO - root -   eval_accuracy after epoch 2: 0.7801998183469573: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 17:40:14 - INFO - root -   lr after epoch 2: 4.9985004048298145e-05
03/30/2020 17:40:14 - INFO - root -   train_loss after epoch 2: 0.3711429099449229
03/30/2020 17:40:14 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_2
03/30/2020 17:40:14 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_2/config.json
03/30/2020 17:40:15 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_2/pytorch_model.bin
█03/30/2020 17:46:10 - INFO - root -   Running evaluation
03/30/2020 17:46:10 - INFO - root -     Num examples = 1101
03/30/2020 17:46:10 - INFO - root -     Batch size = 16
█03/30/2020 17:46:34 - INFO - root -   eval_loss after epoch 3: 0.38706166410575743: 
03/30/2020 17:46:34 - INFO - root -   eval_accuracy after epoch 3: 0.8374205267938238: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 17:46:34 - INFO - root -   lr after epoch 3: 4.994500492530196e-05
03/30/2020 17:46:34 - INFO - root -   train_loss after epoch 3: 0.3103729294454121
03/30/2020 17:46:34 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_3
03/30/2020 17:46:34 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_3/config.json
03/30/2020 17:46:35 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_3/pytorch_model.bin
█03/30/2020 17:52:30 - INFO - root -   Running evaluation
03/30/2020 17:52:30 - INFO - root -     Num examples = 1101
03/30/2020 17:52:30 - INFO - root -     Batch size = 16
█03/30/2020 17:52:55 - INFO - root -   eval_loss after epoch 4: 0.37408510897902475: 
03/30/2020 17:52:55 - INFO - root -   eval_accuracy after epoch 4: 0.8501362397820164: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 17:52:55 - INFO - root -   lr after epoch 4: 4.9879935193812935e-05
03/30/2020 17:52:55 - INFO - root -   train_loss after epoch 4: 0.25827864598746525
03/30/2020 17:52:55 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_4
03/30/2020 17:52:55 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_4/config.json
03/30/2020 17:52:55 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_4/pytorch_model.bin
█03/30/2020 17:58:51 - INFO - root -   Running evaluation
03/30/2020 17:58:51 - INFO - root -     Num examples = 1101
03/30/2020 17:58:51 - INFO - root -     Batch size = 16
█03/30/2020 17:59:15 - INFO - root -   eval_loss after epoch 5: 0.3892971517491168: 
03/30/2020 17:59:15 - INFO - root -   eval_accuracy after epoch 5: 0.8782924613987284: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 17:59:15 - INFO - root -   lr after epoch 5: 4.9789860251202935e-05
03/30/2020 17:59:15 - INFO - root -   train_loss after epoch 5: 0.22585582222025538
03/30/2020 17:59:15 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_5
03/30/2020 17:59:15 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_5/config.json
03/30/2020 17:59:16 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_5/pytorch_model.bin
█03/30/2020 18:05:11 - INFO - root -   Running evaluation
03/30/2020 18:05:11 - INFO - root -     Num examples = 1101
03/30/2020 18:05:11 - INFO - root -     Batch size = 16
█03/30/2020 18:05:35 - INFO - root -   eval_loss after epoch 6: 0.42420311142569: 
03/30/2020 18:05:35 - INFO - root -   eval_accuracy after epoch 6: 0.8728428701180745: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 18:05:35 - INFO - root -   lr after epoch 6: 4.967487062596163e-05
03/30/2020 18:05:35 - INFO - root -   train_loss after epoch 6: 0.2026247018424223
03/30/2020 18:05:35 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_6
03/30/2020 18:05:35 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_6/config.json
03/30/2020 18:05:36 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_6/pytorch_model.bin
█03/30/2020 18:11:32 - INFO - root -   Running evaluation
03/30/2020 18:11:32 - INFO - root -     Num examples = 1101
03/30/2020 18:11:32 - INFO - root -     Batch size = 16
█03/30/2020 18:11:56 - INFO - root -   eval_loss after epoch 7: 0.45448544275933417: 
03/30/2020 18:11:56 - INFO - root -   eval_accuracy after epoch 7: 0.8755676657584015: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 18:11:56 - INFO - root -   lr after epoch 7: 4.9535081886712234e-05
03/30/2020 18:11:56 - INFO - root -   train_loss after epoch 7: 0.18013761394205902
03/30/2020 18:11:56 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_7
03/30/2020 18:11:56 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_7/config.json
03/30/2020 18:11:57 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_7/pytorch_model.bin
█03/30/2020 18:17:52 - INFO - root -   Running evaluation
03/30/2020 18:17:52 - INFO - root -     Num examples = 1101
03/30/2020 18:17:52 - INFO - root -     Batch size = 16
█03/30/2020 18:18:16 - INFO - root -   eval_loss after epoch 8: 0.5742018896600475: 
03/30/2020 18:18:16 - INFO - root -   eval_accuracy after epoch 8: 0.8810172570390554: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 18:18:16 - INFO - root -   lr after epoch 8: 4.937063452606095e-05
03/30/2020 18:18:16 - INFO - root -   train_loss after epoch 8: 0.15190280474846618
03/30/2020 18:18:16 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_8
03/30/2020 18:18:17 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_8/config.json
03/30/2020 18:18:17 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_8/pytorch_model.bin
█03/30/2020 18:24:12 - INFO - root -   Running evaluation
03/30/2020 18:24:12 - INFO - root -     Num examples = 1101
03/30/2020 18:24:12 - INFO - root -     Batch size = 16
█03/30/2020 18:24:37 - INFO - root -   eval_loss after epoch 9: 0.569907111912102: 
03/30/2020 18:24:37 - INFO - root -   eval_accuracy after epoch 9: 0.8792007266121707: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 18:24:37 - INFO - root -   lr after epoch 9: 4.918169381939692e-05
03/30/2020 18:24:37 - INFO - root -   train_loss after epoch 9: 0.13322014303475674
03/30/2020 18:24:37 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_9
03/30/2020 18:24:37 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_9/config.json
03/30/2020 18:24:37 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_9/pytorch_model.bin
█03/30/2020 18:30:32 - INFO - root -   Running evaluation
03/30/2020 18:30:33 - INFO - root -     Num examples = 1101
03/30/2020 18:30:33 - INFO - root -     Batch size = 16
█03/30/2020 18:30:57 - INFO - root -   eval_loss after epoch 10: 0.6132918863516786: 
03/30/2020 18:30:57 - INFO - root -   eval_accuracy after epoch 10: 0.8773841961852861: 
>> saving to..  /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/results.json
03/30/2020 18:30:57 - INFO - root -   lr after epoch 10: 4.89684496587847e-05
03/30/2020 18:30:57 - INFO - root -   train_loss after epoch 10: 0.11515515720317082
03/30/2020 18:30:57 - INFO - root -   

saving to file
/scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_10
03/30/2020 18:30:57 - INFO - transformers.configuration_utils -   Configuration saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_10/config.json
03/30/2020 18:30:58 - INFO - transformers.modeling_utils -   Model weights saved in /scratch/da2734/twitter/mturk_mar6/output_binary_is_unemployed/model_out_10/pytorch_model.bin
█