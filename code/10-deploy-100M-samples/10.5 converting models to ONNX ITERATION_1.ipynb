{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertTokenizer, BertTokenizerFast, BertForSequenceClassification\n",
    "import onnxruntime as ort\n",
    "from onnxruntime_tools import optimizer\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from transformers.convert_graph_to_onnx import convert\n",
    "from convert_graph_to_onnx import convert\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import onnx\n",
    "from quantize import quantize, QuantizationMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_unemployed\n",
      "converting\n",
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/pytorch_models/is_unemployed/, tokenizer: DeepPavlov/bert-base-cased-conversational)\n",
      "PyTorch: 1.5.0+cu101\n",
      "optimizing\n",
      "Optimized model saved at : /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/onnx/is_unemployed/bert_optimized.onnx\n",
      "job_search\n",
      "converting\n",
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/pytorch_models/job_search/, tokenizer: DeepPavlov/bert-base-cased-conversational)\n",
      "PyTorch: 1.5.0+cu101\n",
      "optimizing\n",
      "Optimized model saved at : /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/onnx/job_search/bert_optimized.onnx\n",
      "is_hired_1mo\n",
      "converting\n",
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/pytorch_models/is_hired_1mo/, tokenizer: DeepPavlov/bert-base-cased-conversational)\n",
      "PyTorch: 1.5.0+cu101\n",
      "optimizing\n",
      "Optimized model saved at : /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/onnx/is_hired_1mo/bert_optimized.onnx\n",
      "job_offer\n",
      "converting\n",
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/pytorch_models/job_offer/, tokenizer: DeepPavlov/bert-base-cased-conversational)\n",
      "PyTorch: 1.5.0+cu101\n",
      "optimizing\n",
      "Optimized model saved at : /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/onnx/job_offer/bert_optimized.onnx\n",
      "lost_job_1mo\n",
      "converting\n",
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/pytorch_models/lost_job_1mo/, tokenizer: DeepPavlov/bert-base-cased-conversational)\n",
      "PyTorch: 1.5.0+cu101\n",
      "optimizing\n",
      "Optimized model saved at : /scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/onnx/lost_job_1mo/bert_optimized.onnx\n"
     ]
    }
   ],
   "source": [
    "# conversion\n",
    "# model_path = '/scratch/da2734/twitter/jobs/onnx/results_simpletransformers_jun3_10Klabels_0_all_labels/lost_job_1mo/'\n",
    "# onnx_path = '/scratch/da2734/twitter/jobs/onnx/results_simpletransformers_jun3_10Klabels_0_all_labels/lost_job_1mo/onnx/'\n",
    "\n",
    "import shutil\n",
    "\n",
    "for label in [\"is_unemployed\", \"job_search\", \"is_hired_1mo\", \"job_offer\", \"lost_job_1mo\"]:\n",
    "    \n",
    "#     if label == 'lost_job_1mo': continue\n",
    "\n",
    "    print(label)\n",
    "#     model_path = '/scratch/da2734/twitter/jobs/onnx/results_simpletransformers_jun3_10Klabels_0_all_labels/{}/'.format(label)\n",
    "    model_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/pytorch_models/{}/'.format(label)\n",
    "\n",
    "    onnx_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/onnx/{}/'.format(label)\n",
    "\n",
    "#     os.rmdir(onnx_path)\n",
    "    if os.path.isdir(onnx_path):\n",
    "        shutil.rmtree(onnx_path)\n",
    "    \n",
    "    os.makedirs(onnx_path)\n",
    "    \n",
    "    print('converting')\n",
    "    convert(framework=\"pt\", \n",
    "        model=model_path, \n",
    "        tokenizer=\"DeepPavlov/bert-base-cased-conversational\",\n",
    "        output=onnx_path+'converted.onnx', \n",
    "        opset=11)\n",
    "\n",
    "    print('optimizing')\n",
    "    # ONNX optimization\n",
    "    optimized_model = optimizer.optimize_model(onnx_path+'/converted.onnx',\n",
    "                                               model_type='bert', \n",
    "                                               num_heads=12, \n",
    "                                               hidden_size=768)\n",
    "\n",
    "    optimized_onnx_model_path = os.path.join(onnx_path, 'bert_optimized.onnx')\n",
    "    optimized_model.save_model_to_file(optimized_onnx_model_path)\n",
    "    print('Optimized model saved at :', optimized_onnx_model_path)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "def get_tokens(tokens_dict, i):\n",
    "    i_tokens_dict = dict()\n",
    "    for key in ['input_ids','token_type_ids','attention_mask']:\n",
    "        i_tokens_dict[key] = tokens_dict[key][i]\n",
    "    tokens = {name: np.atleast_2d(value) for name, value in i_tokens_dict.items()}\n",
    "    return tokens\n",
    "\n",
    "def inference(onnx_model, model_dir, examples, fast_tokenizer, num_threads):\n",
    "    quantized_str = ''\n",
    "    if 'quantized' in onnx_model:\n",
    "        quantized_str = 'quantized'\n",
    "    onnx_inference = []\n",
    "#     pytorch_inference = []\n",
    "    # onnx session\n",
    "    options = ort.SessionOptions()\n",
    "    options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "    options.intra_op_num_threads = 1\n",
    "    print(onnx_model)\n",
    "    ort_session = ort.InferenceSession(onnx_model, options)\n",
    "\n",
    "    # pytorch pretrained model and tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_dir)\n",
    "    tokenizer_str = \"BertTokenizerFast\"\n",
    "\n",
    "    print(\"**************** {} ONNX inference with batch tokenization and with {} tokenizer****************\".format(quantized_str, tokenizer_str))\n",
    "    start_onnx_inference_batch = time.time()\n",
    "    start_batch_tokenization = time.time()\n",
    "    tokens_dict = tokenizer.batch_encode_plus(examples, max_length=128)\n",
    "    total_batch_tokenization_time = time.time() - start_batch_tokenization\n",
    "    total_inference_time = 0\n",
    "    total_build_label_time = 0\n",
    "    for i in range(len(examples)):\n",
    "        \"\"\"\n",
    "        Onnx inference with batch tokenization\n",
    "        \"\"\"\n",
    "        tokens = get_tokens(tokens_dict, i)\n",
    "        #inference\n",
    "        start_inference = time.time()\n",
    "        ort_outs = ort_session.run(None, tokens)\n",
    "        total_inference_time = total_inference_time + (time.time() - start_inference)\n",
    "        #build label\n",
    "        start_build_label = time.time()\n",
    "        torch_onnx_output = torch.tensor(ort_outs[0], dtype=torch.float32)\n",
    "        onnx_logits = F.softmax(torch_onnx_output, dim=1)\n",
    "        logits_label = torch.argmax(onnx_logits, dim=1)\n",
    "        label = logits_label.detach().cpu().numpy()\n",
    "#         onnx_inference.append(label[0])\n",
    "        onnx_inference.append( onnx_logits.detach().cpu().numpy()[0].tolist() )\n",
    "        total_build_label_time = total_build_label_time + (time.time() - start_build_label)\n",
    "#         print(i, label[0], onnx_logits.detach().cpu().numpy()[0].tolist(), type(onnx_logits.detach().cpu().numpy()[0]) )\n",
    "\n",
    "    end_onnx_inference_batch = time.time()\n",
    "    print(\"Total batch tokenization time (in seconds): \", total_batch_tokenization_time)\n",
    "    print(\"Total inference time (in seconds): \", total_inference_time)\n",
    "    print(\"Total build label time (in seconds): \", total_build_label_time)\n",
    "    print(\"Duration ONNX inference (in seconds) with {} and batch tokenization: \".format(tokenizer_str), end_onnx_inference_batch - start_onnx_inference_batch, (end_onnx_inference_batch - start_onnx_inference_batch)/len(examples))\n",
    "\n",
    "#     print(onnx_inference)\n",
    "    return onnx_inference\n",
    "\n",
    "\n",
    "# model_path = '/scratch/da2734/twitter/jobs/onnx/results_simpletransformers_jun3_10Klabels_0_all_labels/{}/'.format(label)\n",
    "# onnx_labels = inference(onnx_path+'converted.onnx', \n",
    "#                                         model_path, \n",
    "#                                         examples, \n",
    "#                                         fast_tokenizer=True, \n",
    "#                                         num_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  6683\n",
      "[\"I'm am currently lolling for a new job in either the video, media, or graphic design fields. If anyone has any leads or openings please letâ€¦\", 'Can a NFL player who gets laid off file for Unemployment??? Lol', \"My new job just called me cause they haven't heard from me in a few days uhhhh maybe it's cause you gave me one shift this week?\", 'Unemployed? Buy An iPhone: By Reuven Gorsht, Vice-President, Customer Strategy, SAP http://t.co/bwOowJd9', 'I thought it was gunna get worse before it got better.. I was right when I got fired today', 'Also my co-workers \"fired\" me cause I didn\\'t come into work yesterday', \"It's a good time to be an #ExecutiveChef. It's an even better time to be an Executive Chef who's looking for a new job. Here's one opportunity (of many) available through @EHSGetHired  : https://t.co/VbQ60Bucup\", 'liz &amp; i got hired at 2 diff places today &amp; we\\'re like \"ok let me use ur discount &amp; u can use mine ok ?\"', \"I've been unemployed for a week. I haven't told anyone but have had 3 recruiters call and offer me shitty jobs.\", 'Currently trying to learn how to skateboard again and I hate myself for ever quitting']\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.read_csv('/scratch/da2734/twitter/data/jun3_10Klabels/data_binary_pos_neg_unbalanced/train_lost_job_1mo.csv')\n",
    "# eval_df.head()\n",
    "eval_df = eval_df[['text', 'class']]\n",
    "# print(eval_df.head())\n",
    "print('Number of examples: ', eval_df.shape[0])\n",
    "examples = eval_df.text.values.tolist()\n",
    "# len(examples)\n",
    "# examples[:1]\n",
    "examples = examples[:10]\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/onnx/is_unemployed/converted.onnx\n",
      "****************  ONNX inference with batch tokenization and with BertTokenizerFast tokenizer****************\n",
      "Total batch tokenization time (in seconds):  0.0011553764343261719\n",
      "Total inference time (in seconds):  0.3947763442993164\n",
      "Total build label time (in seconds):  0.0019130706787109375\n",
      "Duration ONNX inference (in seconds) with BertTokenizerFast and batch tokenization:  0.39865732192993164 0.039865732192993164\n"
     ]
    }
   ],
   "source": [
    "label = \"is_unemployed\"\n",
    "onnx_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/onnx/{}/'.format(label)\n",
    "model_path = '/scratch/da2734/twitter/jobs/running_on_200Msamples/iteration1/bert/pytorch_models/{}/'.format(label)\n",
    "onnx_labels = inference(onnx_path+'converted.onnx', \n",
    "                                        model_path, \n",
    "                                       examples, \n",
    "                                        fast_tokenizer=True, \n",
    "                                        num_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.9606851935386658, 0.03931479528546333],\n",
       " [0.9667195081710815, 0.033280499279499054],\n",
       " [0.9853585958480835, 0.014641438610851765],\n",
       " [0.9842662811279297, 0.01573370397090912],\n",
       " [0.5481763482093811, 0.4518236219882965],\n",
       " [0.6770866513252258, 0.32291334867477417],\n",
       " [0.9867992997169495, 0.013200734741985798],\n",
       " [0.9336254596710205, 0.0663745179772377],\n",
       " [0.04140980914235115, 0.9585901498794556],\n",
       " [0.9710121750831604, 0.028987891972064972]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#files: 1\n",
      "/scratch/spf248/twitter/data/classification/US/filtered_10perct_sample/part-00011-e5ecb1c7-f08f-4246-a46d-2643ba831074-c000.snappy.parquet\n",
      "(42990, 2)\n",
      "             tweet_id                                               text\n",
      "0  703470004939796480  @MarquesEvans @jcalv6 I'm proud of both of y'a...\n",
      "1  703471678433538048  RT @agbxlwt: @MelanieLBBH my sister (@vesselhs...\n",
      "2  703473306058358784  RT @BrysonTiIIller: relax, u'll graduate, u'll...\n",
      "3  703473997854461952  Let's have a wrap party!!!! Message me to set ...\n",
      "4  703475533300912129  #Job #Lexington Uber Full Time Driver - Indepe...\n",
      "(10, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"@MarquesEvans @jcalv6 I'm proud of both of y'all keep up the good work.\",\n",
       " 'RT @agbxlwt: @MelanieLBBH my sister (@vesselhs) drew you! please notice this ðŸ’“ðŸ’“ her instagram is @/quiteartzy xx https://t.co/zzmmy9uyyS',\n",
       " \"RT @BrysonTiIIller: relax, u'll graduate, u'll get a job, u'll become an adult, u'll find someone who loves you. u have an entire life. thiâ€¦\",\n",
       " \"Let's have a wrap party!!!! Message me to set up a party and get wrapped for free. #itworksâ€¦ https://t.co/6VMYeUDMHm\",\n",
       " '#Job #Lexington Uber Full Time Driver - Independent Contractor (Lexington) - Uber Driving Partners (Lexington,... https://t.co/LLkheC7o3e',\n",
       " 'Montco Employer Of 1,000 Workers Expects Major Layoffs: Reports https://t.co/6kZCQR3EW5',\n",
       " '@firstagentarp ...That attack should /not/ be TPKing. Show me your party composition?',\n",
       " 'RT @CarAutoDaily: Future Transport Networks... -  https://t.co/vL9JoFcZID',\n",
       " '#NeverTrump Our country worked too damn hard recovering from the last Republican disaster of a President!  https://t.co/kYO5d9JICM',\n",
       " 'RT @UFC_Shanda: Just got home from work to work from home then turn around and go back to work. ðŸ˜´ https://t.co/Mfhy3FrL5l']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pyarrow.parquet as pq\n",
    "# from glob import glob\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "SLURM_JOB_ID = 123123123\n",
    "SLURM_ARRAY_TASK_ID = 10\n",
    "SLURM_ARRAY_TASK_COUNT = 500\n",
    "\n",
    "path_to_data='/scratch/spf248/twitter/data/classification/US/'\n",
    "paths_to_filtered=list(np.array_split(\n",
    "                        # glob(os.path.join(path_to_data,'filtered','*.parquet')),\n",
    "                        glob(os.path.join(path_to_data,'filtered_10perct_sample','*.parquet')),\n",
    "#                         glob(os.path.join(path_to_data,'filtered_1perct_sample','*.parquet')),\n",
    "                        SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID])\n",
    "print('#files:', len(paths_to_filtered))\n",
    "\n",
    "tweets_filtered=pd.DataFrame()\n",
    "for file in paths_to_filtered:\n",
    "    print(file)\n",
    "    tweets_filtered=pd.concat([tweets_filtered,pd.read_parquet(file)[['tweet_id','text']]])\n",
    "    print(tweets_filtered.shape)\n",
    "    \n",
    "\n",
    "tweets_filtered = tweets_filtered[:10]\n",
    "print(tweets_filtered.head())\n",
    "print(tweets_filtered.shape)\n",
    "\n",
    "examples = tweets_filtered.text.values.tolist()\n",
    "# len(examples)\n",
    "# examples[:1]\n",
    "# examples = examples[:10]\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/da2734/twitter/jobs/onnx/results_simpletransformers_jun3_10Klabels_0_all_labels/lost_job_1mo/onnx/converted.onnx\n",
      "****************  ONNX inference with batch tokenization and with BertTokenizerFast tokenizer****************\n",
      "Total batch tokenization time (in seconds):  0.000499725341796875\n",
      "Total inference time (in seconds):  0.3943750858306885\n",
      "Total build label time (in seconds):  0.0016336441040039062\n",
      "Duration ONNX inference (in seconds) with BertTokenizerFast and batch tokenization:  0.39713454246520996 0.03971345424652099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.9864523410797119, 0.013547663576900959],\n",
       " [0.9874812960624695, 0.012518745847046375],\n",
       " [0.9874129295349121, 0.01258703414350748],\n",
       " [0.9917016625404358, 0.00829832162708044],\n",
       " [0.9961840510368347, 0.003815914737060666],\n",
       " [0.9908242225646973, 0.009175794199109077],\n",
       " [0.9920403361320496, 0.007959687151014805],\n",
       " [0.9957973957061768, 0.00420261500403285],\n",
       " [0.9902395009994507, 0.009760518558323383],\n",
       " [0.9889292120933533, 0.011070745065808296]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_labels = inference(onnx_path+'converted.onnx', \n",
    "                                        model_path, \n",
    "                                       examples, \n",
    "                                        fast_tokenizer=True, \n",
    "                                        num_threads=1)\n",
    "onnx_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>703470004939796480</th>\n",
       "      <td>0.986452</td>\n",
       "      <td>0.013548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703471678433538048</th>\n",
       "      <td>0.987481</td>\n",
       "      <td>0.012519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703473306058358784</th>\n",
       "      <td>0.987413</td>\n",
       "      <td>0.012587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703473997854461952</th>\n",
       "      <td>0.991702</td>\n",
       "      <td>0.008298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703475533300912129</th>\n",
       "      <td>0.996184</td>\n",
       "      <td>0.003816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703476921569513472</th>\n",
       "      <td>0.990824</td>\n",
       "      <td>0.009176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703477601117929473</th>\n",
       "      <td>0.992040</td>\n",
       "      <td>0.007960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703477722534666240</th>\n",
       "      <td>0.995797</td>\n",
       "      <td>0.004203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703483250677669888</th>\n",
       "      <td>0.990240</td>\n",
       "      <td>0.009761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703483829613244417</th>\n",
       "      <td>0.988929</td>\n",
       "      <td>0.011071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       first    second\n",
       "tweet_id                              \n",
       "703470004939796480  0.986452  0.013548\n",
       "703471678433538048  0.987481  0.012519\n",
       "703473306058358784  0.987413  0.012587\n",
       "703473997854461952  0.991702  0.008298\n",
       "703475533300912129  0.996184  0.003816\n",
       "703476921569513472  0.990824  0.009176\n",
       "703477601117929473  0.992040  0.007960\n",
       "703477722534666240  0.995797  0.004203\n",
       "703483250677669888  0.990240  0.009761\n",
       "703483829613244417  0.988929  0.011071"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_filtered_df = pd.DataFrame(data=onnx_labels, columns = ['first', 'second'])\n",
    "predictions_filtered_df = predictions_filtered_df.set_index(tweets_filtered.tweet_id)\n",
    "\n",
    "predictions_filtered_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
