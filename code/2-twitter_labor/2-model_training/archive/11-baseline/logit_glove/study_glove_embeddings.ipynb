{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "from sklearn.preprocessing import scale\n",
    "import enchant\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_general(tweet, size, vectors, aggregation='mean'):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tweet.split():\n",
    "        try:\n",
    "            vec += vectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if aggregation == 'mean':\n",
    "        if count != 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    elif aggregation == 'sum':\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = glove_twitter.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dic = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/manuto/Documents/world_bank/bert_twitter_labor/code/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word score determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to determine which words have the biggest influence on the classification decision for each label. \n",
    "\n",
    "To do this, we import word embeddings from GloVe (trained on Twitter data) and train one logistic regression per label, using the embeddings as input and each label as target. For each label, we then use the coefficient vector $b$ containing  the coefficient assigned to each dimension of the word embedding $x$. For each label $y$, the score assigned to each word $w$ is defined as follows: \n",
    "\n",
    "$$P(y=1| w) = \\frac{1}{1 + e^{-b*x_w}} $$ with $x_w$ the Twitter GloVe word embedding of word $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Label is_hired_1mo ************\n",
      "Precision:  0.7894736842105263\n",
      "Recall:  0.7741935483870968\n",
      "F-1 Score: 0.7817589576547231\n",
      "**********************************\n",
      "Top 30 word scores for label is_hired_1mo:\n",
      "ultrasound 0.9925333400677396\n",
      "panerai 0.9917193725300089\n",
      "resigns 0.990287840329032\n",
      "tyj 0.9902841202466814\n",
      "shoreline 0.9893935403492077\n",
      "battisti 0.9872657569351381\n",
      "montanas 0.985815433265916\n",
      "unopened 0.985789008853867\n",
      "riverdale 0.9824460327078864\n",
      "hired 0.980605939113789\n",
      "bloomfield 0.9774575055206486\n",
      "divorcee 0.9761434360421456\n",
      "accepted 0.9699050573807927\n",
      "mortenson 0.9697020065360366\n",
      "whoooop 0.9689413331541138\n",
      "booked 0.9676561110821127\n",
      "refurbished 0.9645882085584606\n",
      "armani 0.9623800075560839\n",
      "seatac 0.9622044768977256\n",
      "dancers 0.9618885124298453\n",
      "magnolia 0.9616251512357683\n",
      "openings 0.9611380844425051\n",
      "morphed 0.9605509218306135\n",
      "babysit 0.9600816933843663\n",
      "dental 0.9600648948965494\n",
      "salesmen 0.9575998108468805\n",
      "cfo 0.9565865108141255\n",
      "bartend 0.9552069732815022\n",
      "strippers 0.9536445572953834\n",
      "beachwood 0.9533686279754736\n",
      "                                 \n",
      "************ Label is_unemployed ************\n",
      "Precision:  0.7065217391304348\n",
      "Recall:  0.7008086253369272\n",
      "F-1 Score: 0.7036535859269283\n",
      "**********************************\n",
      "Top 30 word scores for label is_unemployed:\n",
      "needajob 0.951948519602599\n",
      "canned 0.9498709280259919\n",
      "termination 0.9211205706649704\n",
      "healthiest 0.9105788193214229\n",
      "inglewood 0.9096769183878601\n",
      "unicode 0.9000773513516084\n",
      "hireme 0.8957189126820225\n",
      "toughlife 0.8933463731373944\n",
      "hermit 0.890055225921825\n",
      "nopressure 0.8899239600152538\n",
      "verbally 0.8890761619631061\n",
      "worke 0.8886959525212244\n",
      "nannying 0.884389192196576\n",
      "wuddup 0.8825045852793422\n",
      "monetarily 0.8815091904459293\n",
      "pillows 0.8785748411686496\n",
      "unnamed 0.8770364156288828\n",
      "needhelp 0.8756109563365295\n",
      "workhardplayharder 0.872177098700607\n",
      "valid 0.8715873035908493\n",
      "cognizant 0.8710742912853888\n",
      "netherlands 0.8710705261183666\n",
      "grammatical 0.8689894126897362\n",
      "ayyyyyye 0.8675752781149056\n",
      "ineedajob 0.8671989451261854\n",
      "flats 0.8669061824301487\n",
      "seafoods 0.8659606579807227\n",
      "infraction 0.8647306788472842\n",
      "hurray 0.8638722381695401\n",
      "renewable 0.8635631588880811\n",
      "                                 \n",
      "************ Label job_offer ************\n",
      "Precision:  0.8904899135446686\n",
      "Recall:  0.865546218487395\n",
      "F-1 Score: 0.8778409090909092\n",
      "**********************************\n",
      "Top 30 word scores for label job_offer:\n",
      "flatbed 0.9956445293989635\n",
      "recruitment 0.9919878314154739\n",
      "resumes 0.9912877659853251\n",
      "locum 0.9912538325794793\n",
      "valvoline 0.9889963126153238\n",
      "locums 0.9889600281495158\n",
      "ministries 0.9880620984800379\n",
      "surveyor 0.9878881982893033\n",
      "someome 0.9872515814565512\n",
      "ltd 0.9850963793720661\n",
      "interns 0.9826556531684152\n",
      "arose 0.9823232467435621\n",
      "anyon 0.9817449672290975\n",
      "adecco 0.9795566722708903\n",
      "hostesses 0.9773382123186258\n",
      "bloomfield 0.9769635444019339\n",
      "wildcat 0.9764412364672045\n",
      "enables 0.9760813368304965\n",
      "trak 0.9742296503030068\n",
      "yuba 0.973542118643926\n",
      "negra 0.9731136731429669\n",
      "somewher 0.9709803594603651\n",
      "md 0.9704729563613033\n",
      "sugg 0.9690677768442636\n",
      "recruiters 0.9676666397870065\n",
      "surest 0.9655120314650888\n",
      "prescott 0.9654997535208872\n",
      "y 0.9635876170655643\n",
      "selkirk 0.9629222956697701\n",
      "teton 0.962811425966538\n",
      "                                 \n",
      "************ Label job_search ************\n",
      "Precision:  0.7452830188679245\n",
      "Recall:  0.7022222222222222\n",
      "F-1 Score: 0.7231121281464531\n",
      "**********************************\n",
      "Top 30 word scores for label job_search:\n",
      "dock 0.9867035785599724\n",
      "shortest 0.984178915770702\n",
      "hears 0.982174138937633\n",
      "feedback 0.977026039637058\n",
      "anybodys 0.975252573358889\n",
      "bcd 0.9727424750924402\n",
      "amenable 0.972167330016503\n",
      "de 0.9721468828437816\n",
      "ihop 0.9693120184277847\n",
      "attractions 0.9675826486087781\n",
      "vet 0.9670508682328527\n",
      "visa 0.9668872274602568\n",
      "lukes 0.9668280115408381\n",
      "integration 0.9665238577238338\n",
      "services 0.9661437798281511\n",
      "careerbuilder 0.9649018328096569\n",
      "repairman 0.9643539109251892\n",
      "colliers 0.9633786716037913\n",
      "fl 0.9623227746773357\n",
      "google 0.9612355634702041\n",
      "consulting 0.9611089498067681\n",
      "termination 0.9606320599594942\n",
      "university 0.959750840553607\n",
      "freelancer 0.9596827572264014\n",
      "learner 0.9595561079965326\n",
      "dalai 0.959307326965442\n",
      "search 0.9584115522965998\n",
      "nails 0.9577225295787608\n",
      "med 0.9546423658120445\n",
      "gmail 0.9538832857497686\n",
      "                                 \n",
      "************ Label lost_job_1mo ************\n",
      "Precision:  0.7740112994350282\n",
      "Recall:  0.7569060773480663\n",
      "F-1 Score: 0.7653631284916201\n",
      "**********************************\n",
      "Top 30 word scores for label lost_job_1mo:\n",
      "amarillo 0.9855352514166237\n",
      "revenue 0.9846739941170706\n",
      "bakersfield 0.9844956546702162\n",
      "argentina 0.982357099802654\n",
      "mohegan 0.9791169704051925\n",
      "wuddup 0.973796297963126\n",
      "lobster 0.9727228654146604\n",
      "whoooop 0.9695063738588828\n",
      "au 0.9685710426629195\n",
      "payday 0.9676480843734274\n",
      "gross 0.9642982906694036\n",
      "canned 0.9640796971499701\n",
      "knoxville 0.9633950881814101\n",
      "shutting 0.9631936429402956\n",
      "lake 0.9618125632202492\n",
      "orange 0.9585686812753051\n",
      "ayyyyyye 0.9584868468430316\n",
      "needless 0.957377737368166\n",
      "lifeisgood 0.9569209694890375\n",
      "diagnosed 0.9553456637249068\n",
      "chemical 0.9492799543420549\n",
      "california 0.948052050220661\n",
      "es_f 0.9470941407899579\n",
      "mesquite 0.9465438545423551\n",
      "es 0.9437100188363355\n",
      "houston 0.9434346420119831\n",
      "champagne 0.9407086658733592\n",
      "discharge 0.9406320403061588\n",
      "contacted 0.9396740862964149\n",
      "dyed 0.9388126685819114\n",
      "                                 \n"
     ]
    }
   ],
   "source": [
    "for label in [\"is_hired_1mo\",\"is_unemployed\",\"job_offer\",\"job_search\",\"lost_job_1mo\"]:\n",
    "    #import data\n",
    "    train_file_name = \"train_{}.csv\".format(label)\n",
    "    val_file_name = \"val_{}.csv\".format(label)\n",
    "    df_train = pd.read_csv(os.path.join(path, train_file_name))\n",
    "    df_val = pd.read_csv(os.path.join(path, val_file_name))\n",
    "    #determine unique words in input data\n",
    "    words = df_train['text'].str.lower().str.findall(\"\\w+\")\n",
    "    unique_words = set()\n",
    "    for x in words:\n",
    "        unique_words.update(x)\n",
    "    #import word embeddings\n",
    "    train_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_train[\"text\"]]))\n",
    "    validation_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_val[\"text\"]]))\n",
    "    #define and train model\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(train_vecs_glove_mean,df_train[\"class\"])\n",
    "    #model evaluation\n",
    "    df_val[\"class_predict\"] = clf.predict(validation_vecs_glove_mean)\n",
    "    TP, FP, TN, FN = perf_measure(df_val[\"class\"], df_val[\"class_predict\"])\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    f1 = 2*(precision*recall)/(precision+recall)\n",
    "    print(\"************ Label {} ************\".format(label))\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F-1 Score:\", f1)\n",
    "    print(\"**********************************\")\n",
    "    #define regression coefficients\n",
    "    coeff_array = clf.coef_[0]\n",
    "    score_dict = {}\n",
    "    for word in word_list:\n",
    "        if word in unique_words: #and english_dic.check(word)\n",
    "            word_vector = glove_twitter[word]\n",
    "            word_score = 1/(1+np.exp(-np.sum(word_vector*coeff_array)))\n",
    "            score_dict[word] = word_score\n",
    "    print(\"Top 30 word scores for label {}:\".format(label))\n",
    "    for word in sorted(score_dict, key=score_dict.get, reverse=True)[:30]:\n",
    "        print(word,score_dict[word])\n",
    "    print(\"                                 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
