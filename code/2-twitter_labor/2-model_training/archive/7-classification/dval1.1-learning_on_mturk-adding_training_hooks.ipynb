{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": [
     47
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/17/2020 17:56:11 - INFO - root -   {'run_text': 'multilabel toxic comments with freezable layers', 'train_size': -1, 'val_size': -1, 'log_path': PosixPath('../mturk_mar6/log'), 'full_data_dir': PosixPath('../mturk_mar6/data'), 'data_dir': PosixPath('../mturk_mar6/data'), 'task_name': 'intent', 'no_cuda': True, 'output_dir': PosixPath('../mturk_mar6/output_every_epoch'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 8, 'eval_batch_size': 16, 'learning_rate': 5e-05, 'num_train_epochs': 10, 'warmup_proportion': 0.0, 'local_rank': -1, 'seed': 42, 'gradient_accumulation_steps': 1, 'optimize_on_cpu': False, 'fp16': False, 'fp16_opt_level': 'O1', 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'max_steps': -1, 'warmup_steps': 500, 'logging_steps': 50, 'eval_all_checkpoints': True, 'overwrite_output_dir': True, 'overwrite_cache': False, 'loss_scale': 128, 'model_name': 'bert-base-uncased', 'model_type': 'bert'}\n",
      "03/17/2020 17:56:11 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/dval/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/17/2020 17:56:11 - INFO - root -   Loading features from cached file ../mturk_mar6/data/cache/cached_bert_train_multi_label_512_train.csv\n",
      "03/17/2020 17:56:12 - INFO - root -   Loading features from cached file ../mturk_mar6/data/cache/cached_bert_dev_multi_label_512_val.csv\n",
      "num_labels 5\n",
      "time taken to load all this stuff: 0.4796319007873535 seconds\n"
     ]
    }
   ],
   "source": [
    "#gets all this setup\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "# import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, \\\n",
    "MultiLabelTextProcessor, convert_examples_to_features\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# LOG_PATH=Path('/scratch/da2734/twitter/mturk_mar6/log/')\n",
    "# DATA_PATH=Path('/scratch/da2734/twitter/mturk_mar6/data')\n",
    "# LABEL_PATH=Path('/scratch/da2734/twitter/mturk_mar6/data/')\n",
    "# OUTPUT_PATH=Path('/scratch/da2734/twitter/mturk_mar6/output_100')\n",
    "\n",
    "LOG_PATH=Path('../mturk_mar6/log/')\n",
    "DATA_PATH=Path('../mturk_mar6/data')\n",
    "LABEL_PATH=Path('../mturk_mar6/data/')\n",
    "OUTPUT_PATH=Path('../mturk_mar6/output_every_epoch/')\n",
    "\n",
    "FINETUNED_PATH = None\n",
    "\n",
    "args = Box({\n",
    "    \"run_text\": \"multilabel toxic comments with freezable layers\",\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"log_path\": LOG_PATH,\n",
    "    \"full_data_dir\": DATA_PATH,\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"task_name\": \"labor_market_classification\",\n",
    "    \"no_cuda\": True,\n",
    "#     \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "    \"output_dir\": OUTPUT_PATH,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"warmup_proportion\": 0.0,\n",
    "#     \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimize_on_cpu\": False,\n",
    "    \"fp16\": False,\n",
    "    \"fp16_opt_level\": \"O1\",\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"max_steps\": -1,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"logging_steps\": 50,\n",
    "    \"eval_all_checkpoints\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"overwrite_cache\": False,\n",
    "    \"seed\": 42,\n",
    "    \"loss_scale\": 128,\n",
    "    \"task_name\": 'intent',\n",
    "    \"model_name\": 'bert-base-uncased',\n",
    "    \"model_type\": 'bert'\n",
    "})\n",
    "\n",
    "import logging\n",
    "\n",
    "logfile = str(LOG_PATH/'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler(logfile),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logger.info(args)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    args.multi_gpu = True\n",
    "else:\n",
    "    args.multi_gpu = False\n",
    "\n",
    "# label_cols = [\"job_loss\",\"is_unemployed\",\"job_search\",\"is_hired\",\"job_offer\"]\n",
    "label_cols = [\"is_unemployed\", \"lost_job_1mo\", \"job_search\",\"is_hired_1mo\",\"job_offer\"]\n",
    "\n",
    "databunch = BertDataBunch(\n",
    "                        args['data_dir'], \n",
    "                        LABEL_PATH, \n",
    "                        args.model_name, \n",
    "                        train_file='train.csv', \n",
    "                        val_file='val.csv',\n",
    "                        # test_data='test.csv',\n",
    "                        #name of the column in the train file that containts the tweet text                    \n",
    "                        text_col=\"text\", \n",
    "                        label_col=label_cols,\n",
    "                        batch_size_per_gpu=args['train_batch_size'], \n",
    "                        max_seq_length=args['max_seq_length'], \n",
    "                        multi_gpu=args.multi_gpu, \n",
    "                        multi_label=True, \n",
    "                        model_type=args.model_type)\n",
    "\n",
    "num_labels = len(databunch.labels)\n",
    "print('num_labels', num_labels)\n",
    "\n",
    "\n",
    "print('time taken to load all this stuff:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databunch.train_dl.dataset[2][3] # this train_dlgives us the training dataset for example 2's labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics defined: https://github.com/kaushaltrivedi/fast-bert/blob/d89e2aa01d948d6d3cdea7ad106bf5792fea7dfa/fast_bert/metrics.py\n",
    "metrics = []\n",
    "metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\n",
    "metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "metrics.append({'name': 'accuracy', 'function': accuracy})\n",
    "metrics.append({'name': 'accuracy_multilabel', 'function': accuracy_multilabel})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/17/2020 17:56:31 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/dval/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "03/17/2020 17:56:31 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 5,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "bert-base-uncased\n",
      "<class 'str'>\n",
      "03/17/2020 17:56:31 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/dval/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "03/17/2020 17:56:33 - INFO - transformers.modeling_utils -   Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "03/17/2020 17:56:33 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "learner = BertLearner.from_pretrained_model(\n",
    "                                            databunch, \n",
    "                                            pretrained_path=args.model_name, \n",
    "                                            metrics=metrics, \n",
    "                                            device=device, \n",
    "                                            logger=logger, \n",
    "                                            output_dir=args.output_dir, \n",
    "                                            finetuned_wgts_path=FINETUNED_PATH, \n",
    "                                            warmup_steps=args.warmup_steps,\n",
    "                                            multi_gpu=False, \n",
    "                                            is_fp16=args.fp16, \n",
    "                                            multi_label=True, \n",
    "                                            logging_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# learner.fit(args.num_train_epochs, args.learning_rate, validate=True) #this trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learner.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/17/2020 17:56:22 - INFO - transformers.configuration_utils -   Configuration saved in ../mturk_mar6/output_every_epoch/model_out/config.json\n",
      "03/17/2020 17:56:23 - INFO - transformers.modeling_utils -   Model weights saved in ../mturk_mar6/output_every_epoch/model_out/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "learner.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I just received a job offer']\n",
    "predictions = learner.predict_batch(texts)\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
