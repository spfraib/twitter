{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure you have all libraries installed. \n",
    "use conda environment: /scratch/da2734/twitter/worldbank_twitter_environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of cpus: 16\n",
      "memory available (GB): 251.8069610595703\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "workers = os.cpu_count()\n",
    "if 'sched_getaffinity' in dir(os):\n",
    "    workers = len(os.sched_getaffinity(0))\n",
    "print('number of cpus:', workers)\n",
    "\n",
    "from psutil import virtual_memory\n",
    "\n",
    "mem = virtual_memory()\n",
    "mem.total  # total physical memory available\n",
    "\n",
    "import re\n",
    "with open('/proc/meminfo') as f:\n",
    "    meminfo = f.read()\n",
    "matched = re.search(r'^MemTotal:\\s+(\\d+)', meminfo)\n",
    "if matched: \n",
    "    mem_total_kB = int(matched.groups()[0])\n",
    "# meminfo \n",
    "    \n",
    "print('memory available (GB):', mem_total_kB/1024/1024)\n",
    "\n",
    "# import os\n",
    "# mem=str(os.popen('free -t -m').readlines())\n",
    "# mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#gets all this setup\n",
    "import time\n",
    "start_time = time.time()\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "# import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "# from fast_bert.metrics import accuracy_multilabel, accuracy_thresh, fbeta, roc_auc, accuracy\n",
    "from fast_bert.metrics import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "# LOG_PATH=Path('/scratch/da2734/twitter/sana/log/')\n",
    "# DATA_PATH=Path('/scratch/da2734/twitter/sana/data')\n",
    "# LABEL_PATH=Path('/scratch/da2734/twitter/sana/data/')\n",
    "# OUTPUT_PATH=Path('/scratch/da2734/twitter/sana/output/')\n",
    "LOG_PATH=Path('/scratch/da2734/twitter/mturk_mar6/log/')\n",
    "DATA_PATH=Path('/scratch/da2734/twitter/mturk_mar6/data')\n",
    "LABEL_PATH=Path('/scratch/da2734/twitter/mturk_mar6/data/')\n",
    "OUTPUT_PATH=Path('/scratch/da2734/twitter/mturk_mar6/output_100')\n",
    "FINETUNED_PATH = None\n",
    "\n",
    "args = Box({\n",
    "    \"run_text\": \"multilabel toxic comments with freezable layers\",\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"log_path\": LOG_PATH,\n",
    "    \"full_data_dir\": DATA_PATH,\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"task_name\": \"labor_market_classification\",\n",
    "    \"no_cuda\": False,\n",
    "#     \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "    \"output_dir\": OUTPUT_PATH,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"eval_batch_size\": 200,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_train_epochs\": 6,\n",
    "    \"warmup_proportion\": 0.0,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimize_on_cpu\": False,\n",
    "    \"fp16\": False,\n",
    "    \"fp16_opt_level\": \"O1\",\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"max_steps\": -1,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"logging_steps\": 50,\n",
    "    \"eval_all_checkpoints\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"overwrite_cache\": False,\n",
    "    \"seed\": 42,\n",
    "    \"loss_scale\": 128,\n",
    "    \"task_name\": 'intent',\n",
    "    \"model_name\": 'bert-base-uncased',\n",
    "    \"model_type\": 'bert'\n",
    "})\n",
    "\n",
    "import logging\n",
    "\n",
    "logfile = str(LOG_PATH/'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler(logfile),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# logger.info(args)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    args.multi_gpu = True\n",
    "else:\n",
    "    args.multi_gpu = False\n",
    "\n",
    "label_cols = [\"job_loss\",\"is_unemployed\",\"job_search\",\"is_hired\",\"job_offer\"]\n",
    "\n",
    "# databunch defined here https://github.com/kaushaltrivedi/fast-bert/blob/master/fast_bert/data_cls.py\n",
    "databunch = BertDataBunch(\n",
    "                        args['data_dir'], \n",
    "                        LABEL_PATH, \n",
    "                        args.model_name, \n",
    "                        train_file='train.csv', \n",
    "                        val_file='val.csv',\n",
    "                        # test_data='test.csv',\n",
    "                        text_col=\"text\", #this is the name of the column in the train file that containts the tweet text\n",
    "                        label_col=label_cols,\n",
    "                        batch_size_per_gpu=args['train_batch_size'], \n",
    "                        max_seq_length=args['max_seq_length'], \n",
    "                        multi_gpu=args.multi_gpu, \n",
    "                        multi_label=True, \n",
    "                        model_type=args.model_type)\n",
    "\n",
    "num_labels = len(databunch.labels)\n",
    "print('num_labels', num_labels)\n",
    "\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA VERSION')\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__Devices')\n",
    "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "\n",
    "print ('Available devices ', torch.cuda.device_count())\n",
    "# print ('Current cuda device ', torch.cuda.current_device)\n",
    "\n",
    "# metrics defined: https://github.com/kaushaltrivedi/fast-bert/blob/d89e2aa01d948d6d3cdea7ad106bf5792fea7dfa/fast_bert/metrics.py\n",
    "metrics = []\n",
    "metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\n",
    "metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "metrics.append({'name': 'roc_auc_save_to_plot', 'function': roc_auc_save_to_plot})\n",
    "metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "metrics.append({'name': 'accuracy', 'function': accuracy})\n",
    "metrics.append({'name': 'accuracy_multilabel', 'function': accuracy_multilabel})\n",
    "\n",
    "\n",
    "learner = BertLearner.from_pretrained_model(\n",
    "                                            databunch, \n",
    "                                            pretrained_path='/scratch/da2734/twitter/mturk_mar6/output_100/model_out/', \n",
    "                                            metrics=metrics, \n",
    "                                            device=device, \n",
    "                                            logger=logger, \n",
    "                                            output_dir=args.output_dir, \n",
    "                                            finetuned_wgts_path=FINETUNED_PATH, \n",
    "                                            warmup_steps=args.warmup_steps,\n",
    "                                            multi_gpu=args.multi_gpu, \n",
    "                                            is_fp16=args.fp16, \n",
    "                                            multi_label=True, \n",
    "                                            logging_steps=0)\n",
    "\n",
    "print('time taken to load all this stuff:', str(time.time() - start_time), 'seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading random and filtered samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to load keyword filtered sample: 329.5322268009186 seconds\n",
      "(92121093, 11)\n"
     ]
    }
   ],
   "source": [
    "# filtered contains 8G of data!!\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "country_code = 'US'\n",
    "month = '2012-1'\n",
    "path_to_data = '/scratch/spf248/twitter/data/classification/US/filtered/'\n",
    "tweets_filtered=pq.ParquetDataset(glob(os.path.join(path_to_data,                                           \n",
    "#                                            country_code,\n",
    "#                                            month,\n",
    "                                           '*.parquet'))).read().to_pandas()\n",
    "print('time taken to load keyword filtered sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to load random sample: 320.91191005706787 seconds\n",
      "(92114009, 11)\n"
     ]
    }
   ],
   "source": [
    "# random contains 7.3G of data!!\n",
    "import time\n",
    "start_time = time.time()\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "country_code = 'US'\n",
    "month = '2012-1'\n",
    "path_to_data = '/scratch/spf248/twitter/data/classification/US/random'\n",
    "tweets_random=pq.ParquetDataset(glob(os.path.join(path_to_data,                                           \n",
    "#                                            country_code,\n",
    "#                                            month,\n",
    "                                           '*.parquet'))).read().to_pandas()\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_random.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read csv output from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to load random sample: 69.51977372169495 seconds\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "start_time = time.time()\n",
    "model_output_path = '/scratch/spf248/twitter/data/classification/US/BERT/twitter_sam/mturk_mar6/pred/'\n",
    "model_output_filtered = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'filtered*.csv')], ignore_index = True)\n",
    "print('time taken to load filtered sample:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken to load random sample: 100.03441596031189 seconds\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "start_time = time.time()\n",
    "model_output_path = '/scratch/spf248/twitter/data/classification/US/BERT/twitter_sam/mturk_mar6/pred/'\n",
    "model_output_random = pd.concat([pd.read_csv(f) for f in glob.glob(model_output_path+'random*.csv')], ignore_index = True)\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with smaller samples for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a63e5199dd0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_random_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_random_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweets_filtered_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_filtered_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_random' is not defined"
     ]
    }
   ],
   "source": [
    "tweets_random_sample = tweets_random.head(10000)\n",
    "print(tweets_random_sample.shape)\n",
    "tweets_filtered_sample = tweets_filtered.head(10000)\n",
    "print(tweets_filtered_sample.shape)\n",
    "\n",
    "model_output_filtered_sample = model_output_filtered.head(10000)\n",
    "print(model_output_filtered_sample.shape)\n",
    "model_output_random_sample = model_output_random.head(10000)\n",
    "print(model_output_filtered_sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finally concat with tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "all_filtered = pd.concat([df_filtered, tweets_filtered], axis=1)\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "all_random = pd.concat([df_random, tweets_random], axis=1)\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# then we can pick tweets close to any threshold..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'job_search'\n",
    "threshold = 0.5\n",
    "boundary_width = 0.05\n",
    "start_time = time.time()\n",
    "all_filtered_boundary = all_filtered.loc[(all_filtered[column] >= threshold - boundary_width) & \n",
    "                       (all_filtered[column] <= threshold + boundary_width)]\n",
    "print(all_filtered_boundary['text'])\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "all_random_boundary = all_random.loc[(all_random[column] >= threshold - boundary_width) & \n",
    "                       (all_random[column] <= threshold + boundary_width)]\n",
    "print(all_filtered_boundary['text'])\n",
    "print('time taken:', str(time.time() - start_time), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
