{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started 10.2-BERT-deploying-1M-randomONLY_1pct.py\n",
      "SLURM_JOB_ID 123123123\n",
      "SLURM_ARRAY_TASK_ID 10\n",
      "SLURM_ARRAY_TASK_COUNT 500\n",
      "Load Random Tweets:\n",
      "#files: 1\n",
      "/scratch/spf248/twitter/data/classification/US/random_1perct_sample/part-00974-1c1e6466-49fa-411b-beb0-276d14cdffab-c000.snappy.parquet\n",
      "(42521, 2)\n",
      "time taken to load random sample: 0.07409262657165527 seconds\n",
      "(1000, 2)\n",
      "dropping duplicates:\n",
      "time taken to load random sample: 0.004042863845825195 seconds\n",
      "(1000, 2)\n",
      "\n",
      "\n",
      "!!!!! is_unemployed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/da2734/pyenv_dval_wb_twitter/py3.7/lib/python3.7/site-packages/ipykernel_launcher.py:69: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model: 2.4975130558013916 seconds\n",
      "Predictions of Random Tweets:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e4f704e8b2483b83e4b8f143061a04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f6fd98e700483f8fc8aca264def2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=125.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time taken: 5.255791425704956 seconds\n",
      "per tweet: 0.0052560982704162595 seconds\n",
      "Save Predictions of Random Tweets:\n",
      "time taken: 0.006187915802001953 seconds\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# run using: sbatch --array=0-9 7.9-get-predictions-from-BERT.sh\n",
    "\n",
    "print('started 10.2-BERT-deploying-1M-randomONLY_1pct.py')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# column = sys.argv[1]\n",
    "# column = 'is_unemployed'\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# loading the model\n",
    "####################################################################################################################################\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "from transformers import BertTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from box import Box\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# import sys\n",
    "import random\n",
    "import numpy as np\n",
    "# import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import classification_report\n",
    "import gensim.downloader as api\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "# sys.path.append('../')\n",
    "sys.path.append('../twitter_labor/model_training/simple_transformers/')\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "\n",
    "# from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "# from fast_bert.data_cls import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, \\\n",
    "#     convert_examples_to_features\n",
    "# from fast_bert.learner_cls import BertLearner\n",
    "# # from fast_bert.metrics import accuracy_multilabel, accuracy_thresh, fbeta, roc_auc, accuracy\n",
    "# from fast_bert.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "root_path='/scratch/da2734/twitter/jobs/running_on_200Msamples/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_env_var(varname, default):\n",
    "    if os.environ.get(varname) != None:\n",
    "        var = int(os.environ.get(varname))\n",
    "        print(varname, ':', var)\n",
    "    else:\n",
    "        var = default\n",
    "        print(varname, ':', var, '(Default)')\n",
    "    return var\n",
    "\n",
    "\n",
    "# # Choose Number of Nodes To Distribute Credentials: e.g. jobarray=0-4, cpu_per_task=20, credentials = 90 (<100)\n",
    "# SLURM_JOB_ID = get_env_var('SLURM_JOB_ID', 0)\n",
    "# SLURM_ARRAY_TASK_ID = get_env_var('SLURM_ARRAY_TASK_ID', 0)\n",
    "# SLURM_ARRAY_TASK_COUNT = get_env_var('SLURM_ARRAY_TASK_COUNT', 1)\n",
    "\n",
    "SLURM_JOB_ID = 123123123\n",
    "SLURM_ARRAY_TASK_ID = 10\n",
    "SLURM_ARRAY_TASK_COUNT = 500\n",
    "\n",
    "\n",
    "print('SLURM_JOB_ID', SLURM_JOB_ID)\n",
    "print('SLURM_ARRAY_TASK_ID', SLURM_ARRAY_TASK_ID)\n",
    "print('SLURM_ARRAY_TASK_COUNT', SLURM_ARRAY_TASK_COUNT)\n",
    "\n",
    "\n",
    "# ####################################################################################################################################\n",
    "# # loading data\n",
    "# ####################################################################################################################################\n",
    "\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path_to_data='/scratch/spf248/twitter/data/classification/US/'\n",
    "\n",
    "\n",
    "print('Load Random Tweets:')\n",
    "# random contains 7.3G of data!!\n",
    "start_time = time.time()\n",
    "\n",
    "paths_to_random=list(np.array_split(\n",
    "                        # glob(os.path.join(path_to_data,'random','*.parquet')),\n",
    "                        # glob(os.path.join(path_to_data,'random_10perct_sample','*.parquet')),\n",
    "                        glob(os.path.join(path_to_data,'random_1perct_sample','*.parquet')),\n",
    "                        SLURM_ARRAY_TASK_COUNT)[SLURM_ARRAY_TASK_ID])\n",
    "print('#files:', len(paths_to_random))\n",
    "\n",
    "tweets_random=pd.DataFrame()\n",
    "for file in paths_to_random:\n",
    "    print(file)\n",
    "    tweets_random=pd.concat([tweets_random,pd.read_parquet(file)[['tweet_id','text']]])\n",
    "    print(tweets_random.shape)\n",
    "\n",
    "    break\n",
    "\n",
    "tweets_random = tweets_random[:1000]\n",
    "\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_random.shape)\n",
    "\n",
    "\n",
    "print('dropping duplicates:')\n",
    "# random contains 7.3G of data!!\n",
    "start_time = time.time()\n",
    "tweets_random = tweets_random.drop_duplicates('text')\n",
    "print('time taken to load random sample:', str(time.time() - start_time), 'seconds')\n",
    "print(tweets_random.shape)\n",
    "\n",
    "\n",
    "for column in [\"is_unemployed\", \"lost_job_1mo\", \"job_search\", \"is_hired_1mo\", \"job_offer\"]:\n",
    "\n",
    "    print('\\n\\n!!!!!', column)\n",
    "#     print(x)\n",
    "\n",
    "    start = time.time()\n",
    "#     learner = create_model(column, best_epochs[column])\n",
    "    model = ClassificationModel('bert', \n",
    "                            '/scratch/da2734/twitter/jobs/training_binary/simple_transformers_manu_bertbase/is_hired_1mo/', \n",
    "                        args={'evaluate_during_training': True, \n",
    "                              'evaluate_during_training_verbose': True, \n",
    "                              'num_train_epochs': 20})\n",
    "    print('load model:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "#     print('Predictions of Filtered Tweets:')\n",
    "#     start_time = time.time()\n",
    "#     predictions_filtered = learner.predict_batch(tweets_filtered['text'].values.tolist())\n",
    "#     print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "#     print('per tweet:', (time.time() - start_time)/tweets_filtered.shape[0], 'seconds')\n",
    "\n",
    "#     # In[ ]:\n",
    "\n",
    "#     data_path = \"/scratch/da2734/twitter/data/may20_9Klabels/data_binary_pos_neg_balanced/\"\n",
    "#     print(\"************ {} ************\".format(column))\n",
    "\n",
    "#     train_file_name = \"train_{}.csv\".format(column)\n",
    "#     val_file_name = \"val_{}.csv\".format(column)\n",
    "#     #download data\n",
    "#     df_train = pd.read_csv(os.path.join(data_path, train_file_name))\n",
    "# #     print(df_train.head())\n",
    "#     df_val = pd.read_csv(os.path.join(data_path, val_file_name))\n",
    "#     #create embeddings\n",
    "#     train_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_train[\"text\"]]))\n",
    "#     validation_vecs_glove_mean = scale(np.concatenate([get_w2v_general(z, 200, glove_twitter,'mean') for z in df_val[\"text\"]]))\n",
    "#     #train\n",
    "#     clf = LogisticRegression(max_iter=1000)\n",
    "#     clf.fit(train_vecs_glove_mean,df_train[\"class\"])\n",
    "#     #evaluate\n",
    "#     df_val[\"class_predict\"] = clf.predict(validation_vecs_glove_mean)\n",
    "#     TP, FP, TN, FN = perf_measure(df_val[\"class\"], df_val[\"class_predict\"])\n",
    "#     print(\"Precision: \", TP/(TP+FP))\n",
    "#     print(\"Recall: \", TP/(TP+FN))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Predictions of Random Tweets:')\n",
    "    start_time = time.time()\n",
    "    #     predictions_random = learner.predict_batch(tweets_random['text'].values.tolist())\n",
    "#     predictions_random = clf.predict_proba(random_data_vecs_glove_mean)\n",
    "    predictions, predictions_random = model.predict(tweets_random['text'].values.tolist())\n",
    "#     print(type(predictions_random))\n",
    "    # print(predictions_random)\n",
    "\n",
    "    print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "    print('per tweet:', (time.time() - start_time)/tweets_random.shape[0], 'seconds')\n",
    "\n",
    "    # In[ ]:\n",
    "\n",
    "\n",
    "    #     print('Save Predictions of Filtered Tweets:')\n",
    "    #     start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    #     df_filtered = predictions_filtered.set_index(tweets_filtered.tweet_id).rename(columns={\n",
    "    #             '0':'pos_model',\n",
    "    #             '1':'neg_model',\n",
    "    #     })\n",
    "\n",
    "    if not os.path.exists(os.path.join(root_path,'pred_output_1pct_sample_BERT', column)):\n",
    "        print('>>>> directory doesnt exists, creating it')\n",
    "        os.makedirs(os.path.join(root_path,'pred_output_1pct_sample_BERT', column))\n",
    "\n",
    "    #     # if not os.path.exists(os.path.join(root_path,'pred_output_10pct_sample', column)):\n",
    "    #     #     os.makedirs(os.path.join(root_path,'pred_output_10pct_sample', column))\n",
    "\n",
    "    #     # if not os.path.exists(os.path.join(root_path,'pred_output_full', column)):\n",
    "    #     #     os.makedirs(os.path.join(root_path,'pred_output_full', column))\n",
    "\n",
    "    #     df_filtered.to_csv(\n",
    "    #             # os.path.join(root_path,'pred_output', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "    #             os.path.join(root_path,'pred_output_1pct_sample', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "    #         )\n",
    "\n",
    "    #     print(os.path.join(root_path,'pred_output_1pct_sample', column, 'filtered'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'), 'saved')\n",
    "\n",
    "    #     print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Save Predictions of Random Tweets:')\n",
    "    start_time = time.time()\n",
    "    # predictions_random_df = pd.DataFrame(data=predictions_random, columns = ['neg', 'pos'])\n",
    "    predictions_random_df = pd.DataFrame(data=predictions_random, columns = ['first', 'second'])\n",
    "    df_random = predictions_random_df.set_index(tweets_random.tweet_id)\n",
    "    # df_random = predictions_random.set_index(tweets_random.tweet_id).rename(columns={\n",
    "    #         '0':'pos_model',\n",
    "    #         '1':'neg_model',\n",
    "    # })\n",
    "\n",
    "    # if not os.path.exists(os.path.join(root_path,'pred_output_10pct_sample', column)):\n",
    "    #     os.makedirs(os.path.join(root_path,'pred_output_10pct_sample', column))\n",
    "\n",
    "    df_random.to_csv(\n",
    "        # os.path.join(root_path,'pred_output', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "        os.path.join(root_path,'pred_output_1pct_sample_BERT', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv')\n",
    "        )\n",
    "\n",
    "#     print(os.path.join(root_path,'pred_output_1pct_sample_BERT', column, 'random'+'-'+str(SLURM_JOB_ID)+'-'+str(SLURM_ARRAY_TASK_ID)+'.csv'), 'saved')\n",
    "\n",
    "    print('time taken:', str(time.time() - start_time), 'seconds')\n",
    "\n",
    "\n",
    "    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel('bert', \n",
    "                            '/scratch/da2734/twitter/jobs/training_binary/simple_transformers_manu_bertbase/is_hired_1mo/', \n",
    "                        args={'evaluate_during_training': True, \n",
    "                              'evaluate_during_training_verbose': True, \n",
    "                              'num_train_epochs': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11509bf42be4f148d0beaab58d1475f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0f93981efe43d49f77a1a0161fa6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1]), array([[-1.5437384,  1.4793632]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\"I just got hired!!\"])\n",
    "predictions, raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42843beb2764dea9718ea231989e63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a518d93be53b4bc797eaee3f95fa5fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0]), array([[0.6756104 , 0.08714478]], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict([\"the milk is bad\" * 25])\n",
    "predictions, raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([[ 0.8763451 , -0.33580813]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1168644259400207"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(0.8763451)+math.exp(-0.33580813)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b003c39344ef4f4189f038d989a939b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d4e3fa50b34446b4d1873a700dbd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions, raw_outputs = model.predict(tweets_random['text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83506954, -0.5212148 ],\n",
       "       [ 1.1828201 , -0.75153786],\n",
       "       [ 1.3079685 , -0.9148362 ],\n",
       "       ...,\n",
       "       [ 1.4786477 , -1.1994929 ],\n",
       "       [ 1.0837046 , -0.57951456],\n",
       "       [ 1.2725701 , -0.8254349 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
