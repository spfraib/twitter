{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "from glob import glob\n",
    "import pyarrow.parquet as pq\n",
    "import socket\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from pytorch_pretrained_bert.modeling import BertForNextSentencePrediction\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "from fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(\"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sentences:\n",
      "\n",
      "I lost my job today\n",
      "I was fired earlier this week\n",
      "Now I am unemployed\n",
      "I am currently not working\n",
      "I am searching for a new position\n",
      "Anyone hiring?\n",
      "I got hired today\n",
      "I recently started working at my new job\n",
      "Here is a job opportunity you might be interested in\n",
      "Looking for a new position?\n"
     ]
    }
   ],
   "source": [
    "targets = {\n",
    "0:'I lost my job today',\n",
    "5:'I was fired earlier this week',\n",
    "# 'I recently got laid off',\n",
    "# 'I just quit my job',\n",
    "\n",
    "1:'Now I am unemployed',\n",
    "6:'I am currently not working',\n",
    "    \n",
    "2:'I am searching for a new position',\n",
    "7:'Anyone hiring?',\n",
    "# 'I am looking for a job',\n",
    "    \n",
    "3:'I got hired today',\n",
    "8:'I recently started working at my new job',\n",
    "# 'I just found a position',\n",
    "    \n",
    "4:'Here is a job opportunity you might be interested in',\n",
    "9:'Looking for a new position?', \n",
    "}\n",
    "\n",
    "print('Target Sentences:\\n')\n",
    "print('\\n'.join(targets.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_ARRAY_TASK_ID : 0 (Default)\n"
     ]
    }
   ],
   "source": [
    "def get_env_var(varname,default):\n",
    "    \n",
    "    if os.environ.get(varname) != None:\n",
    "        var = int(os.environ.get(varname))\n",
    "        print(varname,':', var)\n",
    "    else:\n",
    "        var = default\n",
    "        print(varname,':', var,'(Default)')\n",
    "    return var\n",
    "\n",
    "SLURM_ARRAY_TASK_ID = get_env_var('SLURM_ARRAY_TASK_ID',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Partitions: 50\n"
     ]
    }
   ],
   "source": [
    "n_partition = 50\n",
    "print('# Partitions:', n_partition)\n",
    "\n",
    "if SLURM_ARRAY_TASK_ID>=n_partition*len(targets):\n",
    "    sys.exit('Sorry come again ;)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sentence Index: 0  /  9\n",
      "Partition Index: 0  /  49\n"
     ]
    }
   ],
   "source": [
    "i_target, i_partition = np.unravel_index(SLURM_ARRAY_TASK_ID, (len(targets), n_partition))\n",
    "print(\"Target Sentence Index:\", i_target, ' / ', len(targets)-1)\n",
    "print(\"Partition Index:\", i_partition, ' / ', n_partition-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Sentence: I lost my job today\n"
     ]
    }
   ],
   "source": [
    "target = targets[i_target]\n",
    "print(\"Target Sentence:\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: US\n",
      "path to data: ../../data/classification/US\n",
      "output file: target-0-partition-0.csv\n"
     ]
    }
   ],
   "source": [
    "country = 'US'\n",
    "print('Country:', country)\n",
    "\n",
    "if 'samuel' in socket.gethostname().lower():\n",
    "    path_to_data = os.path.join('../../data/classification/',country)\n",
    "else:\n",
    "    path_to_data = os.path.join('/scratch/spf248/twitter/data/classification/',country)\n",
    "print(\"path to data:\", path_to_data)\n",
    "\n",
    "os.makedirs(os.path.join(path_to_data,'similarity'), exist_ok=True)\n",
    "\n",
    "output_file  = 'target-'+str(i_target)+'-partition-'+str(i_partition)+'.csv'\n",
    "print(\"output file:\", output_file)\n",
    "\n",
    "if os.path.exists(os.path.join(path_to_data,output_file)):\n",
    "    sys.exit('Let\"s go for a walk!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling: 1.0 %\n"
     ]
    }
   ],
   "source": [
    "frac = 1.0\n",
    "print('Sampling:', frac*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length: 128\n"
     ]
    }
   ],
   "source": [
    "# Default\n",
    "max_seq_length = 128\n",
    "print('max_seq_length:', max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "# Default\n",
    "BATCH_SIZE = 32\n",
    "print('BATCH_SIZE:', BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Files:\n",
      "../../data/classification/US/filtered/part-00000-8b187f2d-a1f8-446d-845c-ad55c236e103-c000.snappy.parquet\n",
      "# Files: 1\n",
      "# Tweets Containing At Least One Keyword: 9111\n",
      "Done in 0 sec\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "files_filtered=list(np.array_split(sorted(glob(os.path.join(path_to_data,'filtered','*.parquet'))),n_partition)[i_partition])\n",
    "print('Import Files:')\n",
    "print('\\n'.join(files_filtered))\n",
    "print('# Files:', len(files_filtered))\n",
    "\n",
    "tweets_filtered=pq.ParquetDataset(files_filtered).read().to_pandas().set_index('tweet_id')['text']\n",
    "print('# Tweets Containing At Least One Keyword:', tweets_filtered.shape[0])\n",
    "\n",
    "print(\"Done in\", round(timer()-start), \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Files:\n",
      "../../data/classification/US/random/part-00000-ee5d86d2-6cc0-4365-82d3-7693f30340c7-c000.snappy.parquet\n",
      "# Files: 1\n",
      "# Random Tweets: 8667\n",
      "Done in 0 sec\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "files_random=list(np.array_split(sorted(glob(os.path.join(path_to_data,'random','*.parquet'))),n_partition)[i_partition])\n",
    "print('Import Files:')\n",
    "print('\\n'.join(files_random))\n",
    "print('# Files:', len(files_random))\n",
    "\n",
    "tweets_random=pq.ParquetDataset(files_random).read().to_pandas().set_index('tweet_id')['text']\n",
    "print('# Random Tweets:', tweets_random.shape[0])\n",
    "\n",
    "print(\"Done in\", round(timer()-start), \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Data...\n",
      "# Tweets Sampled: 178\n",
      "Done in 0 sec\n"
     ]
    }
   ],
   "source": [
    "print('Partition Data...')\n",
    "start = timer()\n",
    "\n",
    "data=pd.concat([tweets_filtered,tweets_random]).sample(frac=frac,random_state=0).copy()\n",
    "print('# Tweets Sampled:', data.shape[0])\n",
    "del tweets_filtered, tweets_random\n",
    "\n",
    "print(\"Done in\", round(timer()-start), \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262.2500000000001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.apply(len).quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id\n",
       "1035899433400000512    RT @nyssabarfield: If another girl got your at...\n",
       "1016808169560002560    RT @DavidBegnaud: The officer in this video - ...\n",
       "1050222272743698432    @politico Nikki Haley isn't as stupid as the N...\n",
       "1042124258703421440    @CBS4Dom @RobCBS4 Beautiful @RobCBS4! Always g...\n",
       "1029291462633172992    @StormyDaniels So are you going to tell us wha...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/22/2019 17:06:36 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/samuelfraiberger/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/22/2019 17:06:37 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /Users/samuelfraiberger/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "12/22/2019 17:06:37 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/samuelfraiberger/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /var/folders/f9/mwb36gs54dl_yvpsjtjrsz_80000gn/T/tmpry4tn3qt\n",
      "12/22/2019 17:06:40 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/22/2019 17:06:43 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, target):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.target = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentence_pair(first_sentences, second_sentences, max_seq_length, tokenizer):\n",
    "    features = []\n",
    "    for (ex_index, (first_sentence, second_sentence)) in enumerate(zip(first_sentences, second_sentences)):\n",
    "        tokens_a = tokenizer.tokenize(first_sentence)\n",
    "\n",
    "        tokens_b = None\n",
    "        tokens_b = tokenizer.tokenize(second_sentence)\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(\n",
    "                    input_ids=input_ids,\n",
    "                    input_mask=input_mask,\n",
    "                    segment_ids=segment_ids,\n",
    "                    target=1\n",
    "        ))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/22/2019 17:06:43 - INFO - bert -   *** Example ***\n",
      "12/22/2019 17:06:43 - INFO - bert -   tokens: [CLS] i lost my job today [SEP] rt @ ny ##ssa ##bar ##field : if another girl got your attention , ion want yours no more [SEP]\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_ids: 101 1045 2439 2026 3105 2651 102 19387 1030 6396 11488 8237 3790 1024 2065 2178 2611 2288 2115 3086 1010 10163 2215 6737 2053 2062 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   *** Example ***\n",
      "12/22/2019 17:06:43 - INFO - bert -   tokens: [CLS] i lost my job today [SEP] rt @ david ##be ##gna ##ud : the officer in this video - who appears to do nothing as a woman is harassed - has been placed on desk duty . he works f ##o ‚Ä¶ [SEP]\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_ids: 101 1045 2439 2026 3105 2651 102 19387 1030 2585 4783 16989 6784 1024 1996 2961 1999 2023 2678 1011 2040 3544 2000 2079 2498 2004 1037 2450 2003 28186 1011 2038 2042 2872 2006 4624 4611 1012 2002 2573 1042 2080 1529 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   *** Example ***\n",
      "12/22/2019 17:06:43 - INFO - bert -   tokens: [CLS] i lost my job today [SEP] @ pol ##itic ##o nikki haley isn ' t as stupid as the never trump dead - end ##er st ##oo ##ges at pol ##itic ##o , national review , the hill , weekly standard , ec ##t . [SEP]\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_ids: 101 1045 2439 2026 3105 2651 102 1030 14955 18291 2080 14470 16624 3475 1005 1056 2004 5236 2004 1996 2196 8398 2757 1011 2203 2121 2358 9541 8449 2012 14955 18291 2080 1010 2120 3319 1010 1996 2940 1010 4882 3115 1010 14925 2102 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   *** Example ***\n",
      "12/22/2019 17:06:43 - INFO - bert -   tokens: [CLS] i lost my job today [SEP] @ cbs ##4 ##dom @ rob ##cb ##s ##4 beautiful @ rob ##cb ##s ##4 ! always great work with the drone footage ! [SEP]\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_ids: 101 1045 2439 2026 3105 2651 102 1030 6568 2549 9527 1030 6487 27421 2015 2549 3376 1030 6487 27421 2015 2549 999 2467 2307 2147 2007 1996 18465 8333 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   *** Example ***\n",
      "12/22/2019 17:06:43 - INFO - bert -   tokens: [CLS] i lost my job today [SEP] @ stormy ##dan ##iel ##s so are you going to tell us what you did ? we are on the edge of our chairs . . . [SEP]\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_ids: 101 1045 2439 2026 3105 2651 102 1030 24166 7847 9257 2015 2061 2024 2017 2183 2000 2425 2149 2054 2017 2106 1029 2057 2024 2006 1996 3341 1997 2256 8397 1012 1012 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/22/2019 17:06:43 - INFO - bert -   segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition Data...\n",
      "Done in 0 sec\n"
     ]
    }
   ],
   "source": [
    "print('Partition Data...')\n",
    "start = timer()\n",
    "\n",
    "sentence_pairs  = convert_sentence_pair(\n",
    "[target]*data.shape[0],\n",
    "data.tolist(),\n",
    "max_seq_length=max_seq_length,\n",
    "tokenizer=tokenizer)\n",
    "\n",
    "print(\"Done in\", round(timer()-start), \"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Sentence Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/22/2019 17:06:43 - INFO - bert -   ***** Running evaluation *****\n",
      "12/22/2019 17:06:43 - INFO - bert -     Num examples = 178\n",
      "12/22/2019 17:06:43 - INFO - bert -     Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer Similarities...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6/6 01:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 65 sec\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = BATCH_SIZE # 16 # 32 # 64 # 128 # 256 # 512 #1024\n",
    "\n",
    "print(\"Infer Similarities...\")\n",
    "start = timer()\n",
    "\n",
    "logger.info(\"***** Running evaluation *****\")\n",
    "all_input_ids   = torch.tensor([f.input_ids for f in sentence_pairs], dtype=torch.long)\n",
    "all_input_mask  = torch.tensor([f.input_mask for f in sentence_pairs], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in sentence_pairs], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "logger.info(\"  Num examples = %d\", len(data))\n",
    "logger.info(\"  Batch size = %d\", BATCH_SIZE)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "res = []\n",
    "\n",
    "mb = progress_bar(eval_dataloader)\n",
    "for input_ids, input_mask, segment_ids in mb:\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res.append(nn.functional.softmax(\n",
    "            model(input_ids, segment_ids, input_mask), dim=1\n",
    "        )[:, 0].detach().cpu().numpy())\n",
    "        \n",
    "res = np.concatenate(res)\n",
    "scores = pd.DataFrame({'score':res,'target':[target]*len(res)},index=data.index)\n",
    "\n",
    "print(\"Done in\", round(timer()-start), \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save...\n",
      "Done in 0 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Save...\")\n",
    "start = timer()\n",
    "\n",
    "scores.to_csv(os.path.join(path_to_data,'similarity',output_file))\n",
    "\n",
    "print(\"Done in\", round(timer()-start), \"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 most similar tweets:\n",
      "\n",
      "I half ass did my makeup today cause I never do my makeup anymore and I have finally quit crying over A Star is Born üí´ https://t.co/GLjZbMUPm9\n",
      "@AP I read no remorse in the article. Just woe is me. Sorry she finally got caught and it cost her. I am glad the rest of the cast isn‚Äôt losing their jobs though.\n",
      "Had a long ass week at Busch Gardens but I had a lot of fun with my new co workers and here‚Äôs a video of one of the the dopest rides in Florida https://t.co/HfptNzvtGW\n",
      "Alright, work is a shit show. I already fucked shit up. I‚Äôm ready for my shift to be over and I still have 5 hours too go. Shoot me.\n",
      "RT @tryna_be_famous: Just seen the nigga that possibly got fired two nights ago walk into the building. Lemme take my ass home.\n",
      "On my way to work. Good morning everyone I hope all has a blessed day!\n",
      "I really love my job.\n",
      "RT @nhannahjones: There is a man in Greenwood TODAY, whose teeth got shot out of his face for working on voting rights, who lives on the ed‚Ä¶\n",
      "if i just sit here and don't do any work maybe they can't make me feel like shit.\n",
      "RT @skinny_que: A bag is a bag, a job is a job https://t.co/ntc6zrXvEP\n",
      "RT @ImTheeBrock: Working on my 6 year plan:\n",
      "1. ?\n",
      "2. ?\n",
      "3. ?\n",
      "4. ?\n",
      "5. ?\n",
      "6. And then they‚Äôll all be sorry.\n",
      "RT @mfz_zz: This fully fucked up my day üò™‚òπÔ∏è https://t.co/owxCrAvFto\n",
      "I need a good capper with proven track record, these hours at work won‚Äôt let me cap shit lol\n",
      "I have my own car but the only time I drive it is to get to work, I always make someone else drive cuz I actually hate it\n",
      "RT @goingglocal: My sister works in a library in the USA. She overheard a Trumper say that he didn't care what Trump had done. He said he d‚Ä¶\n",
      "liberal lies:target #GoFundTheWall donors &amp; doxx she then told her flwrs doxx/harass me-they tried.I called her by her name(publicly on the net)so she could feel the wrongness in her actions.Someone called her boss..doesn‚Äôt like turnabout. Blockd her, shes crying I‚Äôm spammin,Not https://t.co/h4Aw5MrcwL\n",
      "@GordonRamsay @HellsKitchenFOX Unfortunately for the employees in restaurants that hired me, I've watched Hells Kitchen &amp; Kitchen Nightmares ... All of them &amp; on more than one occasion the employees have been heard to mutter üò± OMG they hired Gordon Ramsay.... As I tear their kitchens apart. üíñ Thank you!\n",
      "@PattyArquette @AprilDRyan @PressSec @realDonaldTrump You are a twit. You are a shitty actress. Work on your craft\n",
      "@StormyDaniels So are you going to tell us what you did? We are on the edge of our chairs...\n",
      "RT @HEAVYTITTIES: why u gotta be nice to cops in order for them to do their job? customer service reps get all sorts of disrespect n have 2‚Ä¶\n",
      "RT @itsbrandibitcch: My man busts his ass at work in the heat all day to support me, therefore I have no issue cleaning the house or washin‚Ä¶\n",
      "Omarosa gets a job on the show twice plus the WH then after the show she could afford her boob job she got. She is nothing but a liar she did it enough on the shows so who 's going to believe her\n",
      " stinking book https://t.co/oMxI7Wc7ui\n",
      "RT @bIazingxmexican: Avocado farmers in Santa Barbara County are currently struggling to find workers. So to all of the white people who wh‚Ä¶\n",
      "New post: Mysonne Calls Dj Akademiks A SNITCH https://t.co/l3exQ5KO3G #Akademiks #Calls #MySonne\n",
      "It's Friday!\n",
      "\n",
      "#tokerware #cannabis #clouds #vape #work\n",
      "#FF #tokerware #cannabis #marijuana #satindi https://t.co/LqK7qNTomE\n",
      "Now hiring for 5 #Pharmaceutical #job opportunities in #Arkansas. https://t.co/tdofMe4ifr\n",
      "Good job @DCPoliceDept !! https://t.co/lQDx38RTof\n",
      "i crave ramen at least 3-4 days out the week if not everyday üòê my dad would be so disappointed lmao\n",
      "I‚Äôm wearing red flats and dangling earrings AND I straightened my hair so it‚Äôs time for Difficult Conversations At Work Day!\n",
      "RT @JOBAISREAL: TONIGHT https://t.co/nj8ihROyft\n",
      "RT @DavidBegnaud: The officer in this video - who appears to do NOTHING as a woman is harassed - has been placed on desk duty. \n",
      "He works fo‚Ä¶\n",
      "Join the ProDrivers team! See our latest #job opening here: https://t.co/fxyAo6Uhas #Transportation #Corona, CA #Hiring #CareerArc\n",
      "@politico Nikki Haley isn't as stupid as the Never Trump Dead-Ender STOOGES at Politico, National Review, The Hill, Weekly Standard, ect.\n",
      "Now hiring for 1 #Recruiting #job opportunities in #Wilmington. https://t.co/bFpa7JGTRk\n",
      "Nye County spokesman says the Love Ranch South has been closed, license rescinded after the death of sole licenseholder Dennis Hof yesterday. Obituary: https://t.co/hsTRzGSge3\n",
      "RT @CyrusMMcQueen: If Peter Strzok can be fired for texting anti-Trump sentiments, I‚Äôd be curious to see the phone records of every bitch a‚Ä¶\n",
      "RT @PaisaTweets_: when shit goes down hill at work but you already clocked out.. https://t.co/qh7tivkQkG\n",
      "@brithume What thempoll should have said was that the majority of Democrats aren‚Äôt happy with anything....that would have been more accurate.  Always complaining about not having something they haven‚Äôt worked for.\n",
      "@memma46 hahah! RIGHT??\n",
      "@MReco12 @Marxism_Wokeism Yeah, Venezuela is just in the dictatorship that came after the attempted socialist regime failed to keep it's power. Oh that and it's a state that doesn't have property rights and that just seized a Kellogg cereal factory and gave it to the workers.\n",
      "I need to get my screen fixed in this phone fr\n",
      "RT @NissyEstefany: I hate a ‚Äúit‚Äôs their job anyways‚Äù person üôÑ https://t.co/QyrNU2Gt8q\n",
      "@StinaLeicht @SheckyX RIGHT??? Snap!\n",
      "I'll get right on this... the day after drumpf is gone!\n",
      "@MannyOFay @UnkleNeal @tpovio @parscale @LaraLeaTrump Actually her boss is a crook and a liar and she was his apprentice at it and now, the apprentice surpasses the master.\n",
      "RT @bopinion: Undocumented immigrants make up about half the workforce in U.S. agriculture, but that pool of labor is shrinking. Here are t‚Ä¶\n",
      "@PatrickRothfuss Good grief. Hope all will be safely and quickly resolved! Please stick around a long while more, this world needs you in it.\n",
      "Me: I need to work! üò©  Cat: Work? Work?! Not today, Sally!  Me: That's not my--Cat: Go to bed Sally.üòë Me: Yes'm.üòî #catsruleeverythingaroundme #tryingtowrite #authorlife #tw https://t.co/GHUuq9oW3Z\n",
      "Thank you @AmericanAir for getting Doug Dodson Home so he can race at @bapsspeedway tonight.\n",
      "RT @LancienneCour: This is the mob @CNN wants in your neighborhood. https://t.co/cPuOBRQx7v\n"
     ]
    }
   ],
   "source": [
    "print('50 most similar tweets:\\n')\n",
    "print('\\n'.join(pd.concat([scores,data],1).sort_values(by='score',ascending=False)['text'].head(50).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "See Ya :-)",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m See Ya :-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelfraiberger/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "sys.exit('See Ya :-)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 128\n",
    "# max_seq_length = 140\n",
    "# 6 40 Intel(R) Skylake @ 2.40GHz 376 4 Tesla V100\n",
    "pd.DataFrame([\n",
    "(4933,13),\n",
    "(49330,131),\n",
    "(493300,1315),\n",
    "(4932959,13150), # MaxRSS: 41446568K\n",
    "],columns=['N','T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 49330\n",
    "# max_seq_length = 140\n",
    "# 6 40 Intel(R) Skylake @ 2.40GHz 376 4 Tesla V100\n",
    "pd.DataFrame([\n",
    "(1024,132),\n",
    "(512,132),\n",
    "(256,131),\n",
    "(128,131),\n",
    "(64,134),\n",
    "(32,138),\n",
    "(16,147)],columns=['BATCH_SIZE','T'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
